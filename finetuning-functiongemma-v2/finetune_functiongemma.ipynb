{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Fine-tune FunctionGemma for Square Color Function Calling\n\nThis notebook fine-tunes FunctionGemma for function calling using QLoRA.\n\n**Base Model:** `google/functiongemma-270m-it`\n\n**Based on:** [Google's Emoji Gemma notebook](https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Demos/Emoji-Gemma-on-Web/resources/Fine_tune_Gemma_3_270M_for_emoji_generation.ipynb)\n\n## Steps:\n1. Setup environment\n2. Load the square color dataset\n3. Format data for FunctionGemma\n4. Fine-tune with QLoRA\n5. **Validate model outputs** (CRITICAL)\n6. Save merged model"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch tensorboard\n",
    "%pip install -U transformers trl datasets accelerate evaluate sentencepiece bitsandbytes protobuf==3.20.3 peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart runtime after installing packages\n",
    "# import os\n",
    "# os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Hugging Face Authentication\n\n1. Accept license on [model page](http://huggingface.co/google/functiongemma-270m-it)\n2. Get [Access Token](https://huggingface.co/settings/tokens) with 'Write' access\n3. Create Colab secret: `HF_TOKEN`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Dataset\n",
    "\n",
    "Upload the `square_color_dataset.json` file to Colab or mount Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Option 1: Upload file to Colab\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "\n",
    "# Option 2: Use file path directly\n",
    "DATASET_PATH = \"/content/square_color_dataset.json\"  #@param {type:\"string\"}\n",
    "\n",
    "with open(DATASET_PATH, 'r') as f:\n",
    "    raw_dataset = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(raw_dataset)} examples\")\n",
    "print(f\"\\nExample entry:\")\n",
    "print(json.dumps(raw_dataset[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define FunctionGemma Format\n",
    "\n",
    "FunctionGemma uses specific tokens for function calling:\n",
    "- `<start_function_call>` / `<end_function_call>`\n",
    "- `<escape>` to wrap string values\n",
    "\n",
    "Format: `<start_function_call>call:function_name{param:<escape>value<escape>}<end_function_call>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt required by FunctionGemma\n",
    "SYSTEM_PROMPT = \"\"\"You are a model that can do function calling with the following functions\n",
    "\n",
    "<start_function_declaration>\n",
    "name:set_square_color\n",
    "description:Sets the color of the square to a specified color\n",
    "parameters:{color:{type:string,description:The color to set the square to,required:true}}\n",
    "<end_function_declaration>\n",
    "<start_function_declaration>\n",
    "name:get_square_color\n",
    "description:Gets the current color of the square\n",
    "parameters:{}\n",
    "<end_function_declaration>\"\"\"\n",
    "\n",
    "def format_function_call_output(tool_name: str, tool_arguments: str) -> str:\n",
    "    \"\"\"Format the expected output in FunctionGemma format.\"\"\"\n",
    "    if tool_name == \"set_square_color\":\n",
    "        args = json.loads(tool_arguments)\n",
    "        color = args.get(\"color\", \"\")\n",
    "        return f\"<start_function_call>call:set_square_color{{color:<escape>{color}<escape>}}<end_function_call>\"\n",
    "    elif tool_name == \"get_square_color\":\n",
    "        return \"<start_function_call>call:get_square_color{}<end_function_call>\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown tool: {tool_name}\")\n",
    "\n",
    "# Test the format\n",
    "test_output = format_function_call_output(\"set_square_color\", '{\"color\": \"blue\"}')\n",
    "print(f\"Example output format: {test_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Format Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def format_sample(sample):\n",
    "    \"\"\"Convert raw sample to training format.\"\"\"\n",
    "    expected_output = format_function_call_output(\n",
    "        sample[\"tool_name\"],\n",
    "        sample[\"tool_arguments\"]\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": sample[\"user_content\"]},\n",
    "            {\"role\": \"assistant\", \"content\": expected_output}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "formatted_data = [format_sample(s) for s in raw_dataset]\n",
    "dataset = Dataset.from_list(formatted_data)\n",
    "\n",
    "# Split into train/test\n",
    "dataset_splits = dataset.train_test_split(test_size=0.1, shuffle=True, seed=42)\n",
    "\n",
    "print(f\"Training samples: {len(dataset_splits['train'])}\")\n",
    "print(f\"Test samples: {len(dataset_splits['test'])}\")\n",
    "print(f\"\\nExample formatted sample:\")\n",
    "print(json.dumps(dataset_splits['train'][0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Use the FunctionGemma model (specialized for function calling)\nBASE_MODEL = \"google/functiongemma-270m-it\"\n\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\nbase_model = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL,\n    device_map=\"auto\",\n    attn_implementation=\"eager\",\n    torch_dtype=torch.bfloat16\n)\n\nprint(f\"Model loaded on: {base_model.device}\")\nprint(f\"Model dtype: {base_model.dtype}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Base Model (Before Fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=base_model, tokenizer=tokenizer)\n",
    "\n",
    "test_inputs = [\n",
    "    \"change the color to blue\",\n",
    "    \"what color is the square?\",\n",
    "    \"make it red\",\n",
    "    \"tell me the current color\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BASE MODEL OUTPUT (before fine-tuning)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for test_input in test_inputs:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": test_input}\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    output = pipe(prompt, max_new_tokens=64, do_sample=False)\n",
    "    model_output = output[0]['generated_text'][len(prompt):].strip()\n",
    "    \n",
    "    print(f\"\\nInput: {test_input}\")\n",
    "    print(f\"Output: {model_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Configure QLoRA Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig\n",
    "from trl import SFTConfig\n",
    "\n",
    "ADAPTER_PATH = \"/content/functiongemma-adapters\"\n",
    "\n",
    "# Quantization config for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# LoRA config for parameter-efficient fine-tuning\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=\"all-linear\",\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=[\"lm_head\", \"embed_tokens\"]\n",
    ")\n",
    "\n",
    "# Training config\n",
    "training_args = SFTConfig(\n",
    "    output_dir=ADAPTER_PATH,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    max_length=512,  # Longer for function declarations\n",
    "    gradient_checkpointing=False,\n",
    "    packing=False,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    report_to=\"tensorboard\",\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Reload model with quantization\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation='eager'\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(\"Training configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_splits['train'],\n",
    "    eval_dataset=dataset_splits['test'],\n",
    "    peft_config=lora_config,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "trainer.save_model(ADAPTER_PATH)\n",
    "\n",
    "print(f\"\\nLoRA adapters saved to {ADAPTER_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Plot Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "train_losses = [log[\"loss\"] for log in log_history if \"loss\" in log]\n",
    "epoch_train = [log[\"epoch\"] for log in log_history if \"loss\" in log]\n",
    "eval_losses = [log[\"eval_loss\"] for log in log_history if \"eval_loss\" in log]\n",
    "epoch_eval = [log[\"epoch\"] for log in log_history if \"eval_loss\" in log]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epoch_train, train_losses, label=\"Training Loss\", marker='o')\n",
    "plt.plot(epoch_eval, eval_losses, label=\"Validation Loss\", marker='s')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"FunctionGemma Fine-tuning: Training vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Merge LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "MERGED_MODEL_PATH = \"/content/functiongemma-merged\"\n",
    "\n",
    "# Load base model (without quantization for merging)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(ADAPTER_PATH)\n",
    "\n",
    "# Load and merge LoRA adapters\n",
    "model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Save merged model\n",
    "model.save_pretrained(MERGED_MODEL_PATH)\n",
    "tokenizer.save_pretrained(MERGED_MODEL_PATH)\n",
    "\n",
    "print(f\"Merged model saved to {MERGED_MODEL_PATH}\")\n",
    "print(f\"Vocabulary size: {model.config.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. CRITICAL: Validate Fine-tuned Model\n",
    "\n",
    "**IMPORTANT**: Test the model BEFORE exporting to ONNX!\n",
    "\n",
    "If it generates garbage here, the problem is in the fine-tuning, not the ONNX export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the merged model\n",
    "merged_model = AutoModelForCausalLM.from_pretrained(MERGED_MODEL_PATH, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MERGED_MODEL_PATH)\n",
    "pipe = pipeline(\"text-generation\", model=merged_model, tokenizer=tokenizer)\n",
    "\n",
    "test_inputs = [\n",
    "    \"change the color to blue\",\n",
    "    \"what color is the square?\",\n",
    "    \"make it red\",\n",
    "    \"tell me the current color\",\n",
    "    \"set to green\",\n",
    "    \"color?\",\n",
    "    \"I want purple\",\n",
    "    \"gimme yellow\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FINE-TUNED MODEL VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "success_count = 0\n",
    "for test_input in test_inputs:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": test_input}\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    output = pipe(prompt, max_new_tokens=64, do_sample=False)\n",
    "    model_output = output[0]['generated_text'][len(prompt):].strip()\n",
    "    \n",
    "    # Check if output contains expected function call format\n",
    "    is_valid = \"<start_function_call>\" in model_output and \"<end_function_call>\" in model_output\n",
    "    status = \"OK\" if is_valid else \"FAIL\"\n",
    "    if is_valid:\n",
    "        success_count += 1\n",
    "    \n",
    "    print(f\"\\n[{status}] Input: {test_input}\")\n",
    "    print(f\"      Output: {model_output}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"VALIDATION RESULT: {success_count}/{len(test_inputs)} passed\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if success_count < len(test_inputs) * 0.8:\n",
    "    print(\"\\n WARNING: Model is not generating correct function calls!\")\n",
    "    print(\"Do NOT proceed to ONNX export until this is fixed.\")\n",
    "else:\n",
    "    print(\"\\n Model is generating correct function calls. Ready for ONNX export!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Upload to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from huggingface_hub import ModelCard, whoami\n\nMODEL_NAME = \"functiongemma-square-color\"  #@param {type:\"string\"}\n\nusername = whoami()['name']\nhf_repo_id = f\"{username}/{MODEL_NAME}\"\n\n# Push model and tokenizer\nrepo_url = merged_model.push_to_hub(hf_repo_id, create_repo=True, commit_message=\"Upload fine-tuned FunctionGemma\")\ntokenizer.push_to_hub(hf_repo_id)\n\n# Create model card\ncard_content = f\"\"\"\n---\nbase_model: google/functiongemma-270m-it\ntags:\n- text-generation\n- function-calling\n- gemma\n---\n\n# FunctionGemma Square Color\n\nA fine-tuned FunctionGemma model for square color function calling.\n\n## Functions\n- `set_square_color(color: string)` - Sets the square color\n- `get_square_color()` - Gets the current color\n\n## Usage\n```python\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"{hf_repo_id}\")\noutput = pipe(\"change the color to blue\")\n```\n\"\"\"\ncard = ModelCard(card_content)\ncard.push_to_hub(hf_repo_id)\n\nprint(f\"Model uploaded to: {repo_url}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "If the model validation passed, proceed to:\n",
    "\n",
    "1. **Export to ONNX** using `export_to_onnx.ipynb`\n",
    "2. Upload ONNX model to HuggingFace\n",
    "3. Test in the browser with Transformers.js"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}