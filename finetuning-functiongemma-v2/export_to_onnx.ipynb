{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export FunctionGemma to ONNX\n",
    "\n",
    "This notebook converts the fine-tuned FunctionGemma model to ONNX format for use with Transformers.js in the browser.\n",
    "\n",
    "**Based on:** [Google's ONNX conversion notebook](https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Demos/Emoji-Gemma-on-Web/resources/Convert_Gemma_3_270M_to_ONNX.ipynb)\n",
    "\n",
    "## Prerequisites\n",
    "- Complete the fine-tuning notebook first (`finetune_functiongemma.ipynb`)\n",
    "- Model must be validated and uploaded to HuggingFace\n",
    "\n",
    "## Steps:\n",
    "1. Setup environment\n",
    "2. Convert model to ONNX (multiple quantizations)\n",
    "3. Test converted model\n",
    "4. Upload to HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers==4.56.1 onnx==1.19.0 onnx_ir==0.1.7 onnxruntime==1.22.1 numpy==2.3.2 huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart runtime after installing\n",
    "# import os\n",
    "# os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hugging Face Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download Conversion Script\n",
    "\n",
    "The `build_gemma.py` script by Xenova handles the conversion and quantization specifically for Gemma 3 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://gist.githubusercontent.com/xenova/a219dbf3c7da7edd5dbb05f92410d7bd/raw/45f4c5a5227c1123efebe1e36d060672ee685a8e/build_gemma.py\n",
    "\n",
    "print(\"Conversion script downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Convert Model to ONNX\n\nSpecify your fine-tuned model from HuggingFace.\n\n**Note:** We use FP16 quantization as it provides the best balance between size (~544MB) and accuracy. Q4 quantization causes model hallucinations with fine-tuned models."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your fine-tuned model from HuggingFace\n",
    "MODEL_AUTHOR = \"harlley\"  #@param {type:\"string\"}\n",
    "MODEL_NAME = \"functiongemma-square-color\"  #@param {type:\"string\"}\n",
    "\n",
    "REPO_ID = f\"{MODEL_AUTHOR}/{MODEL_NAME}\"\n",
    "SAVE_PATH = f\"/content/{MODEL_NAME}-ONNX\"\n",
    "\n",
    "print(f\"Converting model: {REPO_ID}\")\n",
    "print(f\"Output path: {SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run the conversion (FP16 only - Q4 causes hallucinations with fine-tuned models)\n!python build_gemma.py \\\n    --model_name {REPO_ID} \\\n    --output {SAVE_PATH} \\\n    -p fp16\n\nprint(f\"\\nONNX model saved to {SAVE_PATH}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Verify Output Structure\n\nThe output should have this structure:\n```\nmodel-ONNX/\n├── config.json\n├── tokenizer.json\n├── tokenizer_config.json\n└── onnx/\n    ├── model_fp16.onnx\n    └── model_fp16.onnx_data\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"Output structure:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for root, dirs, files in os.walk(SAVE_PATH):\n",
    "    level = root.replace(SAVE_PATH, '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files:\n",
    "        filepath = os.path.join(root, file)\n",
    "        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
    "        print(f\"{subindent}{file} ({size_mb:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify critical files exist at root\n",
    "critical_files = ['config.json', 'tokenizer.json', 'tokenizer_config.json']\n",
    "\n",
    "print(\"Checking critical files at root...\")\n",
    "for f in critical_files:\n",
    "    path = os.path.join(SAVE_PATH, f)\n",
    "    exists = os.path.exists(path)\n",
    "    status = \"OK\" if exists else \"MISSING\"\n",
    "    print(f\"  [{status}] {f}\")\n",
    "\n",
    "# Check ONNX files\n",
    "onnx_dir = os.path.join(SAVE_PATH, 'onnx')\n",
    "if os.path.exists(onnx_dir):\n",
    "    onnx_files = [f for f in os.listdir(onnx_dir) if f.endswith('.onnx')]\n",
    "    print(f\"\\nONNX files: {len(onnx_files)}\")\n",
    "    for f in onnx_files:\n",
    "        print(f\"  - {f}\")\n",
    "else:\n",
    "    print(\"\\n WARNING: onnx/ directory not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Converted Model\n",
    "\n",
    "Test the ONNX model using ONNX Runtime before uploading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import AutoConfig, AutoTokenizer, GenerationConfig\nimport onnxruntime\nimport numpy as np\n\n# Load config and tokenizer\nconfig = AutoConfig.from_pretrained(SAVE_PATH)\ntokenizer = AutoTokenizer.from_pretrained(SAVE_PATH)\n\n# FP16 model (best balance of size and accuracy)\nMODEL_FILE = \"onnx/model_fp16.onnx\"\n\nmodel_path = f\"{SAVE_PATH}/{MODEL_FILE}\"\nprint(f\"Loading model: {model_path}\")\n\ndecoder_session = onnxruntime.InferenceSession(model_path)\nprint(\"Model loaded!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# System prompt for FunctionGemma\nSYSTEM_PROMPT = \"\"\"You are a model that can do function calling with the following functions\n\n<start_function_declaration>\nname:set_square_color\ndescription:Sets the color of the square to a specified color\nparameters:{color:{type:string,description:The color to set the square to,required:true}}\n<end_function_declaration>\n<start_function_declaration>\nname:get_square_color\ndescription:Gets the current color of the square\nparameters:{}\n<end_function_declaration>\"\"\"\n\ndef run_inference(user_input, max_new_tokens=64):\n    \"\"\"Run inference on the ONNX model.\"\"\"\n    # Config values\n    num_key_value_heads = config.num_key_value_heads\n    head_dim = config.head_dim\n    num_hidden_layers = config.num_hidden_layers\n    eos_token_id = tokenizer.eos_token_id\n    \n    # Get token IDs for stop sequence\n    end_function_token_ids = tokenizer.encode(\"<end_function_call>\", add_special_tokens=False)\n    \n    # Prepare inputs\n    messages = [\n        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n        {\"role\": \"user\", \"content\": user_input},\n    ]\n    \n    inputs = tokenizer.apply_chat_template(\n        messages,\n        add_generation_prompt=True,\n        tokenize=True,\n        return_dict=True,\n        return_tensors=\"np\"\n    )\n    \n    input_ids = inputs['input_ids']\n    attention_mask = inputs['attention_mask']\n    batch_size = input_ids.shape[0]\n    \n    # Use float16 for FP16 model\n    kv_dtype = np.float16\n    \n    past_key_values = {\n        f'past_key_values.{layer}.{kv}': np.zeros([batch_size, num_key_value_heads, 0, head_dim], dtype=kv_dtype)\n        for layer in range(num_hidden_layers)\n        for kv in ('key', 'value')\n    }\n    \n    position_ids = np.tile(np.arange(0, input_ids.shape[-1]), (batch_size, 1))\n    \n    # Generation loop\n    generated_tokens = np.array([[]], dtype=np.int64)\n    \n    for i in range(max_new_tokens):\n        logits, *present_key_values = decoder_session.run(None, dict(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            **past_key_values,\n        ))\n        \n        # Get next token\n        input_ids = logits[:, -1].argmax(-1, keepdims=True)\n        attention_mask = np.concatenate([attention_mask, np.ones_like(input_ids, dtype=np.int64)], axis=-1)\n        position_ids = position_ids[:, -1:] + 1\n        \n        # Update KV cache\n        for j, key in enumerate(past_key_values):\n            past_key_values[key] = present_key_values[j]\n        \n        generated_tokens = np.concatenate([generated_tokens, input_ids], axis=-1)\n        \n        # Stop at EOS or end_function_call\n        if np.isin(input_ids, eos_token_id).any() or np.isin(input_ids, end_function_token_ids).any():\n            break\n    \n    return tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test the ONNX model\nimport re\n\ntest_inputs = [\n    \"change the color to blue\",\n    \"what color is the square?\",\n    \"make it red\",\n    \"tell me the current color\",\n    \"set to green\",\n    \"color?\"\n]\n\n# Expected function call patterns\nSET_COLOR_PATTERN = r\"<start_function_call>call:set_square_color\\{color:<escape>\\w+<escape>\\}<end_function_call>\"\nGET_COLOR_PATTERN = r\"<start_function_call>call:get_square_color\\{\\}<end_function_call>\"\n\nprint(\"=\" * 70)\nprint(\"ONNX MODEL VALIDATION\")\nprint(f\"Model: {MODEL_FILE}\")\nprint(\"=\" * 70)\n\nsuccess_count = 0\nfor test_input in test_inputs:\n    output = run_inference(test_input)\n    \n    # Check if output matches expected function call format\n    is_set_color = re.search(SET_COLOR_PATTERN, output)\n    is_get_color = re.search(GET_COLOR_PATTERN, output)\n    is_valid = bool(is_set_color or is_get_color)\n    \n    status = \"OK\" if is_valid else \"FAIL\"\n    if is_valid:\n        success_count += 1\n    \n    print(f\"\\n[{status}] Input: {test_input}\")\n    print(f\"      Output: {output}\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(f\"RESULT: {success_count}/{len(test_inputs)} passed\")\nprint(\"=\" * 70)\n\nif success_count < len(test_inputs) * 0.8:\n    print(\"\\n WARNING: ONNX model is not generating correct function calls!\")\n    print(\"Check the fine-tuned model before exporting.\")\nelse:\n    print(\"\\n ONNX model is working correctly!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Upload to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "from huggingface_hub import whoami\n",
    "\n",
    "username = whoami()['name']\n",
    "\n",
    "ONNX_MODEL_NAME = \"functiongemma-square-color-ONNX\"  #@param {type:\"string\"}\n",
    "HF_REPO_ID = f\"{username}/{ONNX_MODEL_NAME}\"\n",
    "\n",
    "print(f\"Uploading to: {HF_REPO_ID}\")\n",
    "\n",
    "# Create repo if needed\n",
    "huggingface_hub.create_repo(HF_REPO_ID, exist_ok=True)\n",
    "\n",
    "# Upload entire folder\n",
    "repo_url = huggingface_hub.upload_folder(\n",
    "    folder_path=SAVE_PATH,\n",
    "    repo_id=HF_REPO_ID,\n",
    "    repo_type=\"model\",\n",
    "    commit_message=f\"Upload ONNX model for {ONNX_MODEL_NAME}\"\n",
    ")\n",
    "\n",
    "print(f\"\\nUploaded to: {repo_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Update Browser Code\n\nAfter uploading, update `src/worker.ts` in your project:\n\n```typescript\nconst MODEL_ID = \"harlley/functiongemma-square-color-ONNX\";\n\nAutoModelForCausalLM.from_pretrained(MODEL_ID, {\n  dtype: \"fp16\",  // FP16 for accurate function calling\n  device: \"webgpu\",\n  progress_callback: onProgress,\n})\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nYour FunctionGemma ONNX model is now ready for browser deployment!\n\n**Files uploaded:**\n- `config.json` - Model configuration\n- `tokenizer.json` - Tokenizer\n- `tokenizer_config.json` - Tokenizer config with chat template\n- `onnx/model_fp16.onnx` - FP16 model (~544MB)\n\n**Why FP16?**\n- Q4 quantization causes hallucinations with fine-tuned models\n- FP16 maintains accuracy while being half the size of FP32\n\n**Next steps:**\n1. Update `src/worker.ts` with your model ID and `dtype: \"fp16\"`\n2. Run `npm run dev` to test\n3. Verify function calls work in the browser"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}