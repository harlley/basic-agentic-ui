{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLmoX1dv5Uo2"
      },
      "source": [
        "# Export FunctionGemma to ONNX\n",
        "\n",
        "This notebook converts fine-tuned FunctionGemma models to ONNX format for use with [Transformers.js](https://huggingface.co/docs/transformers.js).\n",
        "\n",
        "**Based on:** [Google's official Gemma 3 to ONNX notebook](https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Demos/Emoji-Gemma-on-Web/resources/Convert_Gemma_3_270M_to_ONNX.ipynb)\n",
        "\n",
        "**Conversion script:** [build_gemma.py by Xenova](https://gist.github.com/xenova/a219dbf3c7da7edd5dbb05f92410d7bd)\n",
        "\n",
        "## Steps:\n",
        "1. Install dependencies (exact versions from Google's notebook)\n",
        "2. Authenticate with Hugging Face\n",
        "3. Configure model parameters\n",
        "4. Convert model to ONNX (fp32, fp16, q4, q4f16)\n",
        "5. Verify file structure\n",
        "6. Test ONNX model\n",
        "7. Upload to Hugging Face\n",
        "8. Integrate with browser code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0G5R-b55Uo4"
      },
      "source": [
        "## 1. Install Dependencies\n",
        "\n",
        "Install the exact package versions from Google's official notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UcVXYBU75Uo4",
        "outputId": "94a4c001-7fc3-4c84-e6bb-84e9200b4d0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.56.1\n",
            "  Downloading transformers-4.56.1-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnx==1.19.0\n",
            "  Downloading onnx-1.19.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting onnx_ir==0.1.7\n",
            "  Downloading onnx_ir-0.1.7-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting onnxruntime==1.22.1\n",
            "  Downloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting numpy==2.3.2\n",
            "  Downloading numpy-2.3.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.1) (3.20.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.1) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.1) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.1) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.1) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.1) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.1) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.1) (4.67.1)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx==1.19.0) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from onnx==1.19.0) (4.15.0)\n",
            "Requirement already satisfied: ml_dtypes in /usr/local/lib/python3.12/dist-packages (from onnx==1.19.0) (0.5.4)\n",
            "Collecting coloredlogs (from onnxruntime==1.22.1)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime==1.22.1) (25.9.23)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime==1.22.1) (1.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.2.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime==1.22.1)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.56.1) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.56.1) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.56.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.56.1) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime==1.22.1) (1.3.0)\n",
            "Downloading transformers-4.56.1-py3-none-any.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.19.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx_ir-0.1.7-py3-none-any.whl (124 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.4/124.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.3.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, humanfriendly, coloredlogs, onnxruntime, onnx, transformers, onnx_ir\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.3\n",
            "    Uninstalling transformers-4.57.3:\n",
            "      Successfully uninstalled transformers-4.57.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.2 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.2 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.2 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.2 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed coloredlogs-15.0.1 humanfriendly-10.0 numpy-2.3.2 onnx-1.19.0 onnx_ir-0.1.7 onnxruntime-1.22.1 transformers-4.56.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "97e186bd53ad4407b0312a38c3e31a1b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Install exact versions from Google's official notebook\n",
        "%pip install transformers==4.56.1 onnx==1.19.0 onnx_ir==0.1.7 onnxruntime==1.22.1 numpy==2.3.2 huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30xU3Hyc5Uo5"
      },
      "outputs": [],
      "source": [
        "# Restart the session runtime to use the newly installed packages\n",
        "# Uncomment and run:\n",
        "# import os\n",
        "# os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPhp-WlO5Uo5"
      },
      "source": [
        "## 2. Hugging Face Authentication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maBgDzgg5Uo5",
        "outputId": "20590c3b-d61b-4e5d-b0e4-3e6e11e4a951"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Authenticated with Hugging Face Hub\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token)\n",
        "print(\"✓ Authenticated with Hugging Face Hub\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UubkVdHD5Uo5"
      },
      "source": [
        "## 3. Configure Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HK_m9I325Uo5",
        "outputId": "5edb4507-78f9-4d94-90e5-1eb3854fcd3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model to convert: harlley/functiongemma-square-color\n"
          ]
        }
      ],
      "source": [
        "# Replace with your fine-tuned model details\n",
        "MODEL_AUTHOR = \"harlley\"  # @param {type:\"string\"}\n",
        "MODEL_NAME = \"functiongemma-square-color\"  # @param {type:\"string\"}\n",
        "\n",
        "REPO_ID = f\"{MODEL_AUTHOR}/{MODEL_NAME}\"\n",
        "\n",
        "print(f\"Model to convert: {REPO_ID}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaBtMDdA5Uo6"
      },
      "source": [
        "## 4. Convert Model to ONNX\n",
        "\n",
        "Uses Xenova's `build_gemma.py` script to convert and quantize the model.\n",
        "\n",
        "**Precisions generated:**\n",
        "- `fp32`: Full precision (largest)\n",
        "- `fp16`: Half precision\n",
        "- `q4`: 4-bit quantized (smallest)\n",
        "- `q4f16`: 4-bit with fp16 (best for WebGPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7k4C2Pbb5Uo6",
        "outputId": "bf988b4c-2b1d-48d2-91d4-c56f7848afd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-05 20:45:25,955 numexpr.utils [INFO] - NumExpr defaulting to 2 threads.\n",
            "Saving config and processing files in /content/functiongemma-square-color-onnx\n",
            "config.json: 1.36kB [00:00, 3.21MB/s]\n",
            "generation_config.json: 100% 176/176 [00:00<00:00, 1.42MB/s]\n",
            "tokenizer_config.json: 1.16MB [00:00, 493MB/s]\n",
            "tokenizer.model: 100% 4.69M/4.69M [00:01<00:00, 3.46MB/s]\n",
            "tokenizer.json: 100% 33.4M/33.4M [00:00<00:00, 56.0MB/s]\n",
            "added_tokens.json: 100% 63.0/63.0 [00:00<00:00, 678kB/s]\n",
            "special_tokens_map.json: 100% 706/706 [00:00<00:00, 7.40MB/s]\n",
            "chat_template.jinja: 13.8kB [00:00, 58.0MB/s]\n",
            "Loading PyTorch model...\n",
            "2026-01-05 20:45:38.982638: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1767645939.007256     668 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1767645939.014812     668 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1767645939.032680     668 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1767645939.032699     668 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1767645939.032703     668 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1767645939.032709     668 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-05 20:45:39.036927: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "model.safetensors: 100% 1.07G/1.07G [00:09<00:00, 112MB/s]\n",
            "Building embedding layer...\n",
            "Building layer 0/17...\n",
            "Building layer 1/17...\n",
            "Building layer 2/17...\n",
            "Building layer 3/17...\n",
            "Building layer 4/17...\n",
            "Building layer 5/17...\n",
            "Building layer 6/17...\n",
            "Building layer 7/17...\n",
            "Building layer 8/17...\n",
            "Building layer 9/17...\n",
            "Building layer 10/17...\n",
            "Building layer 11/17...\n",
            "Building layer 12/17...\n",
            "Building layer 13/17...\n",
            "Building layer 14/17...\n",
            "Building layer 15/17...\n",
            "Building layer 16/17...\n",
            "Building layer 17/17...\n",
            "Building final normalization layer...\n",
            "Building LM head...\n",
            "ONNX model construction complete.\n",
            "Loading PyTorch model...\n",
            "Building embedding layer...\n",
            "Building layer 0/17...\n",
            "Building layer 1/17...\n",
            "Building layer 2/17...\n",
            "Building layer 3/17...\n",
            "Building layer 4/17...\n",
            "Building layer 5/17...\n",
            "Building layer 6/17...\n",
            "Building layer 7/17...\n",
            "Building layer 8/17...\n",
            "Building layer 9/17...\n",
            "Building layer 10/17...\n",
            "Building layer 11/17...\n",
            "Building layer 12/17...\n",
            "Building layer 13/17...\n",
            "Building layer 14/17...\n",
            "Building layer 15/17...\n",
            "Building layer 16/17...\n",
            "Building layer 17/17...\n",
            "Building final normalization layer...\n",
            "Building LM head...\n",
            "ONNX model construction complete.\n",
            "Loading PyTorch model...\n",
            "Building embedding layer...\n",
            "Building layer 0/17...\n",
            "Building layer 1/17...\n",
            "Building layer 2/17...\n",
            "Building layer 3/17...\n",
            "Building layer 4/17...\n",
            "Building layer 5/17...\n",
            "Building layer 6/17...\n",
            "Building layer 7/17...\n",
            "Building layer 8/17...\n",
            "Building layer 9/17...\n",
            "Building layer 10/17...\n",
            "Building layer 11/17...\n",
            "Building layer 12/17...\n",
            "Building layer 13/17...\n",
            "Building layer 14/17...\n",
            "Building layer 15/17...\n",
            "Building layer 16/17...\n",
            "Building layer 17/17...\n",
            "Building final normalization layer...\n",
            "Building LM head...\n",
            "ONNX model construction complete.\n",
            "2026-01-05 20:46:16,764 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/constant_nodes/INT64/[1] ...\n",
            "2026-01-05 20:46:16,764 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/preprocessing/attn_mask/ReduceSum ...\n",
            "2026-01-05 20:46:16,765 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/preprocessing/attn_mask/Sub ...\n",
            "2026-01-05 20:46:16,765 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/preprocessing/attn_mask/seqlens_k_cast/Cast ...\n",
            "2026-01-05 20:46:16,765 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/preprocessing/attn_mask/Shape ...\n",
            "2026-01-05 20:46:16,765 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/constant_nodes/INT64/1 ...\n",
            "2026-01-05 20:46:16,765 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/preprocessing/attn_mask/Gather ...\n",
            "2026-01-05 20:46:16,765 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/preprocessing/attn_mask/total_seq_len_cast/Cast ...\n",
            "2026-01-05 20:46:16,765 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/preprocessing/pos_ids/Shape ...\n",
            "2026-01-05 20:46:16,765 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/preprocessing/pos_ids/Gather ...\n",
            "2026-01-05 20:46:16,765 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/constant_nodes/INT64/[0] ...\n",
            "2026-01-05 20:46:16,765 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/preprocessing/pos_ids/Unsqueeze ...\n",
            "2026-01-05 20:46:16,765 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/constant_nodes/INT64/[-1] ...\n",
            "2026-01-05 20:46:16,765 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/preprocessing/pos_ids/Concat ...\n",
            "2026-01-05 20:46:16,765 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/preprocessing/pos_ids/Reshape ...\n",
            "2026-01-05 20:46:16,765 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/embed_tokens/Gather ...\n",
            "2026-01-05 20:46:16,765 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/constant_nodes/FLOAT/25.298221281347036 ...\n",
            "2026-01-05 20:46:16,765 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/embed_tokens/Mul ...\n",
            "2026-01-05 20:46:16,765 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/input_layernorm ...\n",
            "2026-01-05 20:46:16,765 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/attn/q_proj/MatMul ...\n",
            "2026-01-05 20:46:16,788 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/attn/q_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:16,788 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/attn/k_proj/MatMul ...\n",
            "2026-01-05 20:46:16,791 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/attn/k_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:16,791 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/attn/v_proj/MatMul ...\n",
            "2026-01-05 20:46:16,793 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/attn/v_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:16,793 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/constant_nodes/INT64/[0, -1, 256] ...\n",
            "2026-01-05 20:46:16,793 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/attn/q_norm/Reshape1 ...\n",
            "2026-01-05 20:46:16,793 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/attn/q_norm/LayerNorm ...\n",
            "2026-01-05 20:46:16,794 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/constant_nodes/INT64/[0, -1, 1024] ...\n",
            "2026-01-05 20:46:16,794 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/attn/q_norm/Reshape2 ...\n",
            "2026-01-05 20:46:16,794 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/attn/k_norm/Reshape1 ...\n",
            "2026-01-05 20:46:16,794 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/attn/k_norm/LayerNorm ...\n",
            "2026-01-05 20:46:16,794 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/attn/k_norm/Reshape2 ...\n",
            "2026-01-05 20:46:16,794 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/attn/q_rotary ...\n",
            "2026-01-05 20:46:16,794 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/attn/k_rotary ...\n",
            "2026-01-05 20:46:16,794 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/attn/GroupQueryAttention ...\n",
            "2026-01-05 20:46:16,794 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/attn/o_proj/MatMul ...\n",
            "2026-01-05 20:46:16,803 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/attn/o_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:16,803 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/post_attention_layernorm ...\n",
            "2026-01-05 20:46:16,803 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/Add_1 ...\n",
            "2026-01-05 20:46:16,803 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/pre_feedforward_layernorm ...\n",
            "2026-01-05 20:46:16,803 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/mlp/gate_proj/MatMul ...\n",
            "2026-01-05 20:46:16,823 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:16,823 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/mlp/up_proj/MatMul ...\n",
            "2026-01-05 20:46:16,842 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/mlp/up_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:16,843 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/mlp/act_fn/FastGelu ...\n",
            "2026-01-05 20:46:16,843 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/mlp/Mul ...\n",
            "2026-01-05 20:46:16,843 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/mlp/down_proj/MatMul ...\n",
            "2026-01-05 20:46:16,860 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/mlp/down_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:16,860 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/post_feedforward_layernorm ...\n",
            "2026-01-05 20:46:16,860 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/Add_2 ...\n",
            "2026-01-05 20:46:16,860 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/input_layernorm ...\n",
            "2026-01-05 20:46:16,860 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/attn/q_proj/MatMul ...\n",
            "2026-01-05 20:46:16,870 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/attn/q_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:16,870 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/attn/k_proj/MatMul ...\n",
            "2026-01-05 20:46:16,872 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/attn/k_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:16,872 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/attn/v_proj/MatMul ...\n",
            "2026-01-05 20:46:16,875 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/attn/v_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:16,875 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/attn/q_norm/Reshape1 ...\n",
            "2026-01-05 20:46:16,875 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/attn/q_norm/LayerNorm ...\n",
            "2026-01-05 20:46:16,875 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/attn/q_norm/Reshape2 ...\n",
            "2026-01-05 20:46:16,875 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/attn/k_norm/Reshape1 ...\n",
            "2026-01-05 20:46:16,875 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/attn/k_norm/LayerNorm ...\n",
            "2026-01-05 20:46:16,875 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/attn/k_norm/Reshape2 ...\n",
            "2026-01-05 20:46:16,875 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/attn/q_rotary ...\n",
            "2026-01-05 20:46:16,875 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/attn/k_rotary ...\n",
            "2026-01-05 20:46:16,875 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/attn/GroupQueryAttention ...\n",
            "2026-01-05 20:46:16,875 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/attn/o_proj/MatMul ...\n",
            "2026-01-05 20:46:16,887 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/attn/o_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:16,887 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/post_attention_layernorm ...\n",
            "2026-01-05 20:46:16,887 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/Add_1 ...\n",
            "2026-01-05 20:46:16,887 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/pre_feedforward_layernorm ...\n",
            "2026-01-05 20:46:16,887 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/mlp/gate_proj/MatMul ...\n",
            "2026-01-05 20:46:16,910 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:16,910 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/mlp/up_proj/MatMul ...\n",
            "2026-01-05 20:46:16,934 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/mlp/up_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:16,934 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/mlp/act_fn/FastGelu ...\n",
            "2026-01-05 20:46:16,934 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/mlp/Mul ...\n",
            "2026-01-05 20:46:16,934 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/mlp/down_proj/MatMul ...\n",
            "2026-01-05 20:46:16,955 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/mlp/down_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:16,955 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/post_feedforward_layernorm ...\n",
            "2026-01-05 20:46:16,955 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/Add_2 ...\n",
            "2026-01-05 20:46:16,955 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/input_layernorm ...\n",
            "2026-01-05 20:46:16,955 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/attn/q_proj/MatMul ...\n",
            "2026-01-05 20:46:16,966 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/attn/q_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:16,966 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/attn/k_proj/MatMul ...\n",
            "2026-01-05 20:46:16,969 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/attn/k_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:16,969 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/attn/v_proj/MatMul ...\n",
            "2026-01-05 20:46:16,972 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/attn/v_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:16,972 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/attn/q_norm/Reshape1 ...\n",
            "2026-01-05 20:46:16,972 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/attn/q_norm/LayerNorm ...\n",
            "2026-01-05 20:46:16,972 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/attn/q_norm/Reshape2 ...\n",
            "2026-01-05 20:46:16,972 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/attn/k_norm/Reshape1 ...\n",
            "2026-01-05 20:46:16,972 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/attn/k_norm/LayerNorm ...\n",
            "2026-01-05 20:46:16,972 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/attn/k_norm/Reshape2 ...\n",
            "2026-01-05 20:46:16,972 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/attn/q_rotary ...\n",
            "2026-01-05 20:46:16,972 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/attn/k_rotary ...\n",
            "2026-01-05 20:46:16,972 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/attn/GroupQueryAttention ...\n",
            "2026-01-05 20:46:16,972 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/attn/o_proj/MatMul ...\n",
            "2026-01-05 20:46:16,985 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/attn/o_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:16,985 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/post_attention_layernorm ...\n",
            "2026-01-05 20:46:16,985 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/Add_1 ...\n",
            "2026-01-05 20:46:16,986 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/pre_feedforward_layernorm ...\n",
            "2026-01-05 20:46:16,986 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/mlp/gate_proj/MatMul ...\n",
            "2026-01-05 20:46:17,009 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,009 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/mlp/up_proj/MatMul ...\n",
            "2026-01-05 20:46:17,041 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/mlp/up_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,041 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/mlp/act_fn/FastGelu ...\n",
            "2026-01-05 20:46:17,041 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/mlp/Mul ...\n",
            "2026-01-05 20:46:17,042 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/mlp/down_proj/MatMul ...\n",
            "2026-01-05 20:46:17,068 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/mlp/down_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,068 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/post_feedforward_layernorm ...\n",
            "2026-01-05 20:46:17,068 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/Add_2 ...\n",
            "2026-01-05 20:46:17,068 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/input_layernorm ...\n",
            "2026-01-05 20:46:17,068 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/attn/q_proj/MatMul ...\n",
            "2026-01-05 20:46:17,079 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/attn/q_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,079 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/attn/k_proj/MatMul ...\n",
            "2026-01-05 20:46:17,082 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/attn/k_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,082 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/attn/v_proj/MatMul ...\n",
            "2026-01-05 20:46:17,085 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/attn/v_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,085 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/attn/q_norm/Reshape1 ...\n",
            "2026-01-05 20:46:17,085 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/attn/q_norm/LayerNorm ...\n",
            "2026-01-05 20:46:17,085 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/attn/q_norm/Reshape2 ...\n",
            "2026-01-05 20:46:17,085 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/attn/k_norm/Reshape1 ...\n",
            "2026-01-05 20:46:17,085 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/attn/k_norm/LayerNorm ...\n",
            "2026-01-05 20:46:17,085 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/attn/k_norm/Reshape2 ...\n",
            "2026-01-05 20:46:17,085 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/attn/q_rotary ...\n",
            "2026-01-05 20:46:17,085 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/attn/k_rotary ...\n",
            "2026-01-05 20:46:17,085 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/attn/GroupQueryAttention ...\n",
            "2026-01-05 20:46:17,086 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/attn/o_proj/MatMul ...\n",
            "2026-01-05 20:46:17,096 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/attn/o_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,096 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/post_attention_layernorm ...\n",
            "2026-01-05 20:46:17,096 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/Add_1 ...\n",
            "2026-01-05 20:46:17,096 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/pre_feedforward_layernorm ...\n",
            "2026-01-05 20:46:17,096 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/mlp/gate_proj/MatMul ...\n",
            "2026-01-05 20:46:17,119 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,119 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/mlp/up_proj/MatMul ...\n",
            "2026-01-05 20:46:17,139 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/mlp/up_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,139 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/mlp/act_fn/FastGelu ...\n",
            "2026-01-05 20:46:17,139 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/mlp/Mul ...\n",
            "2026-01-05 20:46:17,139 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/mlp/down_proj/MatMul ...\n",
            "2026-01-05 20:46:17,158 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/mlp/down_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,158 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/post_feedforward_layernorm ...\n",
            "2026-01-05 20:46:17,158 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/Add_2 ...\n",
            "2026-01-05 20:46:17,158 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/input_layernorm ...\n",
            "2026-01-05 20:46:17,158 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/attn/q_proj/MatMul ...\n",
            "2026-01-05 20:46:17,167 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/attn/q_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,168 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/attn/k_proj/MatMul ...\n",
            "2026-01-05 20:46:17,170 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/attn/k_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,170 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/attn/v_proj/MatMul ...\n",
            "2026-01-05 20:46:17,172 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/attn/v_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,172 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/attn/q_norm/Reshape1 ...\n",
            "2026-01-05 20:46:17,172 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/attn/q_norm/LayerNorm ...\n",
            "2026-01-05 20:46:17,173 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/attn/q_norm/Reshape2 ...\n",
            "2026-01-05 20:46:17,173 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/attn/k_norm/Reshape1 ...\n",
            "2026-01-05 20:46:17,173 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/attn/k_norm/LayerNorm ...\n",
            "2026-01-05 20:46:17,173 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/attn/k_norm/Reshape2 ...\n",
            "2026-01-05 20:46:17,173 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/attn/q_rotary ...\n",
            "2026-01-05 20:46:17,173 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/attn/k_rotary ...\n",
            "2026-01-05 20:46:17,173 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/attn/GroupQueryAttention ...\n",
            "2026-01-05 20:46:17,173 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/attn/o_proj/MatMul ...\n",
            "2026-01-05 20:46:17,185 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/attn/o_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,185 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/post_attention_layernorm ...\n",
            "2026-01-05 20:46:17,185 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/Add_1 ...\n",
            "2026-01-05 20:46:17,185 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/pre_feedforward_layernorm ...\n",
            "2026-01-05 20:46:17,185 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/mlp/gate_proj/MatMul ...\n",
            "2026-01-05 20:46:17,206 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,206 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/mlp/up_proj/MatMul ...\n",
            "2026-01-05 20:46:17,227 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/mlp/up_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,227 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/mlp/act_fn/FastGelu ...\n",
            "2026-01-05 20:46:17,227 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/mlp/Mul ...\n",
            "2026-01-05 20:46:17,227 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/mlp/down_proj/MatMul ...\n",
            "2026-01-05 20:46:17,248 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/mlp/down_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,248 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/post_feedforward_layernorm ...\n",
            "2026-01-05 20:46:17,248 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/Add_2 ...\n",
            "2026-01-05 20:46:17,248 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/input_layernorm ...\n",
            "2026-01-05 20:46:17,248 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/attn/q_proj/MatMul ...\n",
            "2026-01-05 20:46:17,258 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/attn/q_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,258 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/attn/k_proj/MatMul ...\n",
            "2026-01-05 20:46:17,261 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/attn/k_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,261 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/attn/v_proj/MatMul ...\n",
            "2026-01-05 20:46:17,264 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/attn/v_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,264 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/attn/q_norm/Reshape1 ...\n",
            "2026-01-05 20:46:17,264 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/attn/q_norm/LayerNorm ...\n",
            "2026-01-05 20:46:17,264 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/attn/q_norm/Reshape2 ...\n",
            "2026-01-05 20:46:17,264 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/attn/k_norm/Reshape1 ...\n",
            "2026-01-05 20:46:17,264 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/attn/k_norm/LayerNorm ...\n",
            "2026-01-05 20:46:17,265 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/attn/k_norm/Reshape2 ...\n",
            "2026-01-05 20:46:17,265 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/attn/q_rotary ...\n",
            "2026-01-05 20:46:17,265 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/attn/k_rotary ...\n",
            "2026-01-05 20:46:17,265 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/attn/GroupQueryAttention ...\n",
            "2026-01-05 20:46:17,265 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/attn/o_proj/MatMul ...\n",
            "2026-01-05 20:46:17,275 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/attn/o_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,275 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/post_attention_layernorm ...\n",
            "2026-01-05 20:46:17,275 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/Add_1 ...\n",
            "2026-01-05 20:46:17,275 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/pre_feedforward_layernorm ...\n",
            "2026-01-05 20:46:17,275 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/mlp/gate_proj/MatMul ...\n",
            "2026-01-05 20:46:17,300 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,300 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/mlp/up_proj/MatMul ...\n",
            "2026-01-05 20:46:17,323 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/mlp/up_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,323 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/mlp/act_fn/FastGelu ...\n",
            "2026-01-05 20:46:17,323 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/mlp/Mul ...\n",
            "2026-01-05 20:46:17,324 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/mlp/down_proj/MatMul ...\n",
            "2026-01-05 20:46:17,344 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/mlp/down_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,345 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/post_feedforward_layernorm ...\n",
            "2026-01-05 20:46:17,345 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/Add_2 ...\n",
            "2026-01-05 20:46:17,345 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/input_layernorm ...\n",
            "2026-01-05 20:46:17,345 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/attn/q_proj/MatMul ...\n",
            "2026-01-05 20:46:17,355 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/attn/q_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,355 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/attn/k_proj/MatMul ...\n",
            "2026-01-05 20:46:17,358 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/attn/k_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,358 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/attn/v_proj/MatMul ...\n",
            "2026-01-05 20:46:17,361 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/attn/v_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,361 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/attn/q_norm/Reshape1 ...\n",
            "2026-01-05 20:46:17,361 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/attn/q_norm/LayerNorm ...\n",
            "2026-01-05 20:46:17,361 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/attn/q_norm/Reshape2 ...\n",
            "2026-01-05 20:46:17,361 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/attn/k_norm/Reshape1 ...\n",
            "2026-01-05 20:46:17,361 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/attn/k_norm/LayerNorm ...\n",
            "2026-01-05 20:46:17,361 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/attn/k_norm/Reshape2 ...\n",
            "2026-01-05 20:46:17,361 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/attn/q_rotary ...\n",
            "2026-01-05 20:46:17,361 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/attn/k_rotary ...\n",
            "2026-01-05 20:46:17,362 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/attn/GroupQueryAttention ...\n",
            "2026-01-05 20:46:17,362 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/attn/o_proj/MatMul ...\n",
            "2026-01-05 20:46:17,372 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/attn/o_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,372 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/post_attention_layernorm ...\n",
            "2026-01-05 20:46:17,372 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/Add_1 ...\n",
            "2026-01-05 20:46:17,372 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/pre_feedforward_layernorm ...\n",
            "2026-01-05 20:46:17,372 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/mlp/gate_proj/MatMul ...\n",
            "2026-01-05 20:46:17,396 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,396 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/mlp/up_proj/MatMul ...\n",
            "2026-01-05 20:46:17,416 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/mlp/up_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,416 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/mlp/act_fn/FastGelu ...\n",
            "2026-01-05 20:46:17,416 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/mlp/Mul ...\n",
            "2026-01-05 20:46:17,416 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/mlp/down_proj/MatMul ...\n",
            "2026-01-05 20:46:17,435 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/mlp/down_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,435 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/post_feedforward_layernorm ...\n",
            "2026-01-05 20:46:17,435 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/Add_2 ...\n",
            "2026-01-05 20:46:17,435 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/input_layernorm ...\n",
            "2026-01-05 20:46:17,435 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/attn/q_proj/MatMul ...\n",
            "2026-01-05 20:46:17,446 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/attn/q_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,446 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/attn/k_proj/MatMul ...\n",
            "2026-01-05 20:46:17,448 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/attn/k_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,449 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/attn/v_proj/MatMul ...\n",
            "2026-01-05 20:46:17,451 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/attn/v_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,451 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/attn/q_norm/Reshape1 ...\n",
            "2026-01-05 20:46:17,451 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/attn/q_norm/LayerNorm ...\n",
            "2026-01-05 20:46:17,452 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/attn/q_norm/Reshape2 ...\n",
            "2026-01-05 20:46:17,452 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/attn/k_norm/Reshape1 ...\n",
            "2026-01-05 20:46:17,452 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/attn/k_norm/LayerNorm ...\n",
            "2026-01-05 20:46:17,452 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/attn/k_norm/Reshape2 ...\n",
            "2026-01-05 20:46:17,452 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/attn/q_rotary ...\n",
            "2026-01-05 20:46:17,452 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/attn/k_rotary ...\n",
            "2026-01-05 20:46:17,452 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/attn/GroupQueryAttention ...\n",
            "2026-01-05 20:46:17,452 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/attn/o_proj/MatMul ...\n",
            "2026-01-05 20:46:17,462 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/attn/o_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,462 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/post_attention_layernorm ...\n",
            "2026-01-05 20:46:17,462 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/Add_1 ...\n",
            "2026-01-05 20:46:17,462 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/pre_feedforward_layernorm ...\n",
            "2026-01-05 20:46:17,462 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/mlp/gate_proj/MatMul ...\n",
            "2026-01-05 20:46:17,486 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,486 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/mlp/up_proj/MatMul ...\n",
            "2026-01-05 20:46:17,507 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/mlp/up_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,507 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/mlp/act_fn/FastGelu ...\n",
            "2026-01-05 20:46:17,507 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/mlp/Mul ...\n",
            "2026-01-05 20:46:17,507 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/mlp/down_proj/MatMul ...\n",
            "2026-01-05 20:46:17,527 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/mlp/down_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,527 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/post_feedforward_layernorm ...\n",
            "2026-01-05 20:46:17,527 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/Add_2 ...\n",
            "2026-01-05 20:46:17,527 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/input_layernorm ...\n",
            "2026-01-05 20:46:17,527 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/attn/q_proj/MatMul ...\n",
            "2026-01-05 20:46:17,537 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/attn/q_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,537 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/attn/k_proj/MatMul ...\n",
            "2026-01-05 20:46:17,539 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/attn/k_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,539 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/attn/v_proj/MatMul ...\n",
            "2026-01-05 20:46:17,542 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/attn/v_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,542 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/attn/q_norm/Reshape1 ...\n",
            "2026-01-05 20:46:17,542 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/attn/q_norm/LayerNorm ...\n",
            "2026-01-05 20:46:17,542 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/attn/q_norm/Reshape2 ...\n",
            "2026-01-05 20:46:17,542 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/attn/k_norm/Reshape1 ...\n",
            "2026-01-05 20:46:17,542 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/attn/k_norm/LayerNorm ...\n",
            "2026-01-05 20:46:17,542 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/attn/k_norm/Reshape2 ...\n",
            "2026-01-05 20:46:17,542 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/attn/q_rotary ...\n",
            "2026-01-05 20:46:17,542 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/attn/k_rotary ...\n",
            "2026-01-05 20:46:17,542 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/attn/GroupQueryAttention ...\n",
            "2026-01-05 20:46:17,542 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/attn/o_proj/MatMul ...\n",
            "2026-01-05 20:46:17,552 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/attn/o_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,552 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/post_attention_layernorm ...\n",
            "2026-01-05 20:46:17,552 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/Add_1 ...\n",
            "2026-01-05 20:46:17,552 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/pre_feedforward_layernorm ...\n",
            "2026-01-05 20:46:17,552 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/mlp/gate_proj/MatMul ...\n",
            "2026-01-05 20:46:17,576 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,576 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/mlp/up_proj/MatMul ...\n",
            "2026-01-05 20:46:17,596 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/mlp/up_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,596 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/mlp/act_fn/FastGelu ...\n",
            "2026-01-05 20:46:17,596 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/mlp/Mul ...\n",
            "2026-01-05 20:46:17,596 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/mlp/down_proj/MatMul ...\n",
            "2026-01-05 20:46:17,614 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/mlp/down_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,614 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/post_feedforward_layernorm ...\n",
            "2026-01-05 20:46:17,614 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/Add_2 ...\n",
            "2026-01-05 20:46:17,614 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/input_layernorm ...\n",
            "2026-01-05 20:46:17,614 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/attn/q_proj/MatMul ...\n",
            "2026-01-05 20:46:17,623 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/attn/q_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,623 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/attn/k_proj/MatMul ...\n",
            "2026-01-05 20:46:17,626 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/attn/k_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,626 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/attn/v_proj/MatMul ...\n",
            "2026-01-05 20:46:17,628 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/attn/v_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,628 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/attn/q_norm/Reshape1 ...\n",
            "2026-01-05 20:46:17,628 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/attn/q_norm/LayerNorm ...\n",
            "2026-01-05 20:46:17,628 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/attn/q_norm/Reshape2 ...\n",
            "2026-01-05 20:46:17,628 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/attn/k_norm/Reshape1 ...\n",
            "2026-01-05 20:46:17,628 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/attn/k_norm/LayerNorm ...\n",
            "2026-01-05 20:46:17,628 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/attn/k_norm/Reshape2 ...\n",
            "2026-01-05 20:46:17,628 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/attn/q_rotary ...\n",
            "2026-01-05 20:46:17,628 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/attn/k_rotary ...\n",
            "2026-01-05 20:46:17,629 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/attn/GroupQueryAttention ...\n",
            "2026-01-05 20:46:17,629 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/attn/o_proj/MatMul ...\n",
            "2026-01-05 20:46:17,637 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/attn/o_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,637 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/post_attention_layernorm ...\n",
            "2026-01-05 20:46:17,638 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/Add_1 ...\n",
            "2026-01-05 20:46:17,638 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/pre_feedforward_layernorm ...\n",
            "2026-01-05 20:46:17,638 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/mlp/gate_proj/MatMul ...\n",
            "2026-01-05 20:46:17,662 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,662 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/mlp/up_proj/MatMul ...\n",
            "2026-01-05 20:46:17,685 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/mlp/up_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,685 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/mlp/act_fn/FastGelu ...\n",
            "2026-01-05 20:46:17,685 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/mlp/Mul ...\n",
            "2026-01-05 20:46:17,685 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/mlp/down_proj/MatMul ...\n",
            "2026-01-05 20:46:17,703 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/mlp/down_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,703 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/post_feedforward_layernorm ...\n",
            "2026-01-05 20:46:17,703 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/Add_2 ...\n",
            "2026-01-05 20:46:17,703 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/input_layernorm ...\n",
            "2026-01-05 20:46:17,703 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/attn/q_proj/MatMul ...\n",
            "2026-01-05 20:46:17,712 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/attn/q_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,712 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/attn/k_proj/MatMul ...\n",
            "2026-01-05 20:46:17,715 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/attn/k_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,715 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/attn/v_proj/MatMul ...\n",
            "2026-01-05 20:46:17,717 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/attn/v_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,717 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/attn/q_norm/Reshape1 ...\n",
            "2026-01-05 20:46:17,717 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/attn/q_norm/LayerNorm ...\n",
            "2026-01-05 20:46:17,718 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/attn/q_norm/Reshape2 ...\n",
            "2026-01-05 20:46:17,718 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/attn/k_norm/Reshape1 ...\n",
            "2026-01-05 20:46:17,718 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/attn/k_norm/LayerNorm ...\n",
            "2026-01-05 20:46:17,718 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/attn/k_norm/Reshape2 ...\n",
            "2026-01-05 20:46:17,718 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/attn/q_rotary ...\n",
            "2026-01-05 20:46:17,718 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/attn/k_rotary ...\n",
            "2026-01-05 20:46:17,718 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/attn/GroupQueryAttention ...\n",
            "2026-01-05 20:46:17,718 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/attn/o_proj/MatMul ...\n",
            "2026-01-05 20:46:17,727 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/attn/o_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,727 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/post_attention_layernorm ...\n",
            "2026-01-05 20:46:17,727 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/Add_1 ...\n",
            "2026-01-05 20:46:17,727 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/pre_feedforward_layernorm ...\n",
            "2026-01-05 20:46:17,727 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/mlp/gate_proj/MatMul ...\n",
            "2026-01-05 20:46:17,747 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,748 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/mlp/up_proj/MatMul ...\n",
            "2026-01-05 20:46:17,768 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/mlp/up_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,768 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/mlp/act_fn/FastGelu ...\n",
            "2026-01-05 20:46:17,768 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/mlp/Mul ...\n",
            "2026-01-05 20:46:17,768 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/mlp/down_proj/MatMul ...\n",
            "2026-01-05 20:46:17,786 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/mlp/down_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,786 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/post_feedforward_layernorm ...\n",
            "2026-01-05 20:46:17,786 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/Add_2 ...\n",
            "2026-01-05 20:46:17,786 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/input_layernorm ...\n",
            "2026-01-05 20:46:17,786 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/attn/q_proj/MatMul ...\n",
            "2026-01-05 20:46:17,795 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/attn/q_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,795 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/attn/k_proj/MatMul ...\n",
            "2026-01-05 20:46:17,798 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/attn/k_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,798 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/attn/v_proj/MatMul ...\n",
            "2026-01-05 20:46:17,800 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/attn/v_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,800 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/attn/q_norm/Reshape1 ...\n",
            "2026-01-05 20:46:17,800 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/attn/q_norm/LayerNorm ...\n",
            "2026-01-05 20:46:17,800 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/attn/q_norm/Reshape2 ...\n",
            "2026-01-05 20:46:17,800 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/attn/k_norm/Reshape1 ...\n",
            "2026-01-05 20:46:17,800 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/attn/k_norm/LayerNorm ...\n",
            "2026-01-05 20:46:17,801 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/attn/k_norm/Reshape2 ...\n",
            "2026-01-05 20:46:17,801 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/attn/q_rotary ...\n",
            "2026-01-05 20:46:17,801 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/attn/k_rotary ...\n",
            "2026-01-05 20:46:17,801 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/attn/GroupQueryAttention ...\n",
            "2026-01-05 20:46:17,801 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/attn/o_proj/MatMul ...\n",
            "2026-01-05 20:46:17,810 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/attn/o_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,810 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/post_attention_layernorm ...\n",
            "2026-01-05 20:46:17,810 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/Add_1 ...\n",
            "2026-01-05 20:46:17,810 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/pre_feedforward_layernorm ...\n",
            "2026-01-05 20:46:17,810 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/mlp/gate_proj/MatMul ...\n",
            "2026-01-05 20:46:17,830 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,830 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/mlp/up_proj/MatMul ...\n",
            "2026-01-05 20:46:17,850 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/mlp/up_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,850 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/mlp/act_fn/FastGelu ...\n",
            "2026-01-05 20:46:17,851 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/mlp/Mul ...\n",
            "2026-01-05 20:46:17,851 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/mlp/down_proj/MatMul ...\n",
            "2026-01-05 20:46:17,868 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/mlp/down_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,868 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/post_feedforward_layernorm ...\n",
            "2026-01-05 20:46:17,868 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/Add_2 ...\n",
            "2026-01-05 20:46:17,868 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/input_layernorm ...\n",
            "2026-01-05 20:46:17,869 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/attn/q_proj/MatMul ...\n",
            "2026-01-05 20:46:17,878 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/attn/q_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,878 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/attn/k_proj/MatMul ...\n",
            "2026-01-05 20:46:17,881 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/attn/k_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,881 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/attn/v_proj/MatMul ...\n",
            "2026-01-05 20:46:17,884 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/attn/v_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,884 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/attn/q_norm/Reshape1 ...\n",
            "2026-01-05 20:46:17,884 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/attn/q_norm/LayerNorm ...\n",
            "2026-01-05 20:46:17,884 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/attn/q_norm/Reshape2 ...\n",
            "2026-01-05 20:46:17,885 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/attn/k_norm/Reshape1 ...\n",
            "2026-01-05 20:46:17,885 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/attn/k_norm/LayerNorm ...\n",
            "2026-01-05 20:46:17,885 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/attn/k_norm/Reshape2 ...\n",
            "2026-01-05 20:46:17,885 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/attn/q_rotary ...\n",
            "2026-01-05 20:46:17,885 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/attn/k_rotary ...\n",
            "2026-01-05 20:46:17,885 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/attn/GroupQueryAttention ...\n",
            "2026-01-05 20:46:17,885 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/attn/o_proj/MatMul ...\n",
            "2026-01-05 20:46:17,895 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/attn/o_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,895 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/post_attention_layernorm ...\n",
            "2026-01-05 20:46:17,895 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/Add_1 ...\n",
            "2026-01-05 20:46:17,895 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/pre_feedforward_layernorm ...\n",
            "2026-01-05 20:46:17,896 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/mlp/gate_proj/MatMul ...\n",
            "2026-01-05 20:46:17,916 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,916 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/mlp/up_proj/MatMul ...\n",
            "2026-01-05 20:46:17,936 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/mlp/up_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,936 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/mlp/act_fn/FastGelu ...\n",
            "2026-01-05 20:46:17,936 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/mlp/Mul ...\n",
            "2026-01-05 20:46:17,936 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/mlp/down_proj/MatMul ...\n",
            "2026-01-05 20:46:17,954 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/mlp/down_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,954 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/post_feedforward_layernorm ...\n",
            "2026-01-05 20:46:17,954 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/Add_2 ...\n",
            "2026-01-05 20:46:17,954 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/input_layernorm ...\n",
            "2026-01-05 20:46:17,954 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/attn/q_proj/MatMul ...\n",
            "2026-01-05 20:46:17,963 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/attn/q_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,963 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/attn/k_proj/MatMul ...\n",
            "2026-01-05 20:46:17,965 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/attn/k_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,966 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/attn/v_proj/MatMul ...\n",
            "2026-01-05 20:46:17,968 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/attn/v_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,968 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/attn/q_norm/Reshape1 ...\n",
            "2026-01-05 20:46:17,968 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/attn/q_norm/LayerNorm ...\n",
            "2026-01-05 20:46:17,968 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/attn/q_norm/Reshape2 ...\n",
            "2026-01-05 20:46:17,968 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/attn/k_norm/Reshape1 ...\n",
            "2026-01-05 20:46:17,968 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/attn/k_norm/LayerNorm ...\n",
            "2026-01-05 20:46:17,968 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/attn/k_norm/Reshape2 ...\n",
            "2026-01-05 20:46:17,968 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/attn/q_rotary ...\n",
            "2026-01-05 20:46:17,968 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/attn/k_rotary ...\n",
            "2026-01-05 20:46:17,968 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/attn/GroupQueryAttention ...\n",
            "2026-01-05 20:46:17,968 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/attn/o_proj/MatMul ...\n",
            "2026-01-05 20:46:17,977 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/attn/o_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:17,977 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/post_attention_layernorm ...\n",
            "2026-01-05 20:46:17,978 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/Add_1 ...\n",
            "2026-01-05 20:46:17,978 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/pre_feedforward_layernorm ...\n",
            "2026-01-05 20:46:17,978 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/mlp/gate_proj/MatMul ...\n",
            "2026-01-05 20:46:18,000 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:18,000 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/mlp/up_proj/MatMul ...\n",
            "2026-01-05 20:46:18,019 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/mlp/up_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:18,019 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/mlp/act_fn/FastGelu ...\n",
            "2026-01-05 20:46:18,019 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/mlp/Mul ...\n",
            "2026-01-05 20:46:18,019 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/mlp/down_proj/MatMul ...\n",
            "2026-01-05 20:46:18,037 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/mlp/down_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:18,037 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/post_feedforward_layernorm ...\n",
            "2026-01-05 20:46:18,037 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/Add_2 ...\n",
            "2026-01-05 20:46:18,037 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/input_layernorm ...\n",
            "2026-01-05 20:46:18,037 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/attn/q_proj/MatMul ...\n",
            "2026-01-05 20:46:18,047 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/attn/q_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:18,047 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/attn/k_proj/MatMul ...\n",
            "2026-01-05 20:46:18,049 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/attn/k_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:18,049 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/attn/v_proj/MatMul ...\n",
            "2026-01-05 20:46:18,051 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/attn/v_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:18,052 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/attn/q_norm/Reshape1 ...\n",
            "2026-01-05 20:46:18,052 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/attn/q_norm/LayerNorm ...\n",
            "2026-01-05 20:46:18,052 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/attn/q_norm/Reshape2 ...\n",
            "2026-01-05 20:46:18,052 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/attn/k_norm/Reshape1 ...\n",
            "2026-01-05 20:46:18,052 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/attn/k_norm/LayerNorm ...\n",
            "2026-01-05 20:46:18,052 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/attn/k_norm/Reshape2 ...\n",
            "2026-01-05 20:46:18,052 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/attn/q_rotary ...\n",
            "2026-01-05 20:46:18,052 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/attn/k_rotary ...\n",
            "2026-01-05 20:46:18,052 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/attn/GroupQueryAttention ...\n",
            "2026-01-05 20:46:18,052 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/attn/o_proj/MatMul ...\n",
            "2026-01-05 20:46:18,064 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/attn/o_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:18,064 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/post_attention_layernorm ...\n",
            "2026-01-05 20:46:18,065 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/Add_1 ...\n",
            "2026-01-05 20:46:18,065 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/pre_feedforward_layernorm ...\n",
            "2026-01-05 20:46:18,065 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/mlp/gate_proj/MatMul ...\n",
            "2026-01-05 20:46:18,094 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:18,094 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/mlp/up_proj/MatMul ...\n",
            "2026-01-05 20:46:18,113 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/mlp/up_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:18,113 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/mlp/act_fn/FastGelu ...\n",
            "2026-01-05 20:46:18,113 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/mlp/Mul ...\n",
            "2026-01-05 20:46:18,113 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/mlp/down_proj/MatMul ...\n",
            "2026-01-05 20:46:18,131 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/mlp/down_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:18,131 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/post_feedforward_layernorm ...\n",
            "2026-01-05 20:46:18,131 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/Add_2 ...\n",
            "2026-01-05 20:46:18,131 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/input_layernorm ...\n",
            "2026-01-05 20:46:18,131 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/attn/q_proj/MatMul ...\n",
            "2026-01-05 20:46:18,140 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/attn/q_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:18,141 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/attn/k_proj/MatMul ...\n",
            "2026-01-05 20:46:18,143 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/attn/k_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:18,143 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/attn/v_proj/MatMul ...\n",
            "2026-01-05 20:46:18,145 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/attn/v_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:18,145 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/attn/q_norm/Reshape1 ...\n",
            "2026-01-05 20:46:18,145 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/attn/q_norm/LayerNorm ...\n",
            "2026-01-05 20:46:18,146 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/attn/q_norm/Reshape2 ...\n",
            "2026-01-05 20:46:18,146 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/attn/k_norm/Reshape1 ...\n",
            "2026-01-05 20:46:18,146 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/attn/k_norm/LayerNorm ...\n",
            "2026-01-05 20:46:18,146 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/attn/k_norm/Reshape2 ...\n",
            "2026-01-05 20:46:18,146 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/attn/q_rotary ...\n",
            "2026-01-05 20:46:18,146 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/attn/k_rotary ...\n",
            "2026-01-05 20:46:18,146 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/attn/GroupQueryAttention ...\n",
            "2026-01-05 20:46:18,146 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/attn/o_proj/MatMul ...\n",
            "2026-01-05 20:46:18,155 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/attn/o_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:18,155 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/post_attention_layernorm ...\n",
            "2026-01-05 20:46:18,155 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/Add_1 ...\n",
            "2026-01-05 20:46:18,155 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/pre_feedforward_layernorm ...\n",
            "2026-01-05 20:46:18,155 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/mlp/gate_proj/MatMul ...\n",
            "2026-01-05 20:46:18,176 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:18,176 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/mlp/up_proj/MatMul ...\n",
            "2026-01-05 20:46:18,199 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/mlp/up_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:18,199 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/mlp/act_fn/FastGelu ...\n",
            "2026-01-05 20:46:18,199 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/mlp/Mul ...\n",
            "2026-01-05 20:46:18,199 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/mlp/down_proj/MatMul ...\n",
            "2026-01-05 20:46:18,217 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/mlp/down_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:18,217 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/post_feedforward_layernorm ...\n",
            "2026-01-05 20:46:18,217 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/Add_2 ...\n",
            "2026-01-05 20:46:18,217 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/input_layernorm ...\n",
            "2026-01-05 20:46:18,217 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/attn/q_proj/MatMul ...\n",
            "2026-01-05 20:46:18,227 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/attn/q_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:18,227 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/attn/k_proj/MatMul ...\n",
            "2026-01-05 20:46:18,230 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/attn/k_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:18,230 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/attn/v_proj/MatMul ...\n",
            "2026-01-05 20:46:18,233 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/attn/v_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:18,233 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/attn/q_norm/Reshape1 ...\n",
            "2026-01-05 20:46:18,233 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/attn/q_norm/LayerNorm ...\n",
            "2026-01-05 20:46:18,233 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/attn/q_norm/Reshape2 ...\n",
            "2026-01-05 20:46:18,233 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/attn/k_norm/Reshape1 ...\n",
            "2026-01-05 20:46:18,233 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/attn/k_norm/LayerNorm ...\n",
            "2026-01-05 20:46:18,233 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/attn/k_norm/Reshape2 ...\n",
            "2026-01-05 20:46:18,233 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/attn/q_rotary ...\n",
            "2026-01-05 20:46:18,233 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/attn/k_rotary ...\n",
            "2026-01-05 20:46:18,233 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/attn/GroupQueryAttention ...\n",
            "2026-01-05 20:46:18,233 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/attn/o_proj/MatMul ...\n",
            "2026-01-05 20:46:18,243 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/attn/o_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:18,243 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/post_attention_layernorm ...\n",
            "2026-01-05 20:46:18,243 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/Add_1 ...\n",
            "2026-01-05 20:46:18,243 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/pre_feedforward_layernorm ...\n",
            "2026-01-05 20:46:18,243 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/mlp/gate_proj/MatMul ...\n",
            "2026-01-05 20:46:18,264 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:18,264 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/mlp/up_proj/MatMul ...\n",
            "2026-01-05 20:46:18,285 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/mlp/up_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:18,285 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/mlp/act_fn/FastGelu ...\n",
            "2026-01-05 20:46:18,285 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/mlp/Mul ...\n",
            "2026-01-05 20:46:18,285 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/mlp/down_proj/MatMul ...\n",
            "2026-01-05 20:46:18,303 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/mlp/down_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:18,303 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/post_feedforward_layernorm ...\n",
            "2026-01-05 20:46:18,303 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/Add_2 ...\n",
            "2026-01-05 20:46:18,303 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/input_layernorm ...\n",
            "2026-01-05 20:46:18,303 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/attn/q_proj/MatMul ...\n",
            "2026-01-05 20:46:18,313 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/attn/q_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:18,313 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/attn/k_proj/MatMul ...\n",
            "2026-01-05 20:46:18,315 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/attn/k_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:18,315 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/attn/v_proj/MatMul ...\n",
            "2026-01-05 20:46:18,318 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/attn/v_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:18,318 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/attn/q_norm/Reshape1 ...\n",
            "2026-01-05 20:46:18,318 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/attn/q_norm/LayerNorm ...\n",
            "2026-01-05 20:46:18,318 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/attn/q_norm/Reshape2 ...\n",
            "2026-01-05 20:46:18,318 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/attn/k_norm/Reshape1 ...\n",
            "2026-01-05 20:46:18,318 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/attn/k_norm/LayerNorm ...\n",
            "2026-01-05 20:46:18,318 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/attn/k_norm/Reshape2 ...\n",
            "2026-01-05 20:46:18,318 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/attn/q_rotary ...\n",
            "2026-01-05 20:46:18,318 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/attn/k_rotary ...\n",
            "2026-01-05 20:46:18,318 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/attn/GroupQueryAttention ...\n",
            "2026-01-05 20:46:18,318 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/attn/o_proj/MatMul ...\n",
            "2026-01-05 20:46:18,327 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/attn/o_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:18,327 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/post_attention_layernorm ...\n",
            "2026-01-05 20:46:18,327 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/Add_1 ...\n",
            "2026-01-05 20:46:18,327 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/pre_feedforward_layernorm ...\n",
            "2026-01-05 20:46:18,327 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/mlp/gate_proj/MatMul ...\n",
            "2026-01-05 20:46:18,348 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:18,348 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/mlp/up_proj/MatMul ...\n",
            "2026-01-05 20:46:18,368 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/mlp/up_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:18,368 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/mlp/act_fn/FastGelu ...\n",
            "2026-01-05 20:46:18,368 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/mlp/Mul ...\n",
            "2026-01-05 20:46:18,368 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/mlp/down_proj/MatMul ...\n",
            "2026-01-05 20:46:18,388 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/mlp/down_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:18,388 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/post_feedforward_layernorm ...\n",
            "2026-01-05 20:46:18,388 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/Add_2 ...\n",
            "2026-01-05 20:46:18,388 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/final_norm ...\n",
            "2026-01-05 20:46:18,388 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /lm_head/Transpose ...\n",
            "2026-01-05 20:46:18,388 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /lm_head/MatMul ...\n",
            "2026-01-05 20:46:18,388 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - MatMul doesn't have const weight. Skip to quantize\n",
            "2026-01-05 20:46:18,388 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /lm_head/MatMul with 4 bits ...\n",
            "Loading PyTorch model...\n",
            "Building embedding layer...\n",
            "Building layer 0/17...\n",
            "Building layer 1/17...\n",
            "Building layer 2/17...\n",
            "Building layer 3/17...\n",
            "Building layer 4/17...\n",
            "Building layer 5/17...\n",
            "Building layer 6/17...\n",
            "Building layer 7/17...\n",
            "Building layer 8/17...\n",
            "Building layer 9/17...\n",
            "Building layer 10/17...\n",
            "Building layer 11/17...\n",
            "Building layer 12/17...\n",
            "Building layer 13/17...\n",
            "Building layer 14/17...\n",
            "Building layer 15/17...\n",
            "Building layer 16/17...\n",
            "Building layer 17/17...\n",
            "Building final normalization layer...\n",
            "Building LM head...\n",
            "ONNX model construction complete.\n",
            "2026-01-05 20:46:33,721 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/constant_nodes/INT64/[1] ...\n",
            "2026-01-05 20:46:33,721 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/preprocessing/attn_mask/ReduceSum ...\n",
            "2026-01-05 20:46:33,721 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/preprocessing/attn_mask/Sub ...\n",
            "2026-01-05 20:46:33,721 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/preprocessing/attn_mask/seqlens_k_cast/Cast ...\n",
            "2026-01-05 20:46:33,722 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/preprocessing/attn_mask/Shape ...\n",
            "2026-01-05 20:46:33,722 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/constant_nodes/INT64/1 ...\n",
            "2026-01-05 20:46:33,722 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/preprocessing/attn_mask/Gather ...\n",
            "2026-01-05 20:46:33,722 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/preprocessing/attn_mask/total_seq_len_cast/Cast ...\n",
            "2026-01-05 20:46:33,722 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/preprocessing/pos_ids/Shape ...\n",
            "2026-01-05 20:46:33,722 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/preprocessing/pos_ids/Gather ...\n",
            "2026-01-05 20:46:33,722 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/constant_nodes/INT64/[0] ...\n",
            "2026-01-05 20:46:33,722 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/preprocessing/pos_ids/Unsqueeze ...\n",
            "2026-01-05 20:46:33,722 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/constant_nodes/INT64/[-1] ...\n",
            "2026-01-05 20:46:33,722 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/preprocessing/pos_ids/Concat ...\n",
            "2026-01-05 20:46:33,722 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/preprocessing/pos_ids/Reshape ...\n",
            "2026-01-05 20:46:33,722 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/embed_tokens/Gather ...\n",
            "2026-01-05 20:46:33,722 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/constant_nodes/FLOAT16/25.298221281347036 ...\n",
            "2026-01-05 20:46:33,722 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/embed_tokens/Mul ...\n",
            "2026-01-05 20:46:33,722 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/input_layernorm/Cast ...\n",
            "2026-01-05 20:46:33,722 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/input_layernorm ...\n",
            "2026-01-05 20:46:33,722 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/input_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:33,722 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/attn/q_proj/MatMul ...\n",
            "2026-01-05 20:46:33,735 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/attn/q_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:33,736 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/attn/k_proj/MatMul ...\n",
            "2026-01-05 20:46:33,739 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/attn/k_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:33,739 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/attn/v_proj/MatMul ...\n",
            "2026-01-05 20:46:33,742 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/attn/v_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:33,742 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/constant_nodes/INT64/[0, -1, 256] ...\n",
            "2026-01-05 20:46:33,742 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/attn/q_norm/Reshape1 ...\n",
            "2026-01-05 20:46:33,742 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/attn/q_norm/LayerNorm/Cast ...\n",
            "2026-01-05 20:46:33,742 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/attn/q_norm/LayerNorm ...\n",
            "2026-01-05 20:46:33,742 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/attn/q_norm/LayerNorm/output/Cast ...\n",
            "2026-01-05 20:46:33,742 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/constant_nodes/INT64/[0, -1, 1024] ...\n",
            "2026-01-05 20:46:33,742 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/attn/q_norm/Reshape2 ...\n",
            "2026-01-05 20:46:33,742 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/attn/k_norm/Reshape1 ...\n",
            "2026-01-05 20:46:33,742 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/attn/k_norm/LayerNorm/Cast ...\n",
            "2026-01-05 20:46:33,742 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/attn/k_norm/LayerNorm ...\n",
            "2026-01-05 20:46:33,742 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/attn/k_norm/LayerNorm/output/Cast ...\n",
            "2026-01-05 20:46:33,742 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/attn/k_norm/Reshape2 ...\n",
            "2026-01-05 20:46:33,743 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/attn/q_rotary ...\n",
            "2026-01-05 20:46:33,743 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/attn/k_rotary ...\n",
            "2026-01-05 20:46:33,743 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/attn/GroupQueryAttention ...\n",
            "2026-01-05 20:46:33,743 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/attn/o_proj/MatMul ...\n",
            "2026-01-05 20:46:33,755 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/attn/o_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:33,755 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/post_attention_layernorm/Cast ...\n",
            "2026-01-05 20:46:33,755 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/post_attention_layernorm ...\n",
            "2026-01-05 20:46:33,755 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/post_attention_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:33,755 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/Add_1 ...\n",
            "2026-01-05 20:46:33,755 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/pre_feedforward_layernorm/Cast ...\n",
            "2026-01-05 20:46:33,755 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/pre_feedforward_layernorm ...\n",
            "2026-01-05 20:46:33,755 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/pre_feedforward_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:33,755 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/mlp/gate_proj/MatMul ...\n",
            "2026-01-05 20:46:33,782 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:33,782 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/mlp/up_proj/MatMul ...\n",
            "2026-01-05 20:46:33,808 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/mlp/up_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:33,808 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/mlp/act_fn/FastGelu ...\n",
            "2026-01-05 20:46:33,808 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/mlp/Mul ...\n",
            "2026-01-05 20:46:33,808 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.0/mlp/down_proj/MatMul ...\n",
            "2026-01-05 20:46:33,832 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.0/mlp/down_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:33,832 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/post_feedforward_layernorm/Cast ...\n",
            "2026-01-05 20:46:33,832 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/post_feedforward_layernorm ...\n",
            "2026-01-05 20:46:33,832 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/post_feedforward_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:33,832 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.0/Add_2 ...\n",
            "2026-01-05 20:46:33,832 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/input_layernorm/Cast ...\n",
            "2026-01-05 20:46:33,832 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/input_layernorm ...\n",
            "2026-01-05 20:46:33,832 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/input_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:33,832 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/attn/q_proj/MatMul ...\n",
            "2026-01-05 20:46:33,845 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/attn/q_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:33,845 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/attn/k_proj/MatMul ...\n",
            "2026-01-05 20:46:33,848 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/attn/k_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:33,848 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/attn/v_proj/MatMul ...\n",
            "2026-01-05 20:46:33,852 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/attn/v_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:33,852 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/attn/q_norm/Reshape1 ...\n",
            "2026-01-05 20:46:33,852 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/attn/q_norm/LayerNorm/Cast ...\n",
            "2026-01-05 20:46:33,852 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/attn/q_norm/LayerNorm ...\n",
            "2026-01-05 20:46:33,852 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/attn/q_norm/LayerNorm/output/Cast ...\n",
            "2026-01-05 20:46:33,852 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/attn/q_norm/Reshape2 ...\n",
            "2026-01-05 20:46:33,852 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/attn/k_norm/Reshape1 ...\n",
            "2026-01-05 20:46:33,852 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/attn/k_norm/LayerNorm/Cast ...\n",
            "2026-01-05 20:46:33,852 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/attn/k_norm/LayerNorm ...\n",
            "2026-01-05 20:46:33,852 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/attn/k_norm/LayerNorm/output/Cast ...\n",
            "2026-01-05 20:46:33,852 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/attn/k_norm/Reshape2 ...\n",
            "2026-01-05 20:46:33,852 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/attn/q_rotary ...\n",
            "2026-01-05 20:46:33,852 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/attn/k_rotary ...\n",
            "2026-01-05 20:46:33,852 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/attn/GroupQueryAttention ...\n",
            "2026-01-05 20:46:33,852 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/attn/o_proj/MatMul ...\n",
            "2026-01-05 20:46:33,864 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/attn/o_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:33,864 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/post_attention_layernorm/Cast ...\n",
            "2026-01-05 20:46:33,864 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/post_attention_layernorm ...\n",
            "2026-01-05 20:46:33,864 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/post_attention_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:33,864 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/Add_1 ...\n",
            "2026-01-05 20:46:33,864 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/pre_feedforward_layernorm/Cast ...\n",
            "2026-01-05 20:46:33,864 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/pre_feedforward_layernorm ...\n",
            "2026-01-05 20:46:33,864 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/pre_feedforward_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:33,864 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/mlp/gate_proj/MatMul ...\n",
            "2026-01-05 20:46:33,891 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:33,891 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/mlp/up_proj/MatMul ...\n",
            "2026-01-05 20:46:33,917 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/mlp/up_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:33,917 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/mlp/act_fn/FastGelu ...\n",
            "2026-01-05 20:46:33,917 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/mlp/Mul ...\n",
            "2026-01-05 20:46:33,917 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.1/mlp/down_proj/MatMul ...\n",
            "2026-01-05 20:46:33,943 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.1/mlp/down_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:33,943 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/post_feedforward_layernorm/Cast ...\n",
            "2026-01-05 20:46:33,943 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/post_feedforward_layernorm ...\n",
            "2026-01-05 20:46:33,943 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/post_feedforward_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:33,943 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.1/Add_2 ...\n",
            "2026-01-05 20:46:33,943 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/input_layernorm/Cast ...\n",
            "2026-01-05 20:46:33,943 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/input_layernorm ...\n",
            "2026-01-05 20:46:33,943 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/input_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:33,943 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/attn/q_proj/MatMul ...\n",
            "2026-01-05 20:46:33,956 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/attn/q_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:33,956 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/attn/k_proj/MatMul ...\n",
            "2026-01-05 20:46:33,959 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/attn/k_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:33,959 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/attn/v_proj/MatMul ...\n",
            "2026-01-05 20:46:33,962 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/attn/v_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:33,962 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/attn/q_norm/Reshape1 ...\n",
            "2026-01-05 20:46:33,962 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/attn/q_norm/LayerNorm/Cast ...\n",
            "2026-01-05 20:46:33,962 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/attn/q_norm/LayerNorm ...\n",
            "2026-01-05 20:46:33,963 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/attn/q_norm/LayerNorm/output/Cast ...\n",
            "2026-01-05 20:46:33,963 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/attn/q_norm/Reshape2 ...\n",
            "2026-01-05 20:46:33,963 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/attn/k_norm/Reshape1 ...\n",
            "2026-01-05 20:46:33,963 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/attn/k_norm/LayerNorm/Cast ...\n",
            "2026-01-05 20:46:33,963 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/attn/k_norm/LayerNorm ...\n",
            "2026-01-05 20:46:33,963 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/attn/k_norm/LayerNorm/output/Cast ...\n",
            "2026-01-05 20:46:33,963 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/attn/k_norm/Reshape2 ...\n",
            "2026-01-05 20:46:33,963 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/attn/q_rotary ...\n",
            "2026-01-05 20:46:33,963 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/attn/k_rotary ...\n",
            "2026-01-05 20:46:33,963 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/attn/GroupQueryAttention ...\n",
            "2026-01-05 20:46:33,963 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/attn/o_proj/MatMul ...\n",
            "2026-01-05 20:46:33,974 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/attn/o_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:33,975 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/post_attention_layernorm/Cast ...\n",
            "2026-01-05 20:46:33,975 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/post_attention_layernorm ...\n",
            "2026-01-05 20:46:33,975 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/post_attention_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:33,975 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/Add_1 ...\n",
            "2026-01-05 20:46:33,975 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/pre_feedforward_layernorm/Cast ...\n",
            "2026-01-05 20:46:33,975 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/pre_feedforward_layernorm ...\n",
            "2026-01-05 20:46:33,975 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/pre_feedforward_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:33,975 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/mlp/gate_proj/MatMul ...\n",
            "2026-01-05 20:46:34,001 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,001 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/mlp/up_proj/MatMul ...\n",
            "2026-01-05 20:46:34,028 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/mlp/up_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,028 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/mlp/act_fn/FastGelu ...\n",
            "2026-01-05 20:46:34,028 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/mlp/Mul ...\n",
            "2026-01-05 20:46:34,028 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.2/mlp/down_proj/MatMul ...\n",
            "2026-01-05 20:46:34,051 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.2/mlp/down_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,051 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/post_feedforward_layernorm/Cast ...\n",
            "2026-01-05 20:46:34,051 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/post_feedforward_layernorm ...\n",
            "2026-01-05 20:46:34,051 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/post_feedforward_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:34,051 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.2/Add_2 ...\n",
            "2026-01-05 20:46:34,051 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/input_layernorm/Cast ...\n",
            "2026-01-05 20:46:34,051 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/input_layernorm ...\n",
            "2026-01-05 20:46:34,051 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/input_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:34,051 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/attn/q_proj/MatMul ...\n",
            "2026-01-05 20:46:34,064 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/attn/q_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,064 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/attn/k_proj/MatMul ...\n",
            "2026-01-05 20:46:34,067 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/attn/k_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,067 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/attn/v_proj/MatMul ...\n",
            "2026-01-05 20:46:34,071 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/attn/v_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,071 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/attn/q_norm/Reshape1 ...\n",
            "2026-01-05 20:46:34,071 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/attn/q_norm/LayerNorm/Cast ...\n",
            "2026-01-05 20:46:34,071 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/attn/q_norm/LayerNorm ...\n",
            "2026-01-05 20:46:34,071 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/attn/q_norm/LayerNorm/output/Cast ...\n",
            "2026-01-05 20:46:34,071 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/attn/q_norm/Reshape2 ...\n",
            "2026-01-05 20:46:34,071 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/attn/k_norm/Reshape1 ...\n",
            "2026-01-05 20:46:34,071 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/attn/k_norm/LayerNorm/Cast ...\n",
            "2026-01-05 20:46:34,071 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/attn/k_norm/LayerNorm ...\n",
            "2026-01-05 20:46:34,071 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/attn/k_norm/LayerNorm/output/Cast ...\n",
            "2026-01-05 20:46:34,071 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/attn/k_norm/Reshape2 ...\n",
            "2026-01-05 20:46:34,071 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/attn/q_rotary ...\n",
            "2026-01-05 20:46:34,071 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/attn/k_rotary ...\n",
            "2026-01-05 20:46:34,071 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/attn/GroupQueryAttention ...\n",
            "2026-01-05 20:46:34,071 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/attn/o_proj/MatMul ...\n",
            "2026-01-05 20:46:34,083 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/attn/o_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,083 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/post_attention_layernorm/Cast ...\n",
            "2026-01-05 20:46:34,083 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/post_attention_layernorm ...\n",
            "2026-01-05 20:46:34,083 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/post_attention_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:34,083 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/Add_1 ...\n",
            "2026-01-05 20:46:34,083 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/pre_feedforward_layernorm/Cast ...\n",
            "2026-01-05 20:46:34,083 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/pre_feedforward_layernorm ...\n",
            "2026-01-05 20:46:34,083 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/pre_feedforward_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:34,083 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/mlp/gate_proj/MatMul ...\n",
            "2026-01-05 20:46:34,109 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,109 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/mlp/up_proj/MatMul ...\n",
            "2026-01-05 20:46:34,136 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/mlp/up_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,136 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/mlp/act_fn/FastGelu ...\n",
            "2026-01-05 20:46:34,136 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/mlp/Mul ...\n",
            "2026-01-05 20:46:34,136 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.3/mlp/down_proj/MatMul ...\n",
            "2026-01-05 20:46:34,164 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.3/mlp/down_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,164 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/post_feedforward_layernorm/Cast ...\n",
            "2026-01-05 20:46:34,164 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/post_feedforward_layernorm ...\n",
            "2026-01-05 20:46:34,164 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/post_feedforward_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:34,164 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.3/Add_2 ...\n",
            "2026-01-05 20:46:34,164 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/input_layernorm/Cast ...\n",
            "2026-01-05 20:46:34,164 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/input_layernorm ...\n",
            "2026-01-05 20:46:34,164 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/input_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:34,164 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/attn/q_proj/MatMul ...\n",
            "2026-01-05 20:46:34,177 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/attn/q_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,177 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/attn/k_proj/MatMul ...\n",
            "2026-01-05 20:46:34,180 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/attn/k_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,180 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/attn/v_proj/MatMul ...\n",
            "2026-01-05 20:46:34,184 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/attn/v_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,184 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/attn/q_norm/Reshape1 ...\n",
            "2026-01-05 20:46:34,184 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/attn/q_norm/LayerNorm/Cast ...\n",
            "2026-01-05 20:46:34,184 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/attn/q_norm/LayerNorm ...\n",
            "2026-01-05 20:46:34,184 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/attn/q_norm/LayerNorm/output/Cast ...\n",
            "2026-01-05 20:46:34,184 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/attn/q_norm/Reshape2 ...\n",
            "2026-01-05 20:46:34,184 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/attn/k_norm/Reshape1 ...\n",
            "2026-01-05 20:46:34,184 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/attn/k_norm/LayerNorm/Cast ...\n",
            "2026-01-05 20:46:34,184 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/attn/k_norm/LayerNorm ...\n",
            "2026-01-05 20:46:34,184 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/attn/k_norm/LayerNorm/output/Cast ...\n",
            "2026-01-05 20:46:34,184 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/attn/k_norm/Reshape2 ...\n",
            "2026-01-05 20:46:34,184 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/attn/q_rotary ...\n",
            "2026-01-05 20:46:34,184 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/attn/k_rotary ...\n",
            "2026-01-05 20:46:34,184 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/attn/GroupQueryAttention ...\n",
            "2026-01-05 20:46:34,184 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/attn/o_proj/MatMul ...\n",
            "2026-01-05 20:46:34,196 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/attn/o_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,196 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/post_attention_layernorm/Cast ...\n",
            "2026-01-05 20:46:34,196 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/post_attention_layernorm ...\n",
            "2026-01-05 20:46:34,196 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/post_attention_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:34,196 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/Add_1 ...\n",
            "2026-01-05 20:46:34,196 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/pre_feedforward_layernorm/Cast ...\n",
            "2026-01-05 20:46:34,196 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/pre_feedforward_layernorm ...\n",
            "2026-01-05 20:46:34,196 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/pre_feedforward_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:34,196 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/mlp/gate_proj/MatMul ...\n",
            "2026-01-05 20:46:34,223 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,223 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/mlp/up_proj/MatMul ...\n",
            "2026-01-05 20:46:34,252 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/mlp/up_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,252 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/mlp/act_fn/FastGelu ...\n",
            "2026-01-05 20:46:34,252 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/mlp/Mul ...\n",
            "2026-01-05 20:46:34,252 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.4/mlp/down_proj/MatMul ...\n",
            "2026-01-05 20:46:34,275 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.4/mlp/down_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,275 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/post_feedforward_layernorm/Cast ...\n",
            "2026-01-05 20:46:34,275 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/post_feedforward_layernorm ...\n",
            "2026-01-05 20:46:34,276 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/post_feedforward_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:34,276 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.4/Add_2 ...\n",
            "2026-01-05 20:46:34,276 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/input_layernorm/Cast ...\n",
            "2026-01-05 20:46:34,276 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/input_layernorm ...\n",
            "2026-01-05 20:46:34,276 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/input_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:34,276 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/attn/q_proj/MatMul ...\n",
            "2026-01-05 20:46:34,289 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/attn/q_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,289 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/attn/k_proj/MatMul ...\n",
            "2026-01-05 20:46:34,292 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/attn/k_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,292 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/attn/v_proj/MatMul ...\n",
            "2026-01-05 20:46:34,295 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/attn/v_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,295 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/attn/q_norm/Reshape1 ...\n",
            "2026-01-05 20:46:34,295 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/attn/q_norm/LayerNorm/Cast ...\n",
            "2026-01-05 20:46:34,295 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/attn/q_norm/LayerNorm ...\n",
            "2026-01-05 20:46:34,295 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/attn/q_norm/LayerNorm/output/Cast ...\n",
            "2026-01-05 20:46:34,295 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/attn/q_norm/Reshape2 ...\n",
            "2026-01-05 20:46:34,295 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/attn/k_norm/Reshape1 ...\n",
            "2026-01-05 20:46:34,295 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/attn/k_norm/LayerNorm/Cast ...\n",
            "2026-01-05 20:46:34,295 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/attn/k_norm/LayerNorm ...\n",
            "2026-01-05 20:46:34,295 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/attn/k_norm/LayerNorm/output/Cast ...\n",
            "2026-01-05 20:46:34,295 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/attn/k_norm/Reshape2 ...\n",
            "2026-01-05 20:46:34,296 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/attn/q_rotary ...\n",
            "2026-01-05 20:46:34,296 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/attn/k_rotary ...\n",
            "2026-01-05 20:46:34,296 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/attn/GroupQueryAttention ...\n",
            "2026-01-05 20:46:34,296 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/attn/o_proj/MatMul ...\n",
            "2026-01-05 20:46:34,315 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/attn/o_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,315 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/post_attention_layernorm/Cast ...\n",
            "2026-01-05 20:46:34,315 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/post_attention_layernorm ...\n",
            "2026-01-05 20:46:34,315 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/post_attention_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:34,315 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/Add_1 ...\n",
            "2026-01-05 20:46:34,315 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/pre_feedforward_layernorm/Cast ...\n",
            "2026-01-05 20:46:34,315 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/pre_feedforward_layernorm ...\n",
            "2026-01-05 20:46:34,315 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/pre_feedforward_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:34,315 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/mlp/gate_proj/MatMul ...\n",
            "2026-01-05 20:46:34,360 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,360 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/mlp/up_proj/MatMul ...\n",
            "2026-01-05 20:46:34,402 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/mlp/up_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,403 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/mlp/act_fn/FastGelu ...\n",
            "2026-01-05 20:46:34,403 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/mlp/Mul ...\n",
            "2026-01-05 20:46:34,403 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.5/mlp/down_proj/MatMul ...\n",
            "2026-01-05 20:46:34,440 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.5/mlp/down_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,440 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/post_feedforward_layernorm/Cast ...\n",
            "2026-01-05 20:46:34,440 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/post_feedforward_layernorm ...\n",
            "2026-01-05 20:46:34,440 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/post_feedforward_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:34,440 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.5/Add_2 ...\n",
            "2026-01-05 20:46:34,440 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/input_layernorm/Cast ...\n",
            "2026-01-05 20:46:34,440 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/input_layernorm ...\n",
            "2026-01-05 20:46:34,441 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/input_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:34,441 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/attn/q_proj/MatMul ...\n",
            "2026-01-05 20:46:34,460 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/attn/q_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,460 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/attn/k_proj/MatMul ...\n",
            "2026-01-05 20:46:34,465 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/attn/k_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,465 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/attn/v_proj/MatMul ...\n",
            "2026-01-05 20:46:34,470 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/attn/v_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,470 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/attn/q_norm/Reshape1 ...\n",
            "2026-01-05 20:46:34,471 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/attn/q_norm/LayerNorm/Cast ...\n",
            "2026-01-05 20:46:34,471 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/attn/q_norm/LayerNorm ...\n",
            "2026-01-05 20:46:34,471 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/attn/q_norm/LayerNorm/output/Cast ...\n",
            "2026-01-05 20:46:34,471 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/attn/q_norm/Reshape2 ...\n",
            "2026-01-05 20:46:34,471 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/attn/k_norm/Reshape1 ...\n",
            "2026-01-05 20:46:34,471 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/attn/k_norm/LayerNorm/Cast ...\n",
            "2026-01-05 20:46:34,471 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/attn/k_norm/LayerNorm ...\n",
            "2026-01-05 20:46:34,471 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/attn/k_norm/LayerNorm/output/Cast ...\n",
            "2026-01-05 20:46:34,471 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/attn/k_norm/Reshape2 ...\n",
            "2026-01-05 20:46:34,471 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/attn/q_rotary ...\n",
            "2026-01-05 20:46:34,471 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/attn/k_rotary ...\n",
            "2026-01-05 20:46:34,471 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/attn/GroupQueryAttention ...\n",
            "2026-01-05 20:46:34,471 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/attn/o_proj/MatMul ...\n",
            "2026-01-05 20:46:34,486 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/attn/o_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,486 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/post_attention_layernorm/Cast ...\n",
            "2026-01-05 20:46:34,486 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/post_attention_layernorm ...\n",
            "2026-01-05 20:46:34,486 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/post_attention_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:34,486 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/Add_1 ...\n",
            "2026-01-05 20:46:34,486 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/pre_feedforward_layernorm/Cast ...\n",
            "2026-01-05 20:46:34,486 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/pre_feedforward_layernorm ...\n",
            "2026-01-05 20:46:34,486 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/pre_feedforward_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:34,487 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/mlp/gate_proj/MatMul ...\n",
            "2026-01-05 20:46:34,529 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,529 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/mlp/up_proj/MatMul ...\n",
            "2026-01-05 20:46:34,577 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/mlp/up_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,577 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/mlp/act_fn/FastGelu ...\n",
            "2026-01-05 20:46:34,577 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/mlp/Mul ...\n",
            "2026-01-05 20:46:34,577 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.6/mlp/down_proj/MatMul ...\n",
            "2026-01-05 20:46:34,613 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.6/mlp/down_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,614 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/post_feedforward_layernorm/Cast ...\n",
            "2026-01-05 20:46:34,614 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/post_feedforward_layernorm ...\n",
            "2026-01-05 20:46:34,614 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/post_feedforward_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:34,614 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.6/Add_2 ...\n",
            "2026-01-05 20:46:34,614 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/input_layernorm/Cast ...\n",
            "2026-01-05 20:46:34,614 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/input_layernorm ...\n",
            "2026-01-05 20:46:34,614 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/input_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:34,614 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/attn/q_proj/MatMul ...\n",
            "2026-01-05 20:46:34,634 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/attn/q_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,634 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/attn/k_proj/MatMul ...\n",
            "2026-01-05 20:46:34,639 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/attn/k_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,639 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/attn/v_proj/MatMul ...\n",
            "2026-01-05 20:46:34,644 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/attn/v_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,644 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/attn/q_norm/Reshape1 ...\n",
            "2026-01-05 20:46:34,644 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/attn/q_norm/LayerNorm/Cast ...\n",
            "2026-01-05 20:46:34,644 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/attn/q_norm/LayerNorm ...\n",
            "2026-01-05 20:46:34,644 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/attn/q_norm/LayerNorm/output/Cast ...\n",
            "2026-01-05 20:46:34,644 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/attn/q_norm/Reshape2 ...\n",
            "2026-01-05 20:46:34,644 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/attn/k_norm/Reshape1 ...\n",
            "2026-01-05 20:46:34,644 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/attn/k_norm/LayerNorm/Cast ...\n",
            "2026-01-05 20:46:34,644 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/attn/k_norm/LayerNorm ...\n",
            "2026-01-05 20:46:34,644 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/attn/k_norm/LayerNorm/output/Cast ...\n",
            "2026-01-05 20:46:34,644 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/attn/k_norm/Reshape2 ...\n",
            "2026-01-05 20:46:34,644 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/attn/q_rotary ...\n",
            "2026-01-05 20:46:34,644 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/attn/k_rotary ...\n",
            "2026-01-05 20:46:34,644 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/attn/GroupQueryAttention ...\n",
            "2026-01-05 20:46:34,644 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/attn/o_proj/MatMul ...\n",
            "2026-01-05 20:46:34,662 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/attn/o_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,663 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/post_attention_layernorm/Cast ...\n",
            "2026-01-05 20:46:34,663 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/post_attention_layernorm ...\n",
            "2026-01-05 20:46:34,663 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/post_attention_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:34,663 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/Add_1 ...\n",
            "2026-01-05 20:46:34,663 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/pre_feedforward_layernorm/Cast ...\n",
            "2026-01-05 20:46:34,663 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/pre_feedforward_layernorm ...\n",
            "2026-01-05 20:46:34,663 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/pre_feedforward_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:34,663 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/mlp/gate_proj/MatMul ...\n",
            "2026-01-05 20:46:34,703 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,704 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/mlp/up_proj/MatMul ...\n",
            "2026-01-05 20:46:34,748 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/mlp/up_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,749 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/mlp/act_fn/FastGelu ...\n",
            "2026-01-05 20:46:34,749 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/mlp/Mul ...\n",
            "2026-01-05 20:46:34,749 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.7/mlp/down_proj/MatMul ...\n",
            "2026-01-05 20:46:34,794 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.7/mlp/down_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,794 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/post_feedforward_layernorm/Cast ...\n",
            "2026-01-05 20:46:34,794 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/post_feedforward_layernorm ...\n",
            "2026-01-05 20:46:34,794 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/post_feedforward_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:34,794 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.7/Add_2 ...\n",
            "2026-01-05 20:46:34,794 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/input_layernorm/Cast ...\n",
            "2026-01-05 20:46:34,794 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/input_layernorm ...\n",
            "2026-01-05 20:46:34,794 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/input_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:34,794 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/attn/q_proj/MatMul ...\n",
            "2026-01-05 20:46:34,813 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/attn/q_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,814 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/attn/k_proj/MatMul ...\n",
            "2026-01-05 20:46:34,818 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/attn/k_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,819 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/attn/v_proj/MatMul ...\n",
            "2026-01-05 20:46:34,823 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/attn/v_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,823 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/attn/q_norm/Reshape1 ...\n",
            "2026-01-05 20:46:34,823 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/attn/q_norm/LayerNorm/Cast ...\n",
            "2026-01-05 20:46:34,823 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/attn/q_norm/LayerNorm ...\n",
            "2026-01-05 20:46:34,824 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/attn/q_norm/LayerNorm/output/Cast ...\n",
            "2026-01-05 20:46:34,824 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/attn/q_norm/Reshape2 ...\n",
            "2026-01-05 20:46:34,824 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/attn/k_norm/Reshape1 ...\n",
            "2026-01-05 20:46:34,824 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/attn/k_norm/LayerNorm/Cast ...\n",
            "2026-01-05 20:46:34,824 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/attn/k_norm/LayerNorm ...\n",
            "2026-01-05 20:46:34,824 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/attn/k_norm/LayerNorm/output/Cast ...\n",
            "2026-01-05 20:46:34,824 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/attn/k_norm/Reshape2 ...\n",
            "2026-01-05 20:46:34,824 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/attn/q_rotary ...\n",
            "2026-01-05 20:46:34,824 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/attn/k_rotary ...\n",
            "2026-01-05 20:46:34,824 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/attn/GroupQueryAttention ...\n",
            "2026-01-05 20:46:34,824 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/attn/o_proj/MatMul ...\n",
            "2026-01-05 20:46:34,842 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/attn/o_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,842 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/post_attention_layernorm/Cast ...\n",
            "2026-01-05 20:46:34,842 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/post_attention_layernorm ...\n",
            "2026-01-05 20:46:34,842 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/post_attention_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:34,842 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/Add_1 ...\n",
            "2026-01-05 20:46:34,842 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/pre_feedforward_layernorm/Cast ...\n",
            "2026-01-05 20:46:34,842 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/pre_feedforward_layernorm ...\n",
            "2026-01-05 20:46:34,842 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/pre_feedforward_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:34,842 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/mlp/gate_proj/MatMul ...\n",
            "2026-01-05 20:46:34,884 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,884 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/mlp/up_proj/MatMul ...\n",
            "2026-01-05 20:46:34,924 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/mlp/up_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,924 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/mlp/act_fn/FastGelu ...\n",
            "2026-01-05 20:46:34,924 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/mlp/Mul ...\n",
            "2026-01-05 20:46:34,924 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.8/mlp/down_proj/MatMul ...\n",
            "2026-01-05 20:46:34,961 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.8/mlp/down_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,962 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/post_feedforward_layernorm/Cast ...\n",
            "2026-01-05 20:46:34,962 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/post_feedforward_layernorm ...\n",
            "2026-01-05 20:46:34,962 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/post_feedforward_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:34,962 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.8/Add_2 ...\n",
            "2026-01-05 20:46:34,962 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/input_layernorm/Cast ...\n",
            "2026-01-05 20:46:34,962 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/input_layernorm ...\n",
            "2026-01-05 20:46:34,962 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/input_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:34,962 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/attn/q_proj/MatMul ...\n",
            "2026-01-05 20:46:34,982 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/attn/q_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,982 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/attn/k_proj/MatMul ...\n",
            "2026-01-05 20:46:34,987 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/attn/k_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,987 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/attn/v_proj/MatMul ...\n",
            "2026-01-05 20:46:34,992 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/attn/v_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:34,992 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/attn/q_norm/Reshape1 ...\n",
            "2026-01-05 20:46:34,992 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/attn/q_norm/LayerNorm/Cast ...\n",
            "2026-01-05 20:46:34,992 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/attn/q_norm/LayerNorm ...\n",
            "2026-01-05 20:46:34,992 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/attn/q_norm/LayerNorm/output/Cast ...\n",
            "2026-01-05 20:46:34,992 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/attn/q_norm/Reshape2 ...\n",
            "2026-01-05 20:46:34,992 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/attn/k_norm/Reshape1 ...\n",
            "2026-01-05 20:46:34,992 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/attn/k_norm/LayerNorm/Cast ...\n",
            "2026-01-05 20:46:34,992 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/attn/k_norm/LayerNorm ...\n",
            "2026-01-05 20:46:34,992 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/attn/k_norm/LayerNorm/output/Cast ...\n",
            "2026-01-05 20:46:34,992 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/attn/k_norm/Reshape2 ...\n",
            "2026-01-05 20:46:34,992 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/attn/q_rotary ...\n",
            "2026-01-05 20:46:34,992 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/attn/k_rotary ...\n",
            "2026-01-05 20:46:34,992 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/attn/GroupQueryAttention ...\n",
            "2026-01-05 20:46:34,992 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/attn/o_proj/MatMul ...\n",
            "2026-01-05 20:46:35,010 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/attn/o_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:35,010 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/post_attention_layernorm/Cast ...\n",
            "2026-01-05 20:46:35,010 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/post_attention_layernorm ...\n",
            "2026-01-05 20:46:35,010 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/post_attention_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:35,010 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/Add_1 ...\n",
            "2026-01-05 20:46:35,011 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/pre_feedforward_layernorm/Cast ...\n",
            "2026-01-05 20:46:35,011 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/pre_feedforward_layernorm ...\n",
            "2026-01-05 20:46:35,011 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/pre_feedforward_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:35,011 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/mlp/gate_proj/MatMul ...\n",
            "2026-01-05 20:46:35,055 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:35,056 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/mlp/up_proj/MatMul ...\n",
            "2026-01-05 20:46:35,098 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/mlp/up_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:35,098 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/mlp/act_fn/FastGelu ...\n",
            "2026-01-05 20:46:35,098 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/mlp/Mul ...\n",
            "2026-01-05 20:46:35,098 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.9/mlp/down_proj/MatMul ...\n",
            "2026-01-05 20:46:35,134 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.9/mlp/down_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:35,134 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/post_feedforward_layernorm/Cast ...\n",
            "2026-01-05 20:46:35,134 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/post_feedforward_layernorm ...\n",
            "2026-01-05 20:46:35,134 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/post_feedforward_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:35,134 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.9/Add_2 ...\n",
            "2026-01-05 20:46:35,134 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/input_layernorm/Cast ...\n",
            "2026-01-05 20:46:35,134 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/input_layernorm ...\n",
            "2026-01-05 20:46:35,134 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/input_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:35,134 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/attn/q_proj/MatMul ...\n",
            "2026-01-05 20:46:35,154 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/attn/q_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:35,154 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/attn/k_proj/MatMul ...\n",
            "2026-01-05 20:46:35,159 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/attn/k_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:35,159 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/attn/v_proj/MatMul ...\n",
            "2026-01-05 20:46:35,164 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/attn/v_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:35,164 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/attn/q_norm/Reshape1 ...\n",
            "2026-01-05 20:46:35,164 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/attn/q_norm/LayerNorm/Cast ...\n",
            "2026-01-05 20:46:35,164 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/attn/q_norm/LayerNorm ...\n",
            "2026-01-05 20:46:35,164 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/attn/q_norm/LayerNorm/output/Cast ...\n",
            "2026-01-05 20:46:35,164 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/attn/q_norm/Reshape2 ...\n",
            "2026-01-05 20:46:35,164 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/attn/k_norm/Reshape1 ...\n",
            "2026-01-05 20:46:35,164 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/attn/k_norm/LayerNorm/Cast ...\n",
            "2026-01-05 20:46:35,164 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/attn/k_norm/LayerNorm ...\n",
            "2026-01-05 20:46:35,164 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/attn/k_norm/LayerNorm/output/Cast ...\n",
            "2026-01-05 20:46:35,164 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/attn/k_norm/Reshape2 ...\n",
            "2026-01-05 20:46:35,165 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/attn/q_rotary ...\n",
            "2026-01-05 20:46:35,165 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/attn/k_rotary ...\n",
            "2026-01-05 20:46:35,165 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/attn/GroupQueryAttention ...\n",
            "2026-01-05 20:46:35,165 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/attn/o_proj/MatMul ...\n",
            "2026-01-05 20:46:35,183 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/attn/o_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:35,183 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/post_attention_layernorm/Cast ...\n",
            "2026-01-05 20:46:35,183 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/post_attention_layernorm ...\n",
            "2026-01-05 20:46:35,183 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/post_attention_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:35,183 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/Add_1 ...\n",
            "2026-01-05 20:46:35,183 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/pre_feedforward_layernorm/Cast ...\n",
            "2026-01-05 20:46:35,183 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/pre_feedforward_layernorm ...\n",
            "2026-01-05 20:46:35,183 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/pre_feedforward_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:35,183 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/mlp/gate_proj/MatMul ...\n",
            "2026-01-05 20:46:35,223 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:35,224 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/mlp/up_proj/MatMul ...\n",
            "2026-01-05 20:46:35,264 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/mlp/up_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:35,264 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/mlp/act_fn/FastGelu ...\n",
            "2026-01-05 20:46:35,264 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/mlp/Mul ...\n",
            "2026-01-05 20:46:35,264 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.10/mlp/down_proj/MatMul ...\n",
            "2026-01-05 20:46:35,302 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.10/mlp/down_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:35,302 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/post_feedforward_layernorm/Cast ...\n",
            "2026-01-05 20:46:35,302 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/post_feedforward_layernorm ...\n",
            "2026-01-05 20:46:35,302 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/post_feedforward_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:35,302 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.10/Add_2 ...\n",
            "2026-01-05 20:46:35,302 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/input_layernorm/Cast ...\n",
            "2026-01-05 20:46:35,302 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/input_layernorm ...\n",
            "2026-01-05 20:46:35,302 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/input_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:35,302 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/attn/q_proj/MatMul ...\n",
            "2026-01-05 20:46:35,322 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/attn/q_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:35,322 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/attn/k_proj/MatMul ...\n",
            "2026-01-05 20:46:35,327 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/attn/k_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:35,327 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/attn/v_proj/MatMul ...\n",
            "2026-01-05 20:46:35,332 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/attn/v_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:35,332 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/attn/q_norm/Reshape1 ...\n",
            "2026-01-05 20:46:35,332 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/attn/q_norm/LayerNorm/Cast ...\n",
            "2026-01-05 20:46:35,332 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/attn/q_norm/LayerNorm ...\n",
            "2026-01-05 20:46:35,332 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/attn/q_norm/LayerNorm/output/Cast ...\n",
            "2026-01-05 20:46:35,333 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/attn/q_norm/Reshape2 ...\n",
            "2026-01-05 20:46:35,333 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/attn/k_norm/Reshape1 ...\n",
            "2026-01-05 20:46:35,333 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/attn/k_norm/LayerNorm/Cast ...\n",
            "2026-01-05 20:46:35,333 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/attn/k_norm/LayerNorm ...\n",
            "2026-01-05 20:46:35,333 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/attn/k_norm/LayerNorm/output/Cast ...\n",
            "2026-01-05 20:46:35,333 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/attn/k_norm/Reshape2 ...\n",
            "2026-01-05 20:46:35,333 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/attn/q_rotary ...\n",
            "2026-01-05 20:46:35,333 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/attn/k_rotary ...\n",
            "2026-01-05 20:46:35,333 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/attn/GroupQueryAttention ...\n",
            "2026-01-05 20:46:35,333 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/attn/o_proj/MatMul ...\n",
            "2026-01-05 20:46:35,354 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/attn/o_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:35,354 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/post_attention_layernorm/Cast ...\n",
            "2026-01-05 20:46:35,354 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/post_attention_layernorm ...\n",
            "2026-01-05 20:46:35,354 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/post_attention_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:35,354 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/Add_1 ...\n",
            "2026-01-05 20:46:35,354 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/pre_feedforward_layernorm/Cast ...\n",
            "2026-01-05 20:46:35,354 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/pre_feedforward_layernorm ...\n",
            "2026-01-05 20:46:35,354 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/pre_feedforward_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:35,354 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/mlp/gate_proj/MatMul ...\n",
            "2026-01-05 20:46:35,395 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:35,395 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/mlp/up_proj/MatMul ...\n",
            "2026-01-05 20:46:35,434 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/mlp/up_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:35,434 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/mlp/act_fn/FastGelu ...\n",
            "2026-01-05 20:46:35,434 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/mlp/Mul ...\n",
            "2026-01-05 20:46:35,435 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.11/mlp/down_proj/MatMul ...\n",
            "2026-01-05 20:46:35,471 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.11/mlp/down_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:35,471 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/post_feedforward_layernorm/Cast ...\n",
            "2026-01-05 20:46:35,471 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/post_feedforward_layernorm ...\n",
            "2026-01-05 20:46:35,471 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/post_feedforward_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:35,471 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.11/Add_2 ...\n",
            "2026-01-05 20:46:35,471 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/input_layernorm/Cast ...\n",
            "2026-01-05 20:46:35,472 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/input_layernorm ...\n",
            "2026-01-05 20:46:35,472 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/input_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:35,472 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/attn/q_proj/MatMul ...\n",
            "2026-01-05 20:46:35,491 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/attn/q_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:35,491 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/attn/k_proj/MatMul ...\n",
            "2026-01-05 20:46:35,497 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/attn/k_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:35,497 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/attn/v_proj/MatMul ...\n",
            "2026-01-05 20:46:35,502 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/attn/v_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:35,502 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/attn/q_norm/Reshape1 ...\n",
            "2026-01-05 20:46:35,502 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/attn/q_norm/LayerNorm/Cast ...\n",
            "2026-01-05 20:46:35,502 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/attn/q_norm/LayerNorm ...\n",
            "2026-01-05 20:46:35,502 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/attn/q_norm/LayerNorm/output/Cast ...\n",
            "2026-01-05 20:46:35,502 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/attn/q_norm/Reshape2 ...\n",
            "2026-01-05 20:46:35,502 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/attn/k_norm/Reshape1 ...\n",
            "2026-01-05 20:46:35,502 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/attn/k_norm/LayerNorm/Cast ...\n",
            "2026-01-05 20:46:35,502 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/attn/k_norm/LayerNorm ...\n",
            "2026-01-05 20:46:35,502 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/attn/k_norm/LayerNorm/output/Cast ...\n",
            "2026-01-05 20:46:35,502 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/attn/k_norm/Reshape2 ...\n",
            "2026-01-05 20:46:35,502 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/attn/q_rotary ...\n",
            "2026-01-05 20:46:35,502 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/attn/k_rotary ...\n",
            "2026-01-05 20:46:35,502 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/attn/GroupQueryAttention ...\n",
            "2026-01-05 20:46:35,502 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/attn/o_proj/MatMul ...\n",
            "2026-01-05 20:46:35,520 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/attn/o_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:35,520 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/post_attention_layernorm/Cast ...\n",
            "2026-01-05 20:46:35,520 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/post_attention_layernorm ...\n",
            "2026-01-05 20:46:35,521 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/post_attention_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:35,521 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/Add_1 ...\n",
            "2026-01-05 20:46:35,521 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/pre_feedforward_layernorm/Cast ...\n",
            "2026-01-05 20:46:35,521 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/pre_feedforward_layernorm ...\n",
            "2026-01-05 20:46:35,521 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/pre_feedforward_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:35,521 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/mlp/gate_proj/MatMul ...\n",
            "2026-01-05 20:46:35,563 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:35,563 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/mlp/up_proj/MatMul ...\n",
            "2026-01-05 20:46:35,609 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/mlp/up_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:35,609 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/mlp/act_fn/FastGelu ...\n",
            "2026-01-05 20:46:35,609 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/mlp/Mul ...\n",
            "2026-01-05 20:46:35,609 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.12/mlp/down_proj/MatMul ...\n",
            "2026-01-05 20:46:35,651 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.12/mlp/down_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:35,651 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/post_feedforward_layernorm/Cast ...\n",
            "2026-01-05 20:46:35,651 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/post_feedforward_layernorm ...\n",
            "2026-01-05 20:46:35,651 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/post_feedforward_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:35,651 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.12/Add_2 ...\n",
            "2026-01-05 20:46:35,651 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/input_layernorm/Cast ...\n",
            "2026-01-05 20:46:35,651 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/input_layernorm ...\n",
            "2026-01-05 20:46:35,651 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/input_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:35,651 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/attn/q_proj/MatMul ...\n",
            "2026-01-05 20:46:35,671 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/attn/q_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:35,671 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/attn/k_proj/MatMul ...\n",
            "2026-01-05 20:46:35,676 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/attn/k_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:35,676 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/attn/v_proj/MatMul ...\n",
            "2026-01-05 20:46:35,681 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/attn/v_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:35,681 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/attn/q_norm/Reshape1 ...\n",
            "2026-01-05 20:46:35,681 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/attn/q_norm/LayerNorm/Cast ...\n",
            "2026-01-05 20:46:35,681 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/attn/q_norm/LayerNorm ...\n",
            "2026-01-05 20:46:35,681 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/attn/q_norm/LayerNorm/output/Cast ...\n",
            "2026-01-05 20:46:35,681 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/attn/q_norm/Reshape2 ...\n",
            "2026-01-05 20:46:35,681 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/attn/k_norm/Reshape1 ...\n",
            "2026-01-05 20:46:35,681 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/attn/k_norm/LayerNorm/Cast ...\n",
            "2026-01-05 20:46:35,681 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/attn/k_norm/LayerNorm ...\n",
            "2026-01-05 20:46:35,682 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/attn/k_norm/LayerNorm/output/Cast ...\n",
            "2026-01-05 20:46:35,682 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/attn/k_norm/Reshape2 ...\n",
            "2026-01-05 20:46:35,682 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/attn/q_rotary ...\n",
            "2026-01-05 20:46:35,682 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/attn/k_rotary ...\n",
            "2026-01-05 20:46:35,682 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/attn/GroupQueryAttention ...\n",
            "2026-01-05 20:46:35,682 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/attn/o_proj/MatMul ...\n",
            "2026-01-05 20:46:35,700 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/attn/o_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:35,700 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/post_attention_layernorm/Cast ...\n",
            "2026-01-05 20:46:35,700 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/post_attention_layernorm ...\n",
            "2026-01-05 20:46:35,700 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/post_attention_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:35,700 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/Add_1 ...\n",
            "2026-01-05 20:46:35,700 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/pre_feedforward_layernorm/Cast ...\n",
            "2026-01-05 20:46:35,700 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/pre_feedforward_layernorm ...\n",
            "2026-01-05 20:46:35,700 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/pre_feedforward_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:35,700 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/mlp/gate_proj/MatMul ...\n",
            "2026-01-05 20:46:35,742 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:35,742 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/mlp/up_proj/MatMul ...\n",
            "2026-01-05 20:46:35,785 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/mlp/up_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:35,785 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/mlp/act_fn/FastGelu ...\n",
            "2026-01-05 20:46:35,785 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/mlp/Mul ...\n",
            "2026-01-05 20:46:35,785 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.13/mlp/down_proj/MatMul ...\n",
            "2026-01-05 20:46:35,823 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.13/mlp/down_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:35,823 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/post_feedforward_layernorm/Cast ...\n",
            "2026-01-05 20:46:35,823 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/post_feedforward_layernorm ...\n",
            "2026-01-05 20:46:35,823 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/post_feedforward_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:35,823 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.13/Add_2 ...\n",
            "2026-01-05 20:46:35,823 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/input_layernorm/Cast ...\n",
            "2026-01-05 20:46:35,823 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/input_layernorm ...\n",
            "2026-01-05 20:46:35,823 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/input_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:35,823 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/attn/q_proj/MatMul ...\n",
            "2026-01-05 20:46:35,843 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/attn/q_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:35,843 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/attn/k_proj/MatMul ...\n",
            "2026-01-05 20:46:35,849 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/attn/k_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:35,849 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/attn/v_proj/MatMul ...\n",
            "2026-01-05 20:46:35,856 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/attn/v_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:35,856 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/attn/q_norm/Reshape1 ...\n",
            "2026-01-05 20:46:35,856 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/attn/q_norm/LayerNorm/Cast ...\n",
            "2026-01-05 20:46:35,856 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/attn/q_norm/LayerNorm ...\n",
            "2026-01-05 20:46:35,856 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/attn/q_norm/LayerNorm/output/Cast ...\n",
            "2026-01-05 20:46:35,856 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/attn/q_norm/Reshape2 ...\n",
            "2026-01-05 20:46:35,856 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/attn/k_norm/Reshape1 ...\n",
            "2026-01-05 20:46:35,856 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/attn/k_norm/LayerNorm/Cast ...\n",
            "2026-01-05 20:46:35,856 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/attn/k_norm/LayerNorm ...\n",
            "2026-01-05 20:46:35,856 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/attn/k_norm/LayerNorm/output/Cast ...\n",
            "2026-01-05 20:46:35,856 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/attn/k_norm/Reshape2 ...\n",
            "2026-01-05 20:46:35,856 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/attn/q_rotary ...\n",
            "2026-01-05 20:46:35,856 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/attn/k_rotary ...\n",
            "2026-01-05 20:46:35,856 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/attn/GroupQueryAttention ...\n",
            "2026-01-05 20:46:35,856 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/attn/o_proj/MatMul ...\n",
            "2026-01-05 20:46:35,877 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/attn/o_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:35,877 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/post_attention_layernorm/Cast ...\n",
            "2026-01-05 20:46:35,877 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/post_attention_layernorm ...\n",
            "2026-01-05 20:46:35,877 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/post_attention_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:35,877 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/Add_1 ...\n",
            "2026-01-05 20:46:35,877 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/pre_feedforward_layernorm/Cast ...\n",
            "2026-01-05 20:46:35,877 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/pre_feedforward_layernorm ...\n",
            "2026-01-05 20:46:35,877 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/pre_feedforward_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:35,878 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/mlp/gate_proj/MatMul ...\n",
            "2026-01-05 20:46:35,920 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:35,920 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/mlp/up_proj/MatMul ...\n",
            "2026-01-05 20:46:35,963 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/mlp/up_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:35,963 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/mlp/act_fn/FastGelu ...\n",
            "2026-01-05 20:46:35,963 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/mlp/Mul ...\n",
            "2026-01-05 20:46:35,963 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.14/mlp/down_proj/MatMul ...\n",
            "2026-01-05 20:46:35,999 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.14/mlp/down_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:35,999 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/post_feedforward_layernorm/Cast ...\n",
            "2026-01-05 20:46:36,000 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/post_feedforward_layernorm ...\n",
            "2026-01-05 20:46:36,000 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/post_feedforward_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:36,000 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.14/Add_2 ...\n",
            "2026-01-05 20:46:36,000 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/input_layernorm/Cast ...\n",
            "2026-01-05 20:46:36,000 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/input_layernorm ...\n",
            "2026-01-05 20:46:36,000 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/input_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:36,000 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/attn/q_proj/MatMul ...\n",
            "2026-01-05 20:46:36,020 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/attn/q_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:36,020 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/attn/k_proj/MatMul ...\n",
            "2026-01-05 20:46:36,026 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/attn/k_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:36,026 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/attn/v_proj/MatMul ...\n",
            "2026-01-05 20:46:36,030 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/attn/v_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:36,030 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/attn/q_norm/Reshape1 ...\n",
            "2026-01-05 20:46:36,030 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/attn/q_norm/LayerNorm/Cast ...\n",
            "2026-01-05 20:46:36,030 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/attn/q_norm/LayerNorm ...\n",
            "2026-01-05 20:46:36,030 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/attn/q_norm/LayerNorm/output/Cast ...\n",
            "2026-01-05 20:46:36,030 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/attn/q_norm/Reshape2 ...\n",
            "2026-01-05 20:46:36,030 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/attn/k_norm/Reshape1 ...\n",
            "2026-01-05 20:46:36,030 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/attn/k_norm/LayerNorm/Cast ...\n",
            "2026-01-05 20:46:36,030 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/attn/k_norm/LayerNorm ...\n",
            "2026-01-05 20:46:36,030 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/attn/k_norm/LayerNorm/output/Cast ...\n",
            "2026-01-05 20:46:36,030 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/attn/k_norm/Reshape2 ...\n",
            "2026-01-05 20:46:36,030 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/attn/q_rotary ...\n",
            "2026-01-05 20:46:36,030 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/attn/k_rotary ...\n",
            "2026-01-05 20:46:36,030 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/attn/GroupQueryAttention ...\n",
            "2026-01-05 20:46:36,030 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/attn/o_proj/MatMul ...\n",
            "2026-01-05 20:46:36,049 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/attn/o_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:36,049 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/post_attention_layernorm/Cast ...\n",
            "2026-01-05 20:46:36,049 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/post_attention_layernorm ...\n",
            "2026-01-05 20:46:36,049 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/post_attention_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:36,049 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/Add_1 ...\n",
            "2026-01-05 20:46:36,049 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/pre_feedforward_layernorm/Cast ...\n",
            "2026-01-05 20:46:36,049 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/pre_feedforward_layernorm ...\n",
            "2026-01-05 20:46:36,049 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/pre_feedforward_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:36,049 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/mlp/gate_proj/MatMul ...\n",
            "2026-01-05 20:46:36,092 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:36,092 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/mlp/up_proj/MatMul ...\n",
            "2026-01-05 20:46:36,133 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/mlp/up_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:36,133 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/mlp/act_fn/FastGelu ...\n",
            "2026-01-05 20:46:36,133 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/mlp/Mul ...\n",
            "2026-01-05 20:46:36,133 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.15/mlp/down_proj/MatMul ...\n",
            "2026-01-05 20:46:36,169 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.15/mlp/down_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:36,169 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/post_feedforward_layernorm/Cast ...\n",
            "2026-01-05 20:46:36,170 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/post_feedforward_layernorm ...\n",
            "2026-01-05 20:46:36,170 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/post_feedforward_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:36,170 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.15/Add_2 ...\n",
            "2026-01-05 20:46:36,170 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/input_layernorm/Cast ...\n",
            "2026-01-05 20:46:36,170 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/input_layernorm ...\n",
            "2026-01-05 20:46:36,170 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/input_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:36,170 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/attn/q_proj/MatMul ...\n",
            "2026-01-05 20:46:36,191 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/attn/q_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:36,191 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/attn/k_proj/MatMul ...\n",
            "2026-01-05 20:46:36,196 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/attn/k_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:36,196 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/attn/v_proj/MatMul ...\n",
            "2026-01-05 20:46:36,201 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/attn/v_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:36,201 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/attn/q_norm/Reshape1 ...\n",
            "2026-01-05 20:46:36,201 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/attn/q_norm/LayerNorm/Cast ...\n",
            "2026-01-05 20:46:36,201 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/attn/q_norm/LayerNorm ...\n",
            "2026-01-05 20:46:36,201 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/attn/q_norm/LayerNorm/output/Cast ...\n",
            "2026-01-05 20:46:36,201 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/attn/q_norm/Reshape2 ...\n",
            "2026-01-05 20:46:36,201 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/attn/k_norm/Reshape1 ...\n",
            "2026-01-05 20:46:36,202 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/attn/k_norm/LayerNorm/Cast ...\n",
            "2026-01-05 20:46:36,202 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/attn/k_norm/LayerNorm ...\n",
            "2026-01-05 20:46:36,202 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/attn/k_norm/LayerNorm/output/Cast ...\n",
            "2026-01-05 20:46:36,202 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/attn/k_norm/Reshape2 ...\n",
            "2026-01-05 20:46:36,202 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/attn/q_rotary ...\n",
            "2026-01-05 20:46:36,202 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/attn/k_rotary ...\n",
            "2026-01-05 20:46:36,202 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/attn/GroupQueryAttention ...\n",
            "2026-01-05 20:46:36,202 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/attn/o_proj/MatMul ...\n",
            "2026-01-05 20:46:36,220 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/attn/o_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:36,221 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/post_attention_layernorm/Cast ...\n",
            "2026-01-05 20:46:36,221 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/post_attention_layernorm ...\n",
            "2026-01-05 20:46:36,221 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/post_attention_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:36,221 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/Add_1 ...\n",
            "2026-01-05 20:46:36,221 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/pre_feedforward_layernorm/Cast ...\n",
            "2026-01-05 20:46:36,221 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/pre_feedforward_layernorm ...\n",
            "2026-01-05 20:46:36,221 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/pre_feedforward_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:36,221 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/mlp/gate_proj/MatMul ...\n",
            "2026-01-05 20:46:36,265 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:36,265 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/mlp/up_proj/MatMul ...\n",
            "2026-01-05 20:46:36,314 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/mlp/up_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:36,314 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/mlp/act_fn/FastGelu ...\n",
            "2026-01-05 20:46:36,314 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/mlp/Mul ...\n",
            "2026-01-05 20:46:36,314 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.16/mlp/down_proj/MatMul ...\n",
            "2026-01-05 20:46:36,352 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.16/mlp/down_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:36,353 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/post_feedforward_layernorm/Cast ...\n",
            "2026-01-05 20:46:36,353 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/post_feedforward_layernorm ...\n",
            "2026-01-05 20:46:36,353 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/post_feedforward_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:36,353 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.16/Add_2 ...\n",
            "2026-01-05 20:46:36,353 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/input_layernorm/Cast ...\n",
            "2026-01-05 20:46:36,353 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/input_layernorm ...\n",
            "2026-01-05 20:46:36,353 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/input_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:36,353 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/attn/q_proj/MatMul ...\n",
            "2026-01-05 20:46:36,373 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/attn/q_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:36,373 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/attn/k_proj/MatMul ...\n",
            "2026-01-05 20:46:36,379 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/attn/k_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:36,379 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/attn/v_proj/MatMul ...\n",
            "2026-01-05 20:46:36,384 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/attn/v_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:36,384 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/attn/q_norm/Reshape1 ...\n",
            "2026-01-05 20:46:36,384 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/attn/q_norm/LayerNorm/Cast ...\n",
            "2026-01-05 20:46:36,384 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/attn/q_norm/LayerNorm ...\n",
            "2026-01-05 20:46:36,384 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/attn/q_norm/LayerNorm/output/Cast ...\n",
            "2026-01-05 20:46:36,384 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/attn/q_norm/Reshape2 ...\n",
            "2026-01-05 20:46:36,384 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/attn/k_norm/Reshape1 ...\n",
            "2026-01-05 20:46:36,384 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/attn/k_norm/LayerNorm/Cast ...\n",
            "2026-01-05 20:46:36,384 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/attn/k_norm/LayerNorm ...\n",
            "2026-01-05 20:46:36,384 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/attn/k_norm/LayerNorm/output/Cast ...\n",
            "2026-01-05 20:46:36,385 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/attn/k_norm/Reshape2 ...\n",
            "2026-01-05 20:46:36,385 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/attn/q_rotary ...\n",
            "2026-01-05 20:46:36,385 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/attn/k_rotary ...\n",
            "2026-01-05 20:46:36,385 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/attn/GroupQueryAttention ...\n",
            "2026-01-05 20:46:36,385 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/attn/o_proj/MatMul ...\n",
            "2026-01-05 20:46:36,404 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/attn/o_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:36,404 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/post_attention_layernorm/Cast ...\n",
            "2026-01-05 20:46:36,404 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/post_attention_layernorm ...\n",
            "2026-01-05 20:46:36,404 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/post_attention_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:36,404 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/Add_1 ...\n",
            "2026-01-05 20:46:36,404 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/pre_feedforward_layernorm/Cast ...\n",
            "2026-01-05 20:46:36,404 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/pre_feedforward_layernorm ...\n",
            "2026-01-05 20:46:36,404 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/pre_feedforward_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:36,404 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/mlp/gate_proj/MatMul ...\n",
            "2026-01-05 20:46:36,445 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/mlp/gate_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:36,445 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/mlp/up_proj/MatMul ...\n",
            "2026-01-05 20:46:36,490 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/mlp/up_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:36,490 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/mlp/act_fn/FastGelu ...\n",
            "2026-01-05 20:46:36,491 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/mlp/Mul ...\n",
            "2026-01-05 20:46:36,491 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /model/layers.17/mlp/down_proj/MatMul ...\n",
            "2026-01-05 20:46:36,527 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /model/layers.17/mlp/down_proj/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:36,527 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/post_feedforward_layernorm/Cast ...\n",
            "2026-01-05 20:46:36,527 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/post_feedforward_layernorm ...\n",
            "2026-01-05 20:46:36,527 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/post_feedforward_layernorm/output/Cast ...\n",
            "2026-01-05 20:46:36,527 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/layers.17/Add_2 ...\n",
            "2026-01-05 20:46:36,527 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/final_norm ...\n",
            "2026-01-05 20:46:36,527 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /model/final_norm/output/Cast ...\n",
            "2026-01-05 20:46:36,527 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /lm_head/Transpose ...\n",
            "2026-01-05 20:46:36,527 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - start to quantize /lm_head/MatMul ...\n",
            "2026-01-05 20:46:36,528 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - MatMul doesn't have const weight. Skip to quantize\n",
            "2026-01-05 20:46:36,528 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - complete quantization of /lm_head/MatMul with 4 bits ...\n",
            "2026-01-05 20:46:36,528 onnxruntime.quantization.matmul_nbits_quantizer [INFO] - skip to quantize /lm_head/CastToFloat ...\n",
            "\n",
            "✓ Converted ONNX models saved to /content/functiongemma-square-color-onnx\n"
          ]
        }
      ],
      "source": [
        "# Download Xenova's build_gemma.py script\n",
        "!wget -q https://gist.githubusercontent.com/xenova/a219dbf3c7da7edd5dbb05f92410d7bd/raw/45f4c5a5227c1123efebe1e36d060672ee685a8e/build_gemma.py\n",
        "\n",
        "# Output path\n",
        "OUTPUT_DIR = f\"/content/{MODEL_NAME}-onnx\"\n",
        "\n",
        "# Convert model to ONNX with multiple precisions\n",
        "!python build_gemma.py \\\n",
        "    --model_name {REPO_ID} \\\n",
        "    --output {OUTPUT_DIR} \\\n",
        "    -p fp32 fp16 q4 q4f16\n",
        "\n",
        "print(f\"\\n✓ Converted ONNX models saved to {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lF7Tktf5Uo6"
      },
      "source": [
        "## 5. Verify File Structure\n",
        "\n",
        "Expected output structure (same as onnx-community/functiongemma-270m-it-ONNX):\n",
        "```\n",
        "onnx_output/\n",
        "├── config.json\n",
        "├── tokenizer.json\n",
        "├── tokenizer_config.json\n",
        "└── onnx/\n",
        "    ├── model.onnx\n",
        "    ├── model.onnx_data\n",
        "    ├── model_fp16.onnx\n",
        "    ├── model_fp16.onnx_data\n",
        "    ├── model_q4.onnx\n",
        "    ├── model_q4.onnx_data\n",
        "    ├── model_q4f16.onnx\n",
        "    └── model_q4f16.onnx_data\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBuKfkkK5Uo6",
        "outputId": "55825a5a-33ce-4bc2-8d60-8035e0b804d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verifying generated file structure...\n",
            "============================================================\n",
            "  [✓] config.json\n",
            "  [✓] tokenizer.json\n",
            "  [✓] tokenizer_config.json\n",
            "\n",
            "ONNX models: 4\n",
            "  - model.onnx (0.2 MB) + model.onnx_data\n",
            "  - model_fp16.onnx (0.2 MB) + model_fp16.onnx_data\n",
            "  - model_q4.onnx (0.2 MB) + model_q4.onnx_data\n",
            "  - model_q4f16.onnx (0.3 MB) + model_q4f16.onnx_data\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "print(\"Verifying generated file structure...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Check critical files\n",
        "critical_files = ['config.json', 'tokenizer.json', 'tokenizer_config.json']\n",
        "for f in critical_files:\n",
        "    path = os.path.join(OUTPUT_DIR, f)\n",
        "    exists = os.path.exists(path)\n",
        "    status = \"✓\" if exists else \"✗\"\n",
        "    print(f\"  [{status}] {f}\")\n",
        "\n",
        "# Check ONNX directory\n",
        "onnx_dir = os.path.join(OUTPUT_DIR, 'onnx')\n",
        "if os.path.exists(onnx_dir):\n",
        "    onnx_files = sorted([f for f in os.listdir(onnx_dir) if f.endswith('.onnx')])\n",
        "    print(f\"\\nONNX models: {len(onnx_files)}\")\n",
        "    for f in onnx_files:\n",
        "        size_mb = os.path.getsize(os.path.join(onnx_dir, f)) / (1024 * 1024)\n",
        "        # Check for corresponding .onnx_data file\n",
        "        data_file = f + \"_data\"\n",
        "        has_data = os.path.exists(os.path.join(onnx_dir, data_file))\n",
        "        data_info = f\" + {data_file}\" if has_data else \"\"\n",
        "        print(f\"  - {f} ({size_mb:.1f} MB){data_info}\")\n",
        "else:\n",
        "    print(\"\\n⚠ WARNING: onnx/ directory not found!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsBgQ1UN5Uo6"
      },
      "source": [
        "## 6. Test ONNX Model\n",
        "\n",
        "Test the converted ONNX model using ONNX Runtime. This validates that the model works before uploading to HuggingFace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7rnqFaG5Uo6",
        "outputId": "eeec7c7b-dda3-4f47-d1e0-2d7063dae9e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Loaded onnx/model.onnx\n",
            "  Layers: 18, KV heads: 1, Head dim: 256\n",
            "  KV cache dtype: <class 'numpy.float32'>\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoConfig, AutoTokenizer\n",
        "import onnxruntime\n",
        "import numpy as np\n",
        "\n",
        "# Load config and tokenizer\n",
        "config = AutoConfig.from_pretrained(OUTPUT_DIR)\n",
        "tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n",
        "\n",
        "# Choose which model to test (use fp32 or q4 for easier testing)\n",
        "model_file = \"onnx/model.onnx\"  # Options: model.onnx, model_fp16.onnx, model_q4.onnx, model_q4f16.onnx\n",
        "\n",
        "# Determine dtype based on model (fp16/q4f16 need float16, others use float32)\n",
        "use_fp16 = \"fp16\" in model_file\n",
        "kv_dtype = np.float16 if use_fp16 else np.float32\n",
        "\n",
        "model_path = f\"{OUTPUT_DIR}/{model_file}\"\n",
        "decoder_session = onnxruntime.InferenceSession(model_path)\n",
        "\n",
        "# Config values for KV cache\n",
        "num_key_value_heads = config.num_key_value_heads\n",
        "head_dim = config.head_dim\n",
        "num_hidden_layers = config.num_hidden_layers\n",
        "eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(f\"✓ Loaded {model_file}\")\n",
        "print(f\"  Layers: {num_hidden_layers}, KV heads: {num_key_value_heads}, Head dim: {head_dim}\")\n",
        "print(f\"  KV cache dtype: {kv_dtype}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPqgD8uQ5Uo6",
        "outputId": "b464555f-f04d-4cd5-815c-55163b620f4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: change the color to blue\n",
            "Output: <start_function_call>call:set_square_color{color:<escape>blue<escape>}<end_function_call><start_function_response>user:set_square_color<end_of_turn>\n",
            "<start_function_call>call:get_square_color{}<end_function_call><start_function_response>user:change color<end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn>\n",
            "<start_function_call>call:set_square_color{color:<escape>blue<escape>}<end_function_call>\n",
            "\n",
            "Input: what is the current color?\n",
            "Output: <start_function_call>call:get_square_color{}<end_function_call><start_function_response>call:get_square_color{}<end_function_call><start_function_response>user:set_square_color<end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn>\n",
            "<start_function_call>call:set_square_color{color:<escape>red<escape>}<end_function_call><start_function_response>call:get_square_color{}<end_function_call><start_function_response>user\n"
          ]
        }
      ],
      "source": [
        "# System prompt for FunctionGemma\n",
        "SYSTEM_PROMPT = \"\"\"You are a model that can do function calling with the following functions\n",
        "\n",
        "<start_function_declaration>\n",
        "name:set_square_color\n",
        "description:Sets the color of the square to a specified color\n",
        "parameters:{color:{type:string,description:The color to set the square to,required:true}}\n",
        "<end_function_declaration>\n",
        "<start_function_declaration>\n",
        "name:get_square_color\n",
        "description:Gets the current color of the square\n",
        "parameters:{}\n",
        "<end_function_declaration>\"\"\"\n",
        "\n",
        "# Test prompts\n",
        "test_inputs = [\n",
        "    \"change the color to blue\",\n",
        "    \"what is the current color?\",\n",
        "]\n",
        "\n",
        "for test_input in test_inputs:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": test_input},\n",
        "    ]\n",
        "\n",
        "    inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"np\")\n",
        "    input_ids = inputs['input_ids']\n",
        "    attention_mask = inputs['attention_mask']\n",
        "    batch_size = input_ids.shape[0]\n",
        "\n",
        "    # Use correct dtype for KV cache based on model\n",
        "    past_key_values = {\n",
        "        f'past_key_values.{layer}.{kv}': np.zeros([batch_size, num_key_value_heads, 0, head_dim], dtype=kv_dtype)\n",
        "        for layer in range(num_hidden_layers)\n",
        "        for kv in ('key', 'value')\n",
        "    }\n",
        "    position_ids = np.tile(np.arange(0, input_ids.shape[-1]), (batch_size, 1))\n",
        "\n",
        "    # Generation loop\n",
        "    max_new_tokens = 64\n",
        "    generated_tokens = np.array([[]], dtype=np.int64)\n",
        "\n",
        "    for i in range(max_new_tokens):\n",
        "        logits, *present_key_values = decoder_session.run(None, dict(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            position_ids=position_ids,\n",
        "            **past_key_values,\n",
        "        ))\n",
        "\n",
        "        input_ids = logits[:, -1].argmax(-1, keepdims=True)\n",
        "        attention_mask = np.concatenate([attention_mask, np.ones_like(input_ids, dtype=np.int64)], axis=-1)\n",
        "        position_ids = position_ids[:, -1:] + 1\n",
        "\n",
        "        for j, key in enumerate(past_key_values):\n",
        "            past_key_values[key] = present_key_values[j]\n",
        "\n",
        "        generated_tokens = np.concatenate([generated_tokens, input_ids], axis=-1)\n",
        "\n",
        "        if np.isin(input_ids, eos_token_id).any():\n",
        "            break\n",
        "\n",
        "    output = tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)[0]\n",
        "    print(f\"\\nInput: {test_input}\")\n",
        "    print(f\"Output: {output}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2B3DSI3V5Uo6"
      },
      "source": [
        "## 7. Upload to Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508,
          "referenced_widgets": [
            "27b3d8df46154af2bdd9738747c8e013",
            "9e9ce24b2a2642d7b862d54865eaf020",
            "deb90b544cf8437cb0e3f9873911894d",
            "e06c8e507f8d4e818488587474d00827",
            "e85c0109333b4518839d75f18a6e1b33",
            "f53a846af66d4a628580a620fd0b7890",
            "2c67d977581f4d17b197d5377cfeb037",
            "3c9b654eabfe4242813db9e3106b92bc",
            "226e30e915f14a58a4a27c3590859230",
            "eac070c6c26e4455b9f0ac93cd7f01d6",
            "94eb225ec4c14bb088cd28a9f138fbe1",
            "1bf64edcc6514ade904b333d07f94aba",
            "ee811e0805c34c6993348cdd6140ae65",
            "b2a831de53bc4e8eac608cfdc11b0a38",
            "5458a51de8a6461b86a2ca962f1f21dd",
            "13d8d73ce0c54ccaa125c479e606a5ce",
            "1a952f261e6a4c298243813600b2a83e",
            "ac89168a3d174f70834dc1862b01c08d",
            "fd1cb01a06204d64b926d453dab91441",
            "3ab540c874b04b3fbde683627bc51f92",
            "e010ff80fae14f84b2d8a14c5ccf804e",
            "b7c01ef6e4484a1191ad4130e721ced8",
            "627abb1308cd4fa49f9543ab59620135",
            "ab8b4609191c4f928c76b436fef63c47",
            "6ad4b0e21c284f29bf65b604dce59bd0",
            "38eccde5de9e4fd6ae59f16678d8971f",
            "615750ab5bdd4a9f8ff639462edc6616",
            "79b104a85d66474692330bef45add2cc",
            "a4aab00a76d94286b51325c6bd291ce1",
            "f51e09e4bd884b2986c3d7c1c5078c90",
            "05fbe31b01bf4215bd840745b2efbec0",
            "8811612985e24867844d66949206e79a",
            "1d1b6151afb04c11b708d20679fa1d95",
            "e87d22a2785d483799df4c95b2801a7d",
            "cef82556c8514e9a84954c11dab4dd9e",
            "e1a98d93791a4ffc851811ee784dc687",
            "4e1760e846bf4679b9a9455e2c7f96cf",
            "97b6e68fe0c945919ee3856c0906cb84",
            "554f35bc28f04b699e9d4e7ce4b9f46e",
            "762f9a81739242a79784b9aebbfcdd5c",
            "eca579628e654ae495726d529ef552ac",
            "3eb5346e72fe4cb9ac607007df860a29",
            "bab2e8329f9845bfba5270fd67f15e40",
            "498b3566fad44ec785ddcfc613fcb38f",
            "ff6bf0d84a84401f9342c8d8e1fd1476",
            "b34627f956c14578a08e3625475a9b90",
            "5ea93de317c34449a971a0ab049446a0",
            "70b455af3d9346419bf9eb0d3520dc02",
            "bc6e1ed66ac14901ba839cf651d82a00",
            "b888c0fe466840d7a14370e4bc2c4e9f",
            "1163fd2e96d84f71a6e83310354b5f2b",
            "29278bcb2ecd4af0a5eb025010e56fd7",
            "24f0f127a0dc4ab69ffce337b556d7bb",
            "9e9fecdc931443a68f1ef2dde7957666",
            "98400082faf74a7f9a40869d949d403e",
            "b4050c5f2b3b4fafb4cceb2189329a4c",
            "03cc243a120f4d11b39f1952fb3a6bcf",
            "62a52383c25f4a4b86ff0448ebc9f09a",
            "5a3468ebc4a1430ead74ee3f5feb9496",
            "906fa49f6ffe4b2f95a75e7c09d3fc5e",
            "6547a0cfec5f436096c572484f92ccc4",
            "2cecbe2c1ee5480abe530773b1106c73",
            "32b933b2bc18402ea1c0ed3310c2926d",
            "f7d132e647b2411a8580c03a160bf637",
            "8d9e224c0eba4f14bc4550476ef2e0b4",
            "6022099877bd44e88da3038b86f68365",
            "43cc19c293524253966448c7fb990cb6",
            "9c6b9de372ba45a48c27565779333e71",
            "517e6b9e46f24af3b3912aaca73bf8d1",
            "4a582900a98b4c14a544d1bb4a5137d7",
            "a5c7da5bf861444ba71311b86d41c0e1",
            "9904d2bd36414d00b7df5a3650d41fea",
            "d688edab19ca4cf8affb33ed871ea9d6",
            "7f66f22b0af34735b4bb82b2c0eb7dac",
            "81e23581d5cc410781d1f49fea1fb3c0",
            "6377d3aa77bd4c8bbed35e5e42306703",
            "2087854254e246c7a7b2ccd419c5f872",
            "00808d10f6fb468f860eb844ab46c0b0",
            "6684914ee4f14cca91c5e3bee55c73d1",
            "623880f28faf43f6bf848bb92e8b9687",
            "ea62d5512ead4afcb58a704614ca791d",
            "b23c2ea666dd40758435316b10d0d98a",
            "0a8ea330266a4e938d67ace1c61ead4e",
            "4da7bb9c832c492c8bd23f261d301b39",
            "66fab2c0f5a74499ae69c108761a73f0",
            "5c6dd1c391fb4eaebee474d986bf4fba",
            "8cb36ae18351492e8bc9681d87d97ce2",
            "5968de84e1df49298ab91d899feafa43",
            "dbb2559198ea4007888745f2753dc4b1",
            "238e1e396cd84d3eb6a48db500be0068",
            "e0cc735832cb4d35bfc14b2d1e8f520a",
            "a742f6da10dd4ed3908f2095bc045f68",
            "df2d342636d3486da1bd6f97fa310d0d",
            "51575107fcfe44a9aed4a25f59f014d9",
            "2e42446953c34b8696d3bae808bd279c",
            "abfbc9caed594dfea6e4976266132746",
            "3df694631078446a9a1b448a4c907144",
            "86ee7d69fc9f49b9a34892189bafcf8d",
            "b417bad5f2444281af1331e1be61d502",
            "c42dbdd761f34ceea4ee73da840ed72b",
            "f057f6798bba4d25aee44b1921d7b517",
            "f54abf954efe41de9276736dd900d6f0",
            "2589f3fd2c174dcca384c5b3133ed2fe",
            "079bf75726404a0db9c908daf5e2fd3e",
            "33d5731d72384de88ba589201b3df15c",
            "a0b0db695fcd4a5eb809fd1fb39a90b3",
            "4182d403307b45c78534f5fed018c86e",
            "c51550e89b9f45128c1ae04c11ee6d5c",
            "1de0df802adc48a3990873076b119fcb",
            "257274b26cf746678a98b5464de23f7d",
            "3c29130550d44b56927d0c857f1213fb",
            "2849aee3cb6c4261833e02818804d2ef",
            "baf827ed76354659be787de5e0be9b49",
            "279de607a0d744d3aa27dcbebbbffbe8",
            "bc8d082ed1fb44ae943aaaacee986e94",
            "7be8cf7d5da44bf7baf3f31092d22b06",
            "365c0295ad034dd9b146606bf699921a",
            "fdbaf0d820284e029cd19277d8ebfa8a",
            "534a09c1fe424be88873182db1f376be",
            "d6cf9af764674dc0a547f7e3c727313c",
            "167cb35198cb4bf6b16e6c6cbdd99fbe",
            "8390aa3d3f594566bf9d35ffaa6f0d8e",
            "1c6d34cca38f4a739bebf6934f1f817b",
            "1cce1aee6aaa4202b53db824b4843c5f",
            "0e7eca903d1144f2a1201b40e0a9582b",
            "1fe2b5d2c6e24764a9266c450b7bab93",
            "71bc0421acc7403e9bd49bbd10effa6f",
            "32599930b7f24df3bd62f12a30ebef46",
            "db45eda1d47048a6a9a286fb8f6c8ea0",
            "89a6216a16e649919824938a9391e35c",
            "c4ad7351f3ef47f49b989259fb5cd83b",
            "6288fc8e8ba54d0cbb3c617962626c70"
          ]
        },
        "id": "Lki9YUBP5Uo6",
        "outputId": "ccab2ef2-5910-4ead-fd39-228d46917b31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target repository: harlley/functiongemma-square-color-ONNX\n",
            "Creating repository...\n",
            "Uploading files...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27b3d8df46154af2bdd9738747c8e013"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload               : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1bf64edcc6514ade904b333d07f94aba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...olor-onnx/tokenizer.model: 100%|##########| 4.69MB / 4.69MB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "627abb1308cd4fa49f9543ab59620135"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...olor-onnx/onnx/model.onnx: 100%|##########|  194kB /  194kB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e87d22a2785d483799df4c95b2801a7d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...nnx/onnx/model_q4f16.onnx: 100%|##########|  304kB /  304kB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff6bf0d84a84401f9342c8d8e1fd1476"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...r-onnx/onnx/model_q4.onnx: 100%|##########|  239kB /  239kB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b4050c5f2b3b4fafb4cceb2189329a4c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...onnx/onnx/model_fp16.onnx: 100%|##########|  258kB /  258kB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "43cc19c293524253966448c7fb990cb6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...nnx/model_q4f16.onnx_data:   2%|1         | 8.35MB /  426MB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "00808d10f6fb468f860eb844ab46c0b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...onnx/model_fp16.onnx_data:   1%|1         | 8.35MB /  570MB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dbb2559198ea4007888745f2753dc4b1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...color-onnx/tokenizer.json: 100%|##########| 20.3MB / 20.3MB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c42dbdd761f34ceea4ee73da840ed72b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...x/onnx/model_q4.onnx_data:   3%|3         | 25.2MB /  801MB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c29130550d44b56927d0c857f1213fb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...onnx/onnx/model.onnx_data:   2%|2         | 25.2MB / 1.14GB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8390aa3d3f594566bf9d35ffaa6f0d8e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Upload completed!\n",
            "URL: https://huggingface.co/harlley/functiongemma-square-color-ONNX/tree/main/\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import whoami, create_repo, upload_folder\n",
        "\n",
        "username = whoami()['name']\n",
        "\n",
        "ONNX_REPO_NAME = \"functiongemma-square-color-ONNX\"  # @param {type:\"string\"}\n",
        "HF_REPO_ID = f\"{username}/{ONNX_REPO_NAME}\"\n",
        "\n",
        "print(f\"Target repository: {HF_REPO_ID}\")\n",
        "print(\"Creating repository...\")\n",
        "\n",
        "create_repo(HF_REPO_ID, repo_type=\"model\", exist_ok=True)\n",
        "\n",
        "print(\"Uploading files...\")\n",
        "repo_url = upload_folder(\n",
        "    folder_path=OUTPUT_DIR,\n",
        "    repo_id=HF_REPO_ID,\n",
        "    repo_type=\"model\",\n",
        "    commit_message=f\"Upload ONNX model via official converter - {ONNX_REPO_NAME}\"\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ Upload completed!\")\n",
        "print(f\"URL: {repo_url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuRkov7R5Uo7"
      },
      "source": [
        "## Summary\n",
        "\n",
        "Your FunctionGemma ONNX model has been converted and uploaded!\n",
        "\n",
        "**Files uploaded:**\n",
        "- `config.json`, `tokenizer.json`, `tokenizer_config.json`\n",
        "- `onnx/model.onnx` + `model.onnx_data` (fp32)\n",
        "- `onnx/model_fp16.onnx` + data (fp16)\n",
        "- `onnx/model_q4.onnx` + data (4-bit)\n",
        "- `onnx/model_q4f16.onnx` + data (4-bit with fp16, recommended for WebGPU)\n",
        "\n",
        "**Next Steps:**\n",
        "1. Update `src/worker.ts` with your model ID\n",
        "2. Set `dtype: \"q4f16\"` for best WebGPU performance\n",
        "3. Run `npm run dev` to test in browser"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "27b3d8df46154af2bdd9738747c8e013": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9e9ce24b2a2642d7b862d54865eaf020",
              "IPY_MODEL_deb90b544cf8437cb0e3f9873911894d",
              "IPY_MODEL_e06c8e507f8d4e818488587474d00827"
            ],
            "layout": "IPY_MODEL_e85c0109333b4518839d75f18a6e1b33"
          }
        },
        "9e9ce24b2a2642d7b862d54865eaf020": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f53a846af66d4a628580a620fd0b7890",
            "placeholder": "​",
            "style": "IPY_MODEL_2c67d977581f4d17b197d5377cfeb037",
            "value": "Processing Files (10 / 10)    : 100%"
          }
        },
        "deb90b544cf8437cb0e3f9873911894d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c9b654eabfe4242813db9e3106b92bc",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_226e30e915f14a58a4a27c3590859230",
            "value": 1
          }
        },
        "e06c8e507f8d4e818488587474d00827": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eac070c6c26e4455b9f0ac93cd7f01d6",
            "placeholder": "​",
            "style": "IPY_MODEL_94eb225ec4c14bb088cd28a9f138fbe1",
            "value": " 2.96GB / 2.96GB, 10.7MB/s  "
          }
        },
        "e85c0109333b4518839d75f18a6e1b33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f53a846af66d4a628580a620fd0b7890": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c67d977581f4d17b197d5377cfeb037": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c9b654eabfe4242813db9e3106b92bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "226e30e915f14a58a4a27c3590859230": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eac070c6c26e4455b9f0ac93cd7f01d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94eb225ec4c14bb088cd28a9f138fbe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1bf64edcc6514ade904b333d07f94aba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ee811e0805c34c6993348cdd6140ae65",
              "IPY_MODEL_b2a831de53bc4e8eac608cfdc11b0a38",
              "IPY_MODEL_5458a51de8a6461b86a2ca962f1f21dd"
            ],
            "layout": "IPY_MODEL_13d8d73ce0c54ccaa125c479e606a5ce"
          }
        },
        "ee811e0805c34c6993348cdd6140ae65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a952f261e6a4c298243813600b2a83e",
            "placeholder": "​",
            "style": "IPY_MODEL_ac89168a3d174f70834dc1862b01c08d",
            "value": "New Data Upload               : 100%"
          }
        },
        "b2a831de53bc4e8eac608cfdc11b0a38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd1cb01a06204d64b926d453dab91441",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3ab540c874b04b3fbde683627bc51f92",
            "value": 1
          }
        },
        "5458a51de8a6461b86a2ca962f1f21dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e010ff80fae14f84b2d8a14c5ccf804e",
            "placeholder": "​",
            "style": "IPY_MODEL_b7c01ef6e4484a1191ad4130e721ced8",
            "value": "  592MB /  592MB, 10.7MB/s  "
          }
        },
        "13d8d73ce0c54ccaa125c479e606a5ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a952f261e6a4c298243813600b2a83e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac89168a3d174f70834dc1862b01c08d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd1cb01a06204d64b926d453dab91441": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "3ab540c874b04b3fbde683627bc51f92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e010ff80fae14f84b2d8a14c5ccf804e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7c01ef6e4484a1191ad4130e721ced8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "627abb1308cd4fa49f9543ab59620135": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ab8b4609191c4f928c76b436fef63c47",
              "IPY_MODEL_6ad4b0e21c284f29bf65b604dce59bd0",
              "IPY_MODEL_38eccde5de9e4fd6ae59f16678d8971f"
            ],
            "layout": "IPY_MODEL_615750ab5bdd4a9f8ff639462edc6616"
          }
        },
        "ab8b4609191c4f928c76b436fef63c47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79b104a85d66474692330bef45add2cc",
            "placeholder": "​",
            "style": "IPY_MODEL_a4aab00a76d94286b51325c6bd291ce1",
            "value": "  ...olor-onnx/tokenizer.model: 100%"
          }
        },
        "6ad4b0e21c284f29bf65b604dce59bd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f51e09e4bd884b2986c3d7c1c5078c90",
            "max": 4689144,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_05fbe31b01bf4215bd840745b2efbec0",
            "value": 4689144
          }
        },
        "38eccde5de9e4fd6ae59f16678d8971f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8811612985e24867844d66949206e79a",
            "placeholder": "​",
            "style": "IPY_MODEL_1d1b6151afb04c11b708d20679fa1d95",
            "value": " 4.69MB / 4.69MB            "
          }
        },
        "615750ab5bdd4a9f8ff639462edc6616": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79b104a85d66474692330bef45add2cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4aab00a76d94286b51325c6bd291ce1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f51e09e4bd884b2986c3d7c1c5078c90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05fbe31b01bf4215bd840745b2efbec0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8811612985e24867844d66949206e79a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d1b6151afb04c11b708d20679fa1d95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e87d22a2785d483799df4c95b2801a7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cef82556c8514e9a84954c11dab4dd9e",
              "IPY_MODEL_e1a98d93791a4ffc851811ee784dc687",
              "IPY_MODEL_4e1760e846bf4679b9a9455e2c7f96cf"
            ],
            "layout": "IPY_MODEL_97b6e68fe0c945919ee3856c0906cb84"
          }
        },
        "cef82556c8514e9a84954c11dab4dd9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_554f35bc28f04b699e9d4e7ce4b9f46e",
            "placeholder": "​",
            "style": "IPY_MODEL_762f9a81739242a79784b9aebbfcdd5c",
            "value": "  ...olor-onnx/onnx/model.onnx: 100%"
          }
        },
        "e1a98d93791a4ffc851811ee784dc687": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eca579628e654ae495726d529ef552ac",
            "max": 193505,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3eb5346e72fe4cb9ac607007df860a29",
            "value": 193505
          }
        },
        "4e1760e846bf4679b9a9455e2c7f96cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bab2e8329f9845bfba5270fd67f15e40",
            "placeholder": "​",
            "style": "IPY_MODEL_498b3566fad44ec785ddcfc613fcb38f",
            "value": "  194kB /  194kB            "
          }
        },
        "97b6e68fe0c945919ee3856c0906cb84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "554f35bc28f04b699e9d4e7ce4b9f46e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "762f9a81739242a79784b9aebbfcdd5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eca579628e654ae495726d529ef552ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3eb5346e72fe4cb9ac607007df860a29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bab2e8329f9845bfba5270fd67f15e40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "498b3566fad44ec785ddcfc613fcb38f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff6bf0d84a84401f9342c8d8e1fd1476": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b34627f956c14578a08e3625475a9b90",
              "IPY_MODEL_5ea93de317c34449a971a0ab049446a0",
              "IPY_MODEL_70b455af3d9346419bf9eb0d3520dc02"
            ],
            "layout": "IPY_MODEL_bc6e1ed66ac14901ba839cf651d82a00"
          }
        },
        "b34627f956c14578a08e3625475a9b90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b888c0fe466840d7a14370e4bc2c4e9f",
            "placeholder": "​",
            "style": "IPY_MODEL_1163fd2e96d84f71a6e83310354b5f2b",
            "value": "  ...nnx/onnx/model_q4f16.onnx: 100%"
          }
        },
        "5ea93de317c34449a971a0ab049446a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29278bcb2ecd4af0a5eb025010e56fd7",
            "max": 303994,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_24f0f127a0dc4ab69ffce337b556d7bb",
            "value": 303994
          }
        },
        "70b455af3d9346419bf9eb0d3520dc02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e9fecdc931443a68f1ef2dde7957666",
            "placeholder": "​",
            "style": "IPY_MODEL_98400082faf74a7f9a40869d949d403e",
            "value": "  304kB /  304kB            "
          }
        },
        "bc6e1ed66ac14901ba839cf651d82a00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b888c0fe466840d7a14370e4bc2c4e9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1163fd2e96d84f71a6e83310354b5f2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29278bcb2ecd4af0a5eb025010e56fd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24f0f127a0dc4ab69ffce337b556d7bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9e9fecdc931443a68f1ef2dde7957666": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98400082faf74a7f9a40869d949d403e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4050c5f2b3b4fafb4cceb2189329a4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_03cc243a120f4d11b39f1952fb3a6bcf",
              "IPY_MODEL_62a52383c25f4a4b86ff0448ebc9f09a",
              "IPY_MODEL_5a3468ebc4a1430ead74ee3f5feb9496"
            ],
            "layout": "IPY_MODEL_906fa49f6ffe4b2f95a75e7c09d3fc5e"
          }
        },
        "03cc243a120f4d11b39f1952fb3a6bcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6547a0cfec5f436096c572484f92ccc4",
            "placeholder": "​",
            "style": "IPY_MODEL_2cecbe2c1ee5480abe530773b1106c73",
            "value": "  ...r-onnx/onnx/model_q4.onnx: 100%"
          }
        },
        "62a52383c25f4a4b86ff0448ebc9f09a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32b933b2bc18402ea1c0ed3310c2926d",
            "max": 239370,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f7d132e647b2411a8580c03a160bf637",
            "value": 239370
          }
        },
        "5a3468ebc4a1430ead74ee3f5feb9496": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d9e224c0eba4f14bc4550476ef2e0b4",
            "placeholder": "​",
            "style": "IPY_MODEL_6022099877bd44e88da3038b86f68365",
            "value": "  239kB /  239kB            "
          }
        },
        "906fa49f6ffe4b2f95a75e7c09d3fc5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6547a0cfec5f436096c572484f92ccc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cecbe2c1ee5480abe530773b1106c73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32b933b2bc18402ea1c0ed3310c2926d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7d132e647b2411a8580c03a160bf637": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d9e224c0eba4f14bc4550476ef2e0b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6022099877bd44e88da3038b86f68365": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43cc19c293524253966448c7fb990cb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9c6b9de372ba45a48c27565779333e71",
              "IPY_MODEL_517e6b9e46f24af3b3912aaca73bf8d1",
              "IPY_MODEL_4a582900a98b4c14a544d1bb4a5137d7"
            ],
            "layout": "IPY_MODEL_a5c7da5bf861444ba71311b86d41c0e1"
          }
        },
        "9c6b9de372ba45a48c27565779333e71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9904d2bd36414d00b7df5a3650d41fea",
            "placeholder": "​",
            "style": "IPY_MODEL_d688edab19ca4cf8affb33ed871ea9d6",
            "value": "  ...onnx/onnx/model_fp16.onnx: 100%"
          }
        },
        "517e6b9e46f24af3b3912aaca73bf8d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f66f22b0af34735b4bb82b2c0eb7dac",
            "max": 258100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_81e23581d5cc410781d1f49fea1fb3c0",
            "value": 258100
          }
        },
        "4a582900a98b4c14a544d1bb4a5137d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6377d3aa77bd4c8bbed35e5e42306703",
            "placeholder": "​",
            "style": "IPY_MODEL_2087854254e246c7a7b2ccd419c5f872",
            "value": "  258kB /  258kB            "
          }
        },
        "a5c7da5bf861444ba71311b86d41c0e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9904d2bd36414d00b7df5a3650d41fea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d688edab19ca4cf8affb33ed871ea9d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f66f22b0af34735b4bb82b2c0eb7dac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81e23581d5cc410781d1f49fea1fb3c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6377d3aa77bd4c8bbed35e5e42306703": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2087854254e246c7a7b2ccd419c5f872": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00808d10f6fb468f860eb844ab46c0b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6684914ee4f14cca91c5e3bee55c73d1",
              "IPY_MODEL_623880f28faf43f6bf848bb92e8b9687",
              "IPY_MODEL_ea62d5512ead4afcb58a704614ca791d"
            ],
            "layout": "IPY_MODEL_b23c2ea666dd40758435316b10d0d98a"
          }
        },
        "6684914ee4f14cca91c5e3bee55c73d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a8ea330266a4e938d67ace1c61ead4e",
            "placeholder": "​",
            "style": "IPY_MODEL_4da7bb9c832c492c8bd23f261d301b39",
            "value": "  ...nnx/model_q4f16.onnx_data: 100%"
          }
        },
        "623880f28faf43f6bf848bb92e8b9687": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66fab2c0f5a74499ae69c108761a73f0",
            "max": 425724416,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5c6dd1c391fb4eaebee474d986bf4fba",
            "value": 425724416
          }
        },
        "ea62d5512ead4afcb58a704614ca791d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cb36ae18351492e8bc9681d87d97ce2",
            "placeholder": "​",
            "style": "IPY_MODEL_5968de84e1df49298ab91d899feafa43",
            "value": "  426MB /  426MB            "
          }
        },
        "b23c2ea666dd40758435316b10d0d98a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a8ea330266a4e938d67ace1c61ead4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4da7bb9c832c492c8bd23f261d301b39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66fab2c0f5a74499ae69c108761a73f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c6dd1c391fb4eaebee474d986bf4fba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8cb36ae18351492e8bc9681d87d97ce2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5968de84e1df49298ab91d899feafa43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbb2559198ea4007888745f2753dc4b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_238e1e396cd84d3eb6a48db500be0068",
              "IPY_MODEL_e0cc735832cb4d35bfc14b2d1e8f520a",
              "IPY_MODEL_a742f6da10dd4ed3908f2095bc045f68"
            ],
            "layout": "IPY_MODEL_df2d342636d3486da1bd6f97fa310d0d"
          }
        },
        "238e1e396cd84d3eb6a48db500be0068": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51575107fcfe44a9aed4a25f59f014d9",
            "placeholder": "​",
            "style": "IPY_MODEL_2e42446953c34b8696d3bae808bd279c",
            "value": "  ...onnx/model_fp16.onnx_data: 100%"
          }
        },
        "e0cc735832cb4d35bfc14b2d1e8f520a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abfbc9caed594dfea6e4976266132746",
            "max": 569862656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3df694631078446a9a1b448a4c907144",
            "value": 569862656
          }
        },
        "a742f6da10dd4ed3908f2095bc045f68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86ee7d69fc9f49b9a34892189bafcf8d",
            "placeholder": "​",
            "style": "IPY_MODEL_b417bad5f2444281af1331e1be61d502",
            "value": "  570MB /  570MB            "
          }
        },
        "df2d342636d3486da1bd6f97fa310d0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51575107fcfe44a9aed4a25f59f014d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e42446953c34b8696d3bae808bd279c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "abfbc9caed594dfea6e4976266132746": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3df694631078446a9a1b448a4c907144": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "86ee7d69fc9f49b9a34892189bafcf8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b417bad5f2444281af1331e1be61d502": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c42dbdd761f34ceea4ee73da840ed72b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f057f6798bba4d25aee44b1921d7b517",
              "IPY_MODEL_f54abf954efe41de9276736dd900d6f0",
              "IPY_MODEL_2589f3fd2c174dcca384c5b3133ed2fe"
            ],
            "layout": "IPY_MODEL_079bf75726404a0db9c908daf5e2fd3e"
          }
        },
        "f057f6798bba4d25aee44b1921d7b517": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33d5731d72384de88ba589201b3df15c",
            "placeholder": "​",
            "style": "IPY_MODEL_a0b0db695fcd4a5eb809fd1fb39a90b3",
            "value": "  ...color-onnx/tokenizer.json: 100%"
          }
        },
        "f54abf954efe41de9276736dd900d6f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4182d403307b45c78534f5fed018c86e",
            "max": 20323385,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c51550e89b9f45128c1ae04c11ee6d5c",
            "value": 20323385
          }
        },
        "2589f3fd2c174dcca384c5b3133ed2fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1de0df802adc48a3990873076b119fcb",
            "placeholder": "​",
            "style": "IPY_MODEL_257274b26cf746678a98b5464de23f7d",
            "value": " 20.3MB / 20.3MB            "
          }
        },
        "079bf75726404a0db9c908daf5e2fd3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33d5731d72384de88ba589201b3df15c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0b0db695fcd4a5eb809fd1fb39a90b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4182d403307b45c78534f5fed018c86e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c51550e89b9f45128c1ae04c11ee6d5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1de0df802adc48a3990873076b119fcb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "257274b26cf746678a98b5464de23f7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c29130550d44b56927d0c857f1213fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2849aee3cb6c4261833e02818804d2ef",
              "IPY_MODEL_baf827ed76354659be787de5e0be9b49",
              "IPY_MODEL_279de607a0d744d3aa27dcbebbbffbe8"
            ],
            "layout": "IPY_MODEL_bc8d082ed1fb44ae943aaaacee986e94"
          }
        },
        "2849aee3cb6c4261833e02818804d2ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7be8cf7d5da44bf7baf3f31092d22b06",
            "placeholder": "​",
            "style": "IPY_MODEL_365c0295ad034dd9b146606bf699921a",
            "value": "  ...x/onnx/model_q4.onnx_data: 100%"
          }
        },
        "baf827ed76354659be787de5e0be9b49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdbaf0d820284e029cd19277d8ebfa8a",
            "max": 801090048,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_534a09c1fe424be88873182db1f376be",
            "value": 801090048
          }
        },
        "279de607a0d744d3aa27dcbebbbffbe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6cf9af764674dc0a547f7e3c727313c",
            "placeholder": "​",
            "style": "IPY_MODEL_167cb35198cb4bf6b16e6c6cbdd99fbe",
            "value": "  801MB /  801MB            "
          }
        },
        "bc8d082ed1fb44ae943aaaacee986e94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7be8cf7d5da44bf7baf3f31092d22b06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "365c0295ad034dd9b146606bf699921a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fdbaf0d820284e029cd19277d8ebfa8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "534a09c1fe424be88873182db1f376be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d6cf9af764674dc0a547f7e3c727313c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "167cb35198cb4bf6b16e6c6cbdd99fbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8390aa3d3f594566bf9d35ffaa6f0d8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1c6d34cca38f4a739bebf6934f1f817b",
              "IPY_MODEL_1cce1aee6aaa4202b53db824b4843c5f",
              "IPY_MODEL_0e7eca903d1144f2a1201b40e0a9582b"
            ],
            "layout": "IPY_MODEL_1fe2b5d2c6e24764a9266c450b7bab93"
          }
        },
        "1c6d34cca38f4a739bebf6934f1f817b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71bc0421acc7403e9bd49bbd10effa6f",
            "placeholder": "​",
            "style": "IPY_MODEL_32599930b7f24df3bd62f12a30ebef46",
            "value": "  ...onnx/onnx/model.onnx_data: 100%"
          }
        },
        "1cce1aee6aaa4202b53db824b4843c5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db45eda1d47048a6a9a286fb8f6c8ea0",
            "max": 1139501568,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_89a6216a16e649919824938a9391e35c",
            "value": 1139501568
          }
        },
        "0e7eca903d1144f2a1201b40e0a9582b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4ad7351f3ef47f49b989259fb5cd83b",
            "placeholder": "​",
            "style": "IPY_MODEL_6288fc8e8ba54d0cbb3c617962626c70",
            "value": " 1.14GB / 1.14GB            "
          }
        },
        "1fe2b5d2c6e24764a9266c450b7bab93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71bc0421acc7403e9bd49bbd10effa6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32599930b7f24df3bd62f12a30ebef46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db45eda1d47048a6a9a286fb8f6c8ea0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89a6216a16e649919824938a9391e35c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c4ad7351f3ef47f49b989259fb5cd83b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6288fc8e8ba54d0cbb3c617962626c70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}