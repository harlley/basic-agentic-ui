{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Gemma 3 with Olive + Export for ONNX Runtime Web\n",
    "\n",
    "This notebook:\n",
    "1. Fine-tunes Gemma 3 270M for function calling using Olive + QLoRA\n",
    "2. Exports to ONNX format optimized for browser (WebGPU)\n",
    "3. Uploads to Hugging Face for use with ONNX Runtime Web\n",
    "\n",
    "**Requirements:** Google Colab with GPU runtime (T4 is sufficient)\n",
    "\n",
    "**References:**\n",
    "- [Olive Fine-tune Tutorial](https://onnxruntime.ai/docs/genai/tutorials/finetune.html)\n",
    "- [ONNX Runtime Web Chat Example](https://github.com/microsoft/onnxruntime-inference-examples/tree/main/js/chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n",
    "\n",
    "**Important:** Specific versions required due to compatibility issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU is available\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies with specific versions\n",
    "# Note: torch 2.5.0 has export bugs, transformers >= 4.45.0 is incompatible\n",
    "!pip install torch==2.4.0 transformers==4.44.0 -q\n",
    "!pip install olive-ai[gpu] -q\n",
    "!pip install onnxruntime-genai-cuda -q\n",
    "!pip install optimum peft bitsandbytes accelerate -q\n",
    "!pip install huggingface_hub -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Olive installation\n",
    "!olive --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Hugging Face (required for Gemma models)\n",
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Upload Training Data\n\nUpload the `dataset/train_data.jsonl` file from your local machine."
  },
  {
   "cell_type": "code",
   "source": "# Upload dataset from local machine\nfrom google.colab import files\n\nprint(\"Upload the file: dataset/train_data.jsonl\")\nuploaded = files.upload()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify uploaded file\nimport json\n\n# The uploaded file should be in the current directory\nwith open(\"train_data.jsonl\", \"r\") as f:\n    lines = f.readlines()\n    \nprint(f\"Loaded {len(lines)} training examples\")\nprint(\"\\nFirst 3 examples:\")\nfor line in lines[:3]:\n    example = json.loads(line)\n    print(f\"  Prompt: {example['prompt']}\")\n    print(f\"  Completion: {example['completion']}\")\n    print()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tune with Olive\n",
    "\n",
    "Using QLoRA for efficient fine-tuning on Colab's T4 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune using Olive CLI with QLoRA\n",
    "!olive finetune \\\n",
    "    --method qlora \\\n",
    "    --model_name_or_path google/gemma-3-270m-it \\\n",
    "    --data_name train_data.jsonl \\\n",
    "    --text_template \"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n{completion}\" \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --max_steps 150 \\\n",
    "    --logging_steps 10 \\\n",
    "    --learning_rate 2e-4 \\\n",
    "    --output_path ./finetuned-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify fine-tuned model output\n",
    "import os\n",
    "\n",
    "print(\"Fine-tuned model files:\")\n",
    "for root, dirs, files in os.walk(\"./finetuned-model\"):\n",
    "    for f in files:\n",
    "        path = os.path.join(root, f)\n",
    "        size = os.path.getsize(path) / 1024 / 1024\n",
    "        print(f\"  {path}: {size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Fine-tuned Model (Optional)\n",
    "\n",
    "Quick test before ONNX export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load fine-tuned model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./finetuned-model\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./finetuned-model\")\n",
    "\n",
    "def test_model(prompt: str):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True)\n",
    "    inputs = inputs.to(model.device)\n",
    "    \n",
    "    outputs = model.generate(inputs, max_new_tokens=50, do_sample=False)\n",
    "    response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Test\n",
    "test_prompts = [\n",
    "    \"Change the square to blue\",\n",
    "    \"What color is the square?\",\n",
    "    \"Make it red\",\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Output: {test_model(prompt)}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up GPU memory before ONNX export\n",
    "del model\n",
    "del tokenizer\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Export to ONNX for Web\n",
    "\n",
    "Export optimized for ONNX Runtime Web with WebGPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to ONNX with auto-optimization\n",
    "# Using fp16 precision for WebGPU\n",
    "!olive auto-opt \\\n",
    "    --model_name_or_path ./finetuned-model \\\n",
    "    --output_path ./onnx-model \\\n",
    "    --device gpu \\\n",
    "    --provider cuda \\\n",
    "    --precision fp16 \\\n",
    "    --use_model_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If auto-opt fails, try manual export with Optimum\n",
    "# Uncomment if needed:\n",
    "\n",
    "# from optimum.onnxruntime import ORTModelForCausalLM\n",
    "# \n",
    "# ort_model = ORTModelForCausalLM.from_pretrained(\n",
    "#     \"./finetuned-model\",\n",
    "#     export=True,\n",
    "#     provider=\"CUDAExecutionProvider\"\n",
    "# )\n",
    "# ort_model.save_pretrained(\"./onnx-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify ONNX model output structure\n",
    "import os\n",
    "\n",
    "print(\"ONNX model files:\")\n",
    "total_size = 0\n",
    "for root, dirs, files in os.walk(\"./onnx-model\"):\n",
    "    for f in files:\n",
    "        path = os.path.join(root, f)\n",
    "        size = os.path.getsize(path) / 1024 / 1024\n",
    "        total_size += size\n",
    "        print(f\"  {path}: {size:.1f} MB\")\n",
    "\n",
    "print(f\"\\nTotal size: {total_size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verify ONNX Model Structure\n",
    "\n",
    "Check the model has correct input/output types for WebGPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install onnx -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import glob\n",
    "\n",
    "# Find the ONNX model file\n",
    "onnx_files = glob.glob(\"./onnx-model/**/*.onnx\", recursive=True)\n",
    "if not onnx_files:\n",
    "    onnx_files = glob.glob(\"./onnx-model/*.onnx\")\n",
    "\n",
    "if onnx_files:\n",
    "    model_path = onnx_files[0]\n",
    "    print(f\"Loading: {model_path}\")\n",
    "    \n",
    "    model = onnx.load(model_path, load_external_data=False)\n",
    "    \n",
    "    print(f\"\\nGraph nodes: {len(model.graph.node)}\")\n",
    "    print(f\"\\nInputs ({len(model.graph.input)}):\")\n",
    "    for inp in model.graph.input[:5]:\n",
    "        dtype = inp.type.tensor_type.elem_type\n",
    "        dtype_name = onnx.TensorProto.DataType.Name(dtype)\n",
    "        print(f\"  {inp.name}: {dtype_name}\")\n",
    "    \n",
    "    print(f\"\\nOutputs ({len(model.graph.output)}):\")\n",
    "    for out in model.graph.output[:5]:\n",
    "        dtype = out.type.tensor_type.elem_type\n",
    "        dtype_name = onnx.TensorProto.DataType.Name(dtype)\n",
    "        print(f\"  {out.name}: {dtype_name}\")\n",
    "else:\n",
    "    print(\"No ONNX files found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test ONNX Model with onnxruntime-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with onnxruntime-genai (if available)\n",
    "try:\n",
    "    import onnxruntime_genai as og\n",
    "    \n",
    "    model = og.Model(\"./onnx-model\")\n",
    "    tokenizer = og.Tokenizer(model)\n",
    "    \n",
    "    def generate(prompt: str) -> str:\n",
    "        full_prompt = f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "        input_tokens = tokenizer.encode(full_prompt)\n",
    "        \n",
    "        params = og.GeneratorParams(model)\n",
    "        params.set_search_options(max_length=100)\n",
    "        params.input_ids = input_tokens\n",
    "        \n",
    "        output_tokens = model.generate(params)\n",
    "        return tokenizer.decode(output_tokens[0])\n",
    "    \n",
    "    # Test\n",
    "    print(\"Testing ONNX model with onnxruntime-genai:\")\n",
    "    for prompt in [\"Change the square to blue\", \"What color is the square?\"]:\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        print(f\"Output: {generate(prompt)}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"onnxruntime-genai test skipped: {e}\")\n",
    "    print(\"This is OK - the model will be tested in the browser.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Upload to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure your Hugging Face repo\n",
    "HF_USERNAME = \"harlley\"  # Change to your username\n",
    "REPO_NAME = \"functiongemma-square-color-olive\"\n",
    "REPO_ID = f\"{HF_USERNAME}/{REPO_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# Create repo if it doesn't exist\n",
    "try:\n",
    "    create_repo(REPO_ID, repo_type=\"model\", exist_ok=True)\n",
    "    print(f\"Repository ready: https://huggingface.co/{REPO_ID}\")\n",
    "except Exception as e:\n",
    "    print(f\"Repo creation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload ONNX model\n",
    "api.upload_folder(\n",
    "    folder_path=\"./onnx-model\",\n",
    "    repo_id=REPO_ID,\n",
    "    repo_type=\"model\",\n",
    ")\n",
    "\n",
    "print(f\"\\nModel uploaded to: https://huggingface.co/{REPO_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a README for the model\n",
    "readme_content = f\"\"\"---\n",
    "license: apache-2.0\n",
    "tags:\n",
    "  - onnx\n",
    "  - gemma\n",
    "  - function-calling\n",
    "  - webgpu\n",
    "  - onnxruntime-web\n",
    "---\n",
    "\n",
    "# FunctionGemma Square Color (Olive ONNX)\n",
    "\n",
    "Fine-tuned Gemma 3 270M model for function calling, exported to ONNX using Microsoft Olive.\n",
    "\n",
    "## Usage with ONNX Runtime Web\n",
    "\n",
    "```javascript\n",
    "import * as ort from 'onnxruntime-web';\n",
    "\n",
    "const session = await ort.InferenceSession.create('model.onnx', {{\n",
    "  executionProviders: ['webgpu', 'wasm']\n",
    "}});\n",
    "```\n",
    "\n",
    "## Functions\n",
    "\n",
    "- `set_square_color(color)` - Set the square color\n",
    "- `get_square_color()` - Get the current square color\n",
    "\n",
    "## Training\n",
    "\n",
    "- Base model: google/gemma-3-270m-it\n",
    "- Method: QLoRA via Microsoft Olive\n",
    "- Precision: fp16\n",
    "\"\"\"\n",
    "\n",
    "with open(\"README.md\", \"w\") as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"README.md\",\n",
    "    path_in_repo=\"README.md\",\n",
    "    repo_id=REPO_ID,\n",
    "    repo_type=\"model\",\n",
    ")\n",
    "\n",
    "print(\"README uploaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Next Steps\n",
    "\n",
    "The model is now ready for use with ONNX Runtime Web in the browser.\n",
    "\n",
    "See `OLIVE_MIGRATION_PLAN.md` for instructions on:\n",
    "1. Creating the `LLM` class for browser inference\n",
    "2. Updating the worker to use the hybrid approach\n",
    "3. Testing in the browser with WebGPU\n",
    "\n",
    "**Model URL:** `https://huggingface.co/{REPO_ID}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*50}\")\n",
    "print(\"DONE!\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"\\nModel URL: https://huggingface.co/{REPO_ID}\")\n",
    "print(f\"\\nNext: Update your browser app to use ONNX Runtime Web\")\n",
    "print(f\"See: OLIVE_MIGRATION_PLAN.md for details\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}