{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì¶ Export FunctionGemma to ONNX for Transformers.js\n",
    "\n",
    "This notebook exports a fine-tuned FunctionGemma model to ONNX format with proper quantization.\n",
    "\n",
    "## Expected Output Sizes\n",
    "| Variant | Size |\n",
    "|---------|------|\n",
    "| `model.onnx` (fp32) | ~1.1 GB |\n",
    "| `model_fp16.onnx` | ~570 MB |\n",
    "| `model_q8.onnx` | ~280 MB |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers huggingface_hub onnx onnxslim\n",
    "!pip install -q \"optimum[onnx]\" onnxruntime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clone Transformers.js Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --depth 1 https://github.com/huggingface/transformers.js.git\n",
    "%cd transformers.js"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Login to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuration\n",
    "\n",
    "‚ö†Ô∏è **Edit these values for your model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"harlley/functiongemma-square-color\"\n",
    "OUTPUT_DIR = \"./models\"\n",
    "TASK = \"text-generation-with-past\"\n",
    "HUB_REPO = \"harlley/functiongemma-square-color-ONNX\"\n",
    "\n",
    "print(f\"Source Model: {MODEL_ID}\")\n",
    "print(f\"Output: {OUTPUT_DIR}\")\n",
    "print(f\"Hub Repo: {HUB_REPO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch quantize.py to skip Q4 if not available\n",
    "import os\n",
    "\n",
    "quantize_file = \"scripts/quantize.py\"\n",
    "with open(quantize_file, 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "if \"matmul_4bits_quantizer\" in content:\n",
    "    patched = content.replace(\n",
    "        \"from onnxruntime.quantization.matmul_4bits_quantizer import MatMul4BitsQuantizer\",\n",
    "        \"\"\"try:\n",
    "    from onnxruntime.quantization.matmul_4bits_quantizer import MatMul4BitsQuantizer\n",
    "    HAS_4BIT = True\n",
    "except ImportError:\n",
    "    HAS_4BIT = False\n",
    "    print(\"Warning: 4-bit quantization not available\")\"\"\"\n",
    "    )\n",
    "    with open(quantize_file, 'w') as f:\n",
    "        f.write(patched)\n",
    "    print(\"‚úÖ Patched quantize.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m scripts.convert \\\n",
    "    --model_id {MODEL_ID} \\\n",
    "    --task {TASK} \\\n",
    "    --output_parent_dir {OUTPUT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. FP16 Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnxruntime.transformers.float16 import convert_float_to_float16\n",
    "import os\n",
    "\n",
    "onnx_folder = os.path.join(OUTPUT_DIR, MODEL_ID, \"onnx\")\n",
    "model_path = os.path.join(onnx_folder, \"model.onnx\")\n",
    "model_fp16_path = os.path.join(onnx_folder, \"model_fp16.onnx\")\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"Loading {model_path}...\")\n",
    "    model = onnx.load(model_path)\n",
    "    \n",
    "    print(\"Converting to FP16...\")\n",
    "    model_fp16 = convert_float_to_float16(\n",
    "        model,\n",
    "        keep_io_types=True,\n",
    "        disable_shape_infer=True,\n",
    "        min_positive_val=1e-7,\n",
    "        max_finite_val=65504.0\n",
    "    )\n",
    "    \n",
    "    onnx.save(model_fp16, model_fp16_path)\n",
    "    \n",
    "    orig = os.path.getsize(model_path) / (1024**2)\n",
    "    fp16 = os.path.getsize(model_fp16_path) / (1024**2)\n",
    "    print(f\"\\n‚úÖ FP32: {orig:.0f} MB ‚Üí FP16: {fp16:.0f} MB ({(1-fp16/orig)*100:.0f}% reduction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Q8 Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "import os\n",
    "\n",
    "onnx_folder = os.path.join(OUTPUT_DIR, MODEL_ID, \"onnx\")\n",
    "model_path = os.path.join(onnx_folder, \"model.onnx\")\n",
    "model_q8_path = os.path.join(onnx_folder, \"model_q8.onnx\")\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Quantizing to Q8...\")\n",
    "    quantize_dynamic(\n",
    "        model_input=model_path,\n",
    "        model_output=model_q8_path,\n",
    "        weight_type=QuantType.QInt8,\n",
    "        per_channel=False,\n",
    "        reduce_range=False\n",
    "    )\n",
    "    \n",
    "    orig = os.path.getsize(model_path) / (1024**2)\n",
    "    q8 = os.path.getsize(model_q8_path) / (1024**2)\n",
    "    print(f\"\\n‚úÖ FP32: {orig:.0f} MB ‚Üí Q8: {q8:.0f} MB ({(1-q8/orig)*100:.0f}% reduction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Add Chat Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "REFERENCE_MODEL = \"onnx-community/functiongemma-270m-it-ONNX\"\n",
    "\n",
    "print(f\"Downloading chat_template from {REFERENCE_MODEL}...\")\n",
    "ref_path = hf_hub_download(REFERENCE_MODEL, \"tokenizer_config.json\")\n",
    "\n",
    "with open(ref_path, 'r') as f:\n",
    "    ref_config = json.load(f)\n",
    "\n",
    "model_folder = os.path.join(OUTPUT_DIR, MODEL_ID)\n",
    "output_path = os.path.join(model_folder, \"tokenizer_config.json\")\n",
    "\n",
    "with open(output_path, 'r') as f:\n",
    "    your_config = json.load(f)\n",
    "\n",
    "if 'chat_template' in ref_config:\n",
    "    your_config['chat_template'] = ref_config['chat_template']\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(your_config, f, indent=2)\n",
    "    print(\"‚úÖ chat_template added!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Verify Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model_folder = os.path.join(OUTPUT_DIR, MODEL_ID)\n",
    "\n",
    "def list_files(folder, prefix=\"\"):\n",
    "    total = 0\n",
    "    for f in sorted(os.listdir(folder)):\n",
    "        path = os.path.join(folder, f)\n",
    "        if os.path.isfile(path):\n",
    "            size = os.path.getsize(path)\n",
    "            total += size\n",
    "            print(f\"{prefix}üìÑ {f}: {size/(1024**2):.1f} MB\")\n",
    "        else:\n",
    "            print(f\"{prefix}üìÇ {f}/\")\n",
    "            total += list_files(path, prefix + \"  \")\n",
    "    return total\n",
    "\n",
    "print(f\"üìÅ {model_folder}:\")\n",
    "print(\"=\"*50)\n",
    "total = list_files(model_folder)\n",
    "print(\"=\"*50)\n",
    "print(f\"üìä Total: {total/(1024**2):.0f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test Model\n",
    "\n",
    "‚ö†Ô∏è **Skip this step if testing fails - just proceed to upload.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "model_folder = os.path.join(OUTPUT_DIR, MODEL_ID)\n",
    "onnx_folder = os.path.join(model_folder, \"onnx\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_folder)\n",
    "\n",
    "# Load ONNX model directly with onnxruntime\n",
    "model_path = os.path.join(onnx_folder, \"model.onnx\")\n",
    "session = ort.InferenceSession(model_path, providers=['CPUExecutionProvider'])\n",
    "print(f\"‚úÖ Loaded: model.onnx\")\n",
    "\n",
    "# Tools\n",
    "tools = [\n",
    "    {\"type\": \"function\", \"function\": {\n",
    "        \"name\": \"set_square_color\",\n",
    "        \"description\": \"Sets the color of the square.\",\n",
    "        \"parameters\": {\"type\": \"object\", \"properties\": {\"color\": {\"type\": \"string\"}}, \"required\": [\"color\"]}\n",
    "    }},\n",
    "    {\"type\": \"function\", \"function\": {\n",
    "        \"name\": \"get_square_color\",\n",
    "        \"description\": \"Gets the color.\",\n",
    "        \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []}\n",
    "    }}\n",
    "]\n",
    "\n",
    "# Test\n",
    "msgs = [{\"role\": \"user\", \"content\": \"Change the color to blue\"}]\n",
    "fmt = tokenizer.apply_chat_template(msgs, tools=tools, tokenize=False, add_generation_prompt=True)\n",
    "print(f\"\\nüìù Formatted input length: {len(fmt)} chars\")\n",
    "print(\"\\n‚úÖ Tokenizer and chat template working!\")\n",
    "print(\"\\n‚ö†Ô∏è Full generation test skipped - will test in browser after upload.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Upload to Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "\n",
    "model_folder = os.path.join(OUTPUT_DIR, MODEL_ID)\n",
    "\n",
    "api = HfApi()\n",
    "api.create_repo(repo_id=HUB_REPO, exist_ok=True)\n",
    "api.upload_folder(\n",
    "    folder_path=model_folder,\n",
    "    repo_id=HUB_REPO,\n",
    "    commit_message=\"Upload ONNX model with FP16, Q8 and chat_template\"\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Done!\")\n",
    "print(f\"üîó https://huggingface.co/{HUB_REPO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Usage\n",
    "\n",
    "```javascript\n",
    "import { AutoModelForCausalLM, AutoTokenizer } from '@huggingface/transformers';\n",
    "\n",
    "const model = await AutoModelForCausalLM.from_pretrained(\n",
    "    'harlley/functiongemma-square-color-ONNX',\n",
    "    { dtype: 'q8', device: 'webgpu' }  // or 'fp16'\n",
    ");\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
