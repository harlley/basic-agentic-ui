{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé® Fine-tuning FunctionGemma for Square Color Control\n",
    "\n",
    "This notebook demonstrates how to fine-tune FunctionGemma to recognize color control commands.\n",
    "\n",
    "**Author:** [Your Name]\n",
    "**Portfolio:** AI Engineering\n",
    "\n",
    "## Objectives\n",
    "1. Train the model to call `set_square_color` when the user wants to change the color\n",
    "2. Train the model to call `get_square_color` when the user asks about the current color\n",
    "3. Support various natural language command styles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install -q torch tensorboard\n",
    "%pip install -q transformers datasets accelerate evaluate trl protobuf sentencepiece\n",
    "\n",
    "# If running on Ampere+ GPU (A100, L4), uncomment:\n",
    "# %pip install -q flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Hugging Face Hub\n",
    "from huggingface_hub import login\n",
    "\n",
    "# If using Colab secrets:\n",
    "# from google.colab import userdata\n",
    "# login(token=userdata.get('HF_TOKEN'))\n",
    "\n",
    "# Or interactive login:\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BASE_MODEL = \"google/functiongemma-270m-it\"\n",
    "OUTPUT_DIR = \"functiongemma-square-color\"  # Model name on your HF Hub\n",
    "LEARNING_RATE = 5e-5\n",
    "NUM_EPOCHS = 8\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 2. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers.utils import get_json_schema\n",
    "\n",
    "# Tool definitions\n",
    "def set_square_color(color: str) -> str:\n",
    "    \"\"\"\n",
    "    Sets the color of the square displayed on the screen.\n",
    "    \n",
    "    Args:\n",
    "        color: The color to set, e.g. red, blue, green\n",
    "    \"\"\"\n",
    "    return f\"Color set to {color}\"\n",
    "\n",
    "def get_square_color() -> str:\n",
    "    \"\"\"\n",
    "    Returns the current color of the square.\n",
    "    Use this when the user asks about the current color.\n",
    "    \"\"\"\n",
    "    return \"Current color\"\n",
    "\n",
    "# Generate schemas automatically\n",
    "TOOLS = [\n",
    "    get_json_schema(set_square_color),\n",
    "    get_json_schema(get_square_color)\n",
    "]\n",
    "\n",
    "print(\"Tool schemas:\")\n",
    "print(json.dumps(TOOLS, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset from file\n",
    "with open(\"dataset/square_color_dataset.json\", \"r\") as f:\n",
    "    square_color_dataset = json.load(f)\n",
    "\n",
    "print(f\"Total examples: {len(square_color_dataset)}\")\n",
    "print(f\"  - SET: {len([x for x in square_color_dataset if x['tool_name'] == 'set_square_color'])}\")\n",
    "print(f\"  - GET: {len([x for x in square_color_dataset if x['tool_name'] == 'get_square_color'])}\")\n",
    "\n",
    "# Preview first few examples\n",
    "print(\"\\nFirst 3 examples:\")\n",
    "for i, sample in enumerate(square_color_dataset[:3]):\n",
    "    print(f\"  {i+1}. \\\"{sample['user_content']}\\\" ‚Üí {sample['tool_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to conversation format\n",
    "SYSTEM_PROMPT = \"You are a model that can do function calling with the following functions\"\n",
    "\n",
    "def create_conversation(sample):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"developer\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": sample[\"user_content\"]},\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"tool_calls\": [{\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\n",
    "                        \"name\": sample[\"tool_name\"],\n",
    "                        \"arguments\": json.loads(sample[\"tool_arguments\"])\n",
    "                    }\n",
    "                }]\n",
    "            },\n",
    "        ],\n",
    "        \"tools\": TOOLS\n",
    "    }\n",
    "\n",
    "# Create dataset\n",
    "dataset = Dataset.from_list(square_color_dataset)\n",
    "dataset = dataset.map(create_conversation, remove_columns=dataset.features, batched=False)\n",
    "\n",
    "# Split 80/20\n",
    "dataset = dataset.train_test_split(test_size=0.2, shuffle=True, seed=42)\n",
    "\n",
    "print(f\"Train: {len(dataset['train'])} examples\")\n",
    "print(f\"Test: {len(dataset['test'])} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize an example\n",
    "print(\"Formatted conversation example:\")\n",
    "print(json.dumps(dataset[\"train\"][0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ 3. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config,        \n",
    "    attn_implementation=\"eager\"\n",
    "    \n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "print(f\"Device: {model.device}\")\n",
    "print(f\"DType: {model.dtype}\")\n",
    "print(f\"Parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how the tokenizer formats the prompt\n",
    "debug_msg = tokenizer.apply_chat_template(\n",
    "    dataset[\"train\"][0][\"messages\"],\n",
    "    tools=dataset[\"train\"][0][\"tools\"],\n",
    "    add_generation_prompt=False,\n",
    "    tokenize=False\n",
    ")\n",
    "\n",
    "print(\"=== Formatted prompt ===\")\n",
    "print(debug_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ 3.5. Pre-Training Evaluation (Baseline)\n",
    "\n",
    "Before fine-tuning, let's evaluate the base model to establish a baseline. This helps us measure the actual improvement from fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, test_samples, tools, system_prompt, verbose=True):\n",
    "    \"\"\"\n",
    "    Evaluate model on a set of test samples.\n",
    "    Returns accuracy metrics and detailed results.\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"total\": len(test_samples),\n",
    "        \"correct\": 0,\n",
    "        \"correct_tool\": 0,\n",
    "        \"correct_args\": 0,\n",
    "        \"details\": []\n",
    "    }\n",
    "    \n",
    "    for sample in test_samples:\n",
    "        messages = [\n",
    "            {\"role\": \"developer\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": sample[\"user_content\"]},\n",
    "        ]\n",
    "        \n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tools=tools,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=128,\n",
    "                do_sample=False,\n",
    "            )\n",
    "        \n",
    "        input_length = inputs['input_ids'].shape[1]\n",
    "        response = tokenizer.decode(output[0][input_length:], skip_special_tokens=False)\n",
    "        \n",
    "        # Check if correct tool was called\n",
    "        tool_correct = sample[\"tool_name\"] in response\n",
    "        \n",
    "        # Check if arguments are correct (for set_square_color)\n",
    "        args_correct = False\n",
    "        if tool_correct and sample[\"tool_name\"] == \"set_square_color\":\n",
    "            expected_args = json.loads(sample[\"tool_arguments\"])\n",
    "            args_correct = expected_args.get(\"color\", \"\") in response\n",
    "        elif tool_correct and sample[\"tool_name\"] == \"get_square_color\":\n",
    "            args_correct = True  # No args needed\n",
    "        \n",
    "        if tool_correct:\n",
    "            results[\"correct_tool\"] += 1\n",
    "        if tool_correct and args_correct:\n",
    "            results[\"correct\"] += 1\n",
    "            results[\"correct_args\"] += 1\n",
    "        \n",
    "        results[\"details\"].append({\n",
    "            \"input\": sample[\"user_content\"],\n",
    "            \"expected_tool\": sample[\"tool_name\"],\n",
    "            \"expected_args\": sample[\"tool_arguments\"],\n",
    "            \"response\": response,\n",
    "            \"tool_correct\": tool_correct,\n",
    "            \"args_correct\": args_correct\n",
    "        })\n",
    "    \n",
    "    results[\"tool_accuracy\"] = results[\"correct_tool\"] / results[\"total\"] * 100\n",
    "    results[\"full_accuracy\"] = results[\"correct\"] / results[\"total\"] * 100\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Tool Accuracy: {results['correct_tool']}/{results['total']} ({results['tool_accuracy']:.1f}%)\")\n",
    "        print(f\"Full Accuracy (tool + args): {results['correct']}/{results['total']} ({results['full_accuracy']:.1f}%)\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation test set from the dataset (sample 5 SET + 5 GET)\n",
    "import random\n",
    "\n",
    "random.seed(42)  # For reproducibility\n",
    "\n",
    "set_samples = [s for s in square_color_dataset if s[\"tool_name\"] == \"set_square_color\"]\n",
    "get_samples = [s for s in square_color_dataset if s[\"tool_name\"] == \"get_square_color\"]\n",
    "\n",
    "eval_test_cases = random.sample(set_samples, min(5, len(set_samples))) + \\\n",
    "                  random.sample(get_samples, min(5, len(get_samples)))\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"PRE-TRAINING EVALUATION (Baseline)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nEvaluating base model on {len(eval_test_cases)} test cases...\\n\")\n",
    "\n",
    "baseline_results = evaluate_model(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    test_samples=eval_test_cases,\n",
    "    tools=TOOLS,\n",
    "    system_prompt=SYSTEM_PROMPT\n",
    ")\n",
    "\n",
    "# Show some example outputs\n",
    "print(\"\\n--- Sample Outputs (Base Model) ---\")\n",
    "for i, detail in enumerate(baseline_results[\"details\"][:4]):\n",
    "    status = \"‚úÖ\" if detail[\"tool_correct\"] else \"‚ùå\"\n",
    "    print(f\"\\n{status} Input: {detail['input']}\")\n",
    "    print(f\"   Expected: {detail['expected_tool']}\")\n",
    "    print(f\"   Output: {detail['response'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî• 4. Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(\"Reloading model for fine-tuning (without quantization)...\")\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "print(f\"Device: {model.device}\")\n",
    "print(f\"DType: {model.dtype}\")\n",
    "print(f\"Parameters: {model.num_parameters():,}\")\n",
    "print(\"Ready for fine-tuning!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "torch_dtype = model.dtype\n",
    "\n",
    "# Training configuration\n",
    "args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    max_length=512,\n",
    "    packing=False,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_checkpointing=False,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    logging_steps=1,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    fp16=True if torch_dtype == torch.float16 else False,\n",
    "    bf16=True if torch_dtype == torch.bfloat16 else False,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    push_to_hub=True,\n",
    "    report_to=\"tensorboard\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Trainer created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Start training!\n",
    "print(\"Starting fine-tuning...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "trainer.save_model()\n",
    "print(f\"Model saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà 5. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract loss history\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "train_losses = [log[\"loss\"] for log in log_history if \"loss\" in log]\n",
    "epoch_train = [log[\"epoch\"] for log in log_history if \"loss\" in log]\n",
    "eval_losses = [log[\"eval_loss\"] for log in log_history if \"eval_loss\" in log]\n",
    "epoch_eval = [log[\"epoch\"] for log in log_history if \"eval_loss\" in log]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epoch_train, train_losses, label=\"Training Loss\", alpha=0.7)\n",
    "plt.plot(epoch_eval, eval_losses, label=\"Validation Loss\", marker='o')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ 6. Post-Training Evaluation\n",
    "\n",
    "Now let's evaluate the fine-tuned model and compare it with the baseline to measure the improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"POST-TRAINING EVALUATION (Fine-tuned)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nEvaluating fine-tuned model on {len(eval_test_cases)} test cases...\\n\")\n",
    "\n",
    "finetuned_results = evaluate_model(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    test_samples=eval_test_cases,\n",
    "    tools=TOOLS,\n",
    "    system_prompt=SYSTEM_PROMPT\n",
    ")\n",
    "\n",
    "# Show some example outputs\n",
    "print(\"\\n--- Sample Outputs (Fine-tuned Model) ---\")\n",
    "for i, detail in enumerate(finetuned_results[\"details\"][:4]):\n",
    "    status = \"‚úÖ\" if detail[\"tool_correct\"] else \"‚ùå\"\n",
    "    print(f\"\\n{status} Input: {detail['input']}\")\n",
    "    print(f\"   Expected: {detail['expected_tool']}\")\n",
    "    print(f\"   Output: {detail['response'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare baseline vs fine-tuned results\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä COMPARISON: Baseline vs Fine-tuned\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n{'Metric':<30} {'Baseline':>12} {'Fine-tuned':>12} {'Improvement':>12}\")\n",
    "print(\"-\" * 66)\n",
    "\n",
    "# Tool accuracy comparison\n",
    "tool_improvement = finetuned_results[\"tool_accuracy\"] - baseline_results[\"tool_accuracy\"]\n",
    "print(f\"{'Tool Accuracy':<30} {baseline_results['tool_accuracy']:>11.1f}% {finetuned_results['tool_accuracy']:>11.1f}% {tool_improvement:>+11.1f}%\")\n",
    "\n",
    "# Full accuracy comparison\n",
    "full_improvement = finetuned_results[\"full_accuracy\"] - baseline_results[\"full_accuracy\"]\n",
    "print(f\"{'Full Accuracy (tool + args)':<30} {baseline_results['full_accuracy']:>11.1f}% {finetuned_results['full_accuracy']:>11.1f}% {full_improvement:>+11.1f}%\")\n",
    "\n",
    "print(\"-\" * 66)\n",
    "\n",
    "# Summary\n",
    "if full_improvement > 0:\n",
    "    print(f\"\\n‚úÖ Fine-tuning improved accuracy by {full_improvement:.1f} percentage points!\")\n",
    "elif full_improvement == 0:\n",
    "    print(f\"\\n‚ö†Ô∏è No change in accuracy. Consider adjusting training parameters.\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Accuracy decreased. Check for overfitting or data issues.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Baseline vs Fine-tuned comparison\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Chart 1: Bar chart comparison\n",
    "metrics = ['Tool\\nAccuracy', 'Full\\nAccuracy']\n",
    "baseline_vals = [baseline_results[\"tool_accuracy\"], baseline_results[\"full_accuracy\"]]\n",
    "finetuned_vals = [finetuned_results[\"tool_accuracy\"], finetuned_results[\"full_accuracy\"]]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = axes[0].bar(x - width/2, baseline_vals, width, label='Baseline', color='#ff6b6b', alpha=0.8)\n",
    "bars2 = axes[0].bar(x + width/2, finetuned_vals, width, label='Fine-tuned', color='#4ecdc4', alpha=0.8)\n",
    "\n",
    "axes[0].set_ylabel('Accuracy (%)')\n",
    "axes[0].set_title('Model Performance: Baseline vs Fine-tuned')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(metrics)\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, 110)\n",
    "axes[0].axhline(y=100, color='gray', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    axes[0].annotate(f'{height:.1f}%', xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                     xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=10)\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    axes[0].annotate(f'{height:.1f}%', xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                     xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Chart 2: Per-sample comparison\n",
    "sample_labels = [d[\"input\"][:20] + \"...\" for d in baseline_results[\"details\"]]\n",
    "baseline_correct = [1 if d[\"tool_correct\"] else 0 for d in baseline_results[\"details\"]]\n",
    "finetuned_correct = [1 if d[\"tool_correct\"] else 0 for d in finetuned_results[\"details\"]]\n",
    "\n",
    "x2 = np.arange(len(sample_labels))\n",
    "width2 = 0.35\n",
    "\n",
    "axes[1].barh(x2 - width2/2, baseline_correct, width2, label='Baseline', color='#ff6b6b', alpha=0.8)\n",
    "axes[1].barh(x2 + width2/2, finetuned_correct, width2, label='Fine-tuned', color='#4ecdc4', alpha=0.8)\n",
    "\n",
    "axes[1].set_xlabel('Correct (1) / Incorrect (0)')\n",
    "axes[1].set_title('Per-Sample Results')\n",
    "axes[1].set_yticks(x2)\n",
    "axes[1].set_yticklabels(sample_labels, fontsize=8)\n",
    "axes[1].legend(loc='lower right')\n",
    "axes[1].set_xlim(-0.1, 1.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed per-sample comparison\n",
    "print(\"\\nüìã Detailed Per-Sample Comparison:\")\n",
    "print(\"-\" * 80)\n",
    "for i, (b, f) in enumerate(zip(baseline_results[\"details\"], finetuned_results[\"details\"])):\n",
    "    b_status = \"‚úÖ\" if b[\"tool_correct\"] else \"‚ùå\"\n",
    "    f_status = \"‚úÖ\" if f[\"tool_correct\"] else \"‚ùå\"\n",
    "    change = \"\"\n",
    "    if not b[\"tool_correct\"] and f[\"tool_correct\"]:\n",
    "        change = \" üéâ FIXED!\"\n",
    "    elif b[\"tool_correct\"] and not f[\"tool_correct\"]:\n",
    "        change = \" ‚ö†Ô∏è REGRESSED\"\n",
    "    print(f\"{b['input'][:40]:<42} Base: {b_status}  Fine-tuned: {f_status}{change}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì§ 7. Push to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push to Hub\n",
    "trainer.push_to_hub()\n",
    "\n",
    "print(f\"\\n‚úÖ Model pushed to: https://huggingface.co/{trainer.hub_model_id}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
